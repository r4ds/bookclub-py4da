[
  {
    "objectID": "01_notes.html",
    "href": "01_notes.html",
    "title": "1  Notes",
    "section": "",
    "text": "1.0.1 Why Python for Data Analysis (Data Science)?",
    "crumbs": [
      "1. Preliminaries",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Notes</span>"
    ]
  },
  {
    "objectID": "01_notes.html#essential-python-libraries",
    "href": "01_notes.html#essential-python-libraries",
    "title": "1  Notes",
    "section": "1.1 Essential Python Libraries",
    "text": "1.1 Essential Python Libraries\n\nNumPy ( Numerical Python)\nPandas\nMatplotlib\nIpython and Jupyter\nScipy\nSklearn\nStatsModel",
    "crumbs": [
      "1. Preliminaries",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Notes</span>"
    ]
  },
  {
    "objectID": "01_notes.html#numpy",
    "href": "01_notes.html#numpy",
    "title": "1  Notes",
    "section": "1.2 Numpy",
    "text": "1.2 Numpy\n\nShort for Numerical Python, has long been a cornerstone of numerical computing in Python. It provides the data structures, algorithms, and library glue needed for most scientific applications involving numerical data in Python",
    "crumbs": [
      "1. Preliminaries",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Notes</span>"
    ]
  },
  {
    "objectID": "01_notes.html#pandas",
    "href": "01_notes.html#pandas",
    "title": "1  Notes",
    "section": "1.3 Pandas",
    "text": "1.3 Pandas\n\npandas provides high-level data structures and functions designed to make working with structured or tabular data intuitive and flexible.\n\n\nIt provides convenient indexing functionality to enable you to reshape, slice and dice, perform aggregations, and select subsets of data. Since data manipulation, preparation, and cleaning is such an important skill in data analysis,\nSee R vs Pandas comparison",
    "crumbs": [
      "1. Preliminaries",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Notes</span>"
    ]
  },
  {
    "objectID": "01_notes.html#matplotlib",
    "href": "01_notes.html#matplotlib",
    "title": "1  Notes",
    "section": "1.4 Matplotlib",
    "text": "1.4 Matplotlib\n\nis the most popular Python library for producing plots and other two-dimensional data visualizations",
    "crumbs": [
      "1. Preliminaries",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Notes</span>"
    ]
  },
  {
    "objectID": "01_notes.html#ipython-and-jupyter",
    "href": "01_notes.html#ipython-and-jupyter",
    "title": "1  Notes",
    "section": "1.5 IPython and Jupyter",
    "text": "1.5 IPython and Jupyter\n\nThe IPython system can now be used as a kernel (a programming language mode) for using Python with Jupyter.",
    "crumbs": [
      "1. Preliminaries",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Notes</span>"
    ]
  },
  {
    "objectID": "01_notes.html#scipy",
    "href": "01_notes.html#scipy",
    "title": "1  Notes",
    "section": "1.6 SciPy",
    "text": "1.6 SciPy\n\nSciPy is a collection of packages addressing a number of foundational problems in scientific computing.",
    "crumbs": [
      "1. Preliminaries",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Notes</span>"
    ]
  },
  {
    "objectID": "01_notes.html#scikit-learn",
    "href": "01_notes.html#scikit-learn",
    "title": "1  Notes",
    "section": "1.7 Scikit-learn",
    "text": "1.7 Scikit-learn\n\ngeneral-purpose machine learning toolkit for Python programmers.",
    "crumbs": [
      "1. Preliminaries",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Notes</span>"
    ]
  },
  {
    "objectID": "01_notes.html#statsmodels",
    "href": "01_notes.html#statsmodels",
    "title": "1  Notes",
    "section": "1.8 Statsmodels",
    "text": "1.8 Statsmodels\n\nis a statistical analysis package\n\n\nCompared with scikit-learn, statsmodels contains algorithms for classical (primarily frequentist) statistics and econometrics.",
    "crumbs": [
      "1. Preliminaries",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Notes</span>"
    ]
  },
  {
    "objectID": "01_notes.html#other-packages",
    "href": "01_notes.html#other-packages",
    "title": "1  Notes",
    "section": "1.9 Other Packages",
    "text": "1.9 Other Packages\n\nTensorFlow or PyTorch or Keras",
    "crumbs": [
      "1. Preliminaries",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Notes</span>"
    ]
  },
  {
    "objectID": "01_notes.html#installing-necessary-packages",
    "href": "01_notes.html#installing-necessary-packages",
    "title": "1  Notes",
    "section": "1.10 Installing Necessary Packages",
    "text": "1.10 Installing Necessary Packages\n\nWe can install Python packages using “Pip” or “Conda”. Read more about pip vs python\n\nThe author recommends:\n\nMiniconda, a minimal installation of the conda package manager, along with conda-forge, a community-maintained software distribution based on conda.\nThis book uses Python 3.10 throughout.",
    "crumbs": [
      "1. Preliminaries",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Notes</span>"
    ]
  },
  {
    "objectID": "01_notes.html#mini-conda",
    "href": "01_notes.html#mini-conda",
    "title": "1  Notes",
    "section": "1.11 Mini-conda",
    "text": "1.11 Mini-conda\n\nConda is a packaging tool and installer that aims to do more than what pip does; handle library dependencies outside of the Python packages as well as the Python packages themselves. Conda also creates a virtual environment, like virtualenv does",
    "crumbs": [
      "1. Preliminaries",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Notes</span>"
    ]
  },
  {
    "objectID": "01_notes.html#mini-forge",
    "href": "01_notes.html#mini-forge",
    "title": "1  Notes",
    "section": "1.12 Mini-forge",
    "text": "1.12 Mini-forge\n\nminiforge is the community (conda-forge) driven minimalistic conda installer. Subsequent package installations come thus from conda-forge channel. Mini-forge\nminiconda is the Anaconda (company) driven minimalistic conda installer. Subsequent package installations come from the anaconda channels (default or otherwise).\nminiforge started because miniconda doens’t support aarch64, very quickly the ‘PyPy’ people jumped on board, and in the mean time there are also miniforge versions for all Linux architectures, as well as MacOS.\nAARCH64, sometimes also referred to as ARM64, is a CPU architecture developed by ARM Ltd., and a 64-bit extension of the pre-existing ARM architecture. ARM architectures are primarily known for their energy efficiency and low power consumption. For that reason, virtually all mobile phones and tablets today use ARM architecture-based CPUs.\nAlthough AARCH64 and x64 (Intel, AMD, …) are both 64-bit CPU architectures, their inner basics are vastly different. Programs compiled for one platform, won’t work on the other (except with some magic), and vice-versa. That means, software does not only need to be recompiled, but often requires extensive optimization for either platform.\n\n\nThe first step is to configure conda-forge as your default package channel by running the following commands in a shell:\n\n\n! conda config --add channels conda-forge\n! conda config --set channel_priority strict\n\nWarning: 'conda-forge' already in 'channels' list, moving to the top\n\n\nNow, we will install the essential packages used throughout the book (along with their dependencies) with conda install\n\nconda create -y -n pydata-book python=3.10 # create enviroment with python 3.10 installed\nconda activate pydata-book # activate enviroment \n(pydata-book) $ conda install -y pandas jupyter matplotlib # install a\n\nInstall complete packages used in the the book\nconda install lxml beautifulsoup4 html5lib openpyxl\nrequests sqlalchemy seaborn scipy statsmodels\npatsy scikit-learn pyarrow pytables numba",
    "crumbs": [
      "1. Preliminaries",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Notes</span>"
    ]
  },
  {
    "objectID": "01_notes.html#should-i-use-pip-or-conda",
    "href": "01_notes.html#should-i-use-pip-or-conda",
    "title": "1  Notes",
    "section": "1.13 Should I use Pip or Conda ?",
    "text": "1.13 Should I use Pip or Conda ?\n\nWhile you can use both conda and pip to install packages, you should avoid updating packages originally installed with conda using pip (and vice versa), as doing so can lead to environment problems. I recommend sticking to conda if you can and falling back on pip only for packages which are unavailable with conda install.\n\n\nconda install should always be preferred, but some packages are not available through conda so if conda install $package_name fails, try pip install $package_name.",
    "crumbs": [
      "1. Preliminaries",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Notes</span>"
    ]
  },
  {
    "objectID": "01_notes.html#what-can-we-do-with-conda",
    "href": "01_notes.html#what-can-we-do-with-conda",
    "title": "1  Notes",
    "section": "1.14 What can we do with Conda?",
    "text": "1.14 What can we do with Conda?\n\nMany commands : create env, activate env, delete env, lists env\nInstall tldr (https://github.com/tldr-pages/tldr) : The tldr-pages project is a collection of community-maintained help pages for command-line tools, that aims to be a simpler, more approachable complement to traditional",
    "crumbs": [
      "1. Preliminaries",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Notes</span>"
    ]
  },
  {
    "objectID": "01_notes.html#navigating-this-book",
    "href": "01_notes.html#navigating-this-book",
    "title": "1  Notes",
    "section": "1.15 Navigating This Book",
    "text": "1.15 Navigating This Book\n\nChapter two and three: provides prerequisite knowledge for the remainder of the book. If you have Python experienc you can skip\n\n\nChapter four : Numpy\n\n\nChapter five : Pandas\n\n\nChapter six : Data loading, Storage and File format\n\n\nChapter seven : Data cleaning and Preparation\n\n\nChpater eight : Data wrangling\n\n\nChapter Nine : Plotting and Visualization\n\n\nChapter 10 : Data agreegation and Group operation",
    "crumbs": [
      "1. Preliminaries",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Notes</span>"
    ]
  },
  {
    "objectID": "01_notes.html#import-conventions",
    "href": "01_notes.html#import-conventions",
    "title": "1  Notes",
    "section": "1.16 Import Conventions",
    "text": "1.16 Import Conventions\n\nThe Python community has adopted a number of naming conventions for commonly used modules:\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport statsmodels as\n\n\n  Input In [1]\n    import statsmodels as\n                         ^\nSyntaxError: invalid syntax\n\n\n\n\n\nThis means that when you see np.arange, this is a reference to the arange function in NumPy. This is done because it’s considered bad practice in Python software development to import everything (from numpy import *) from a large package like NumPy\n\n\nimport numpy as np \n\nx = np.random.random((64, 3, 32, 10)) \ny = np.random.random((32, 10)) \n\nz = np.maximum(x, y)",
    "crumbs": [
      "1. Preliminaries",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Notes</span>"
    ]
  },
  {
    "objectID": "12_notes.html",
    "href": "12_notes.html",
    "title": "3  Notes",
    "section": "",
    "text": "3.1 Introduction to Modeling Libraries in Python",
    "crumbs": [
      "12. Introduction to Modeling Libraries in Python",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Notes</span>"
    ]
  },
  {
    "objectID": "12_notes.html#introduction-to-modeling-libraries-in-python",
    "href": "12_notes.html#introduction-to-modeling-libraries-in-python",
    "title": "3  Notes",
    "section": "",
    "text": "Use different libraries depending on the application",
    "crumbs": [
      "12. Introduction to Modeling Libraries in Python",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Notes</span>"
    ]
  },
  {
    "objectID": "12_notes.html#interfacing-between-pandas-and-model-code",
    "href": "12_notes.html#interfacing-between-pandas-and-model-code",
    "title": "3  Notes",
    "section": "3.2 Interfacing Between pandas and Model Code",
    "text": "3.2 Interfacing Between pandas and Model Code\n\nPandas for data loading and cleaning\nModeling library for building model\n\n\ndf = pd.read_excel(\"data/Life Expectancy at Birth.xlsx\", engine=\"openpyxl\")\ndf.dropna(inplace=True)\ndf.columns = df.columns.map(lambda row: \"_\".join(row.lower().split(\" \")))\nsample = df.groupby(\"region\").sample(n=20).sort_values(by=\"year\")\nsample\n\n\n\n\n\n\n\n\ncountry\ncountry_code\nregion\nincome_group\nyear\nlife_expectancy\n\n\n\n\n187\nPoland\nPOL\nEurope & Central Asia\nHigh income\n1960\n67.680488\n\n\n248\nUnited States\nUSA\nNorth America\nHigh income\n1960\n69.770732\n\n\n444\nPakistan\nPAK\nSouth Asia\nLower middle income\n1961\n46.223220\n\n\n274\nAustralia\nAUS\nEast Asia & Pacific\nHigh income\n1961\n70.973171\n\n\n367\nIndonesia\nIDN\nEast Asia & Pacific\nLower middle income\n1961\n49.269805\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n14254\nCzech Republic\nCZE\nEurope & Central Asia\nHigh income\n2014\n78.824390\n\n\n14368\nMalaysia\nMYS\nEast Asia & Pacific\nUpper middle income\n2014\n74.718293\n\n\n14235\nCanada\nCAN\nNorth America\nHigh income\n2014\n81.956610\n\n\n14381\nOman\nOMN\nMiddle East & North Africa\nHigh income\n2014\n77.085098\n\n\n14509\nComoros\nCOM\nSub-Saharan Africa\nLow income\n2015\n63.554024\n\n\n\n\n140 rows × 6 columns\n\n\n\n\nnumeric_cols = [\"year\", \"life_expectancy\"]\ndf_num = sample[numeric_cols]\ndf_num\n\n\n\n\n\n\n\n\nyear\nlife_expectancy\n\n\n\n\n187\n1960\n67.680488\n\n\n248\n1960\n69.770732\n\n\n444\n1961\n46.223220\n\n\n274\n1961\n70.973171\n\n\n367\n1961\n49.269805\n\n\n...\n...\n...\n\n\n14254\n2014\n78.824390\n\n\n14368\n2014\n74.718293\n\n\n14235\n2014\n81.956610\n\n\n14381\n2014\n77.085098\n\n\n14509\n2015\n63.554024\n\n\n\n\n140 rows × 2 columns\n\n\n\nTurn a DataFrame into a NumPy array, use the to_numpy method:\n\ndf_num = df_num.to_numpy()\n\n\nisinstance(df_num, np.ndarray) # an ndarray of Python objects\n\nTrue\n\n\nTo convert back to a DataFrame, as you may recall from earlier chapters, you can pass a two-dimensional ndarray with optional column names:\n\ndf2 = pd.DataFrame(sample.to_numpy(), columns=['country', 'country_code', 'region', 'income_group', 'year', 'life_expectancy'])\ndf2\n\n\n\n\n\n\n\n\ncountry\ncountry_code\nregion\nincome_group\nyear\nlife_expectancy\n\n\n\n\n0\nPoland\nPOL\nEurope & Central Asia\nHigh income\n1960\n67.680488\n\n\n1\nUnited States\nUSA\nNorth America\nHigh income\n1960\n69.770732\n\n\n2\nPakistan\nPAK\nSouth Asia\nLower middle income\n1961\n46.22322\n\n\n3\nAustralia\nAUS\nEast Asia & Pacific\nHigh income\n1961\n70.973171\n\n\n4\nIndonesia\nIDN\nEast Asia & Pacific\nLower middle income\n1961\n49.269805\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n135\nCzech Republic\nCZE\nEurope & Central Asia\nHigh income\n2014\n78.82439\n\n\n136\nMalaysia\nMYS\nEast Asia & Pacific\nUpper middle income\n2014\n74.718293\n\n\n137\nCanada\nCAN\nNorth America\nHigh income\n2014\n81.95661\n\n\n138\nOman\nOMN\nMiddle East & North Africa\nHigh income\n2014\n77.085098\n\n\n139\nComoros\nCOM\nSub-Saharan Africa\nLow income\n2015\n63.554024\n\n\n\n\n140 rows × 6 columns\n\n\n\n\ndf_mix = sample.to_numpy()\ndf_mix\n\narray([['Poland', 'POL', 'Europe & Central Asia', 'High income', 1960,\n        67.680487805],\n       ['United States', 'USA', 'North America', 'High income', 1960,\n        69.770731707],\n       ['Pakistan', 'PAK', 'South Asia', 'Lower middle income', 1961,\n        46.223219512],\n       ['Australia', 'AUS', 'East Asia & Pacific', 'High income', 1961,\n        70.973170732],\n       ['Indonesia', 'IDN', 'East Asia & Pacific', 'Lower middle income',\n        1961, 49.269804878],\n       ['Kazakhstan', 'KAZ', 'Europe & Central Asia',\n        'Upper middle income', 1962, 59.199073171],\n       ['Bangladesh', 'BGD', 'South Asia', 'Lower middle income', 1962,\n        47.08397561],\n       ['Cuba', 'CUB', 'Latin America & Caribbean',\n        'Upper middle income', 1962, 65.138219512],\n       ['Finland', 'FIN', 'Europe & Central Asia', 'High income', 1962,\n        68.577804878],\n       ['Hungary', 'HUN', 'Europe & Central Asia', 'High income', 1962,\n        67.865853659],\n       ['Azerbaijan', 'AZE', 'Europe & Central Asia',\n        'Upper middle income', 1963, 62.052],\n       ['Nepal', 'NPL', 'South Asia', 'Low income', 1963, 36.425170732],\n       ['Macedonia, FYR', 'MKD', 'Europe & Central Asia',\n        'Upper middle income', 1963, 62.459512195],\n       ['Micronesia, Fed. Sts.', 'FSM', 'East Asia & Pacific',\n        'Lower middle income', 1963, 58.781585366],\n       ['Tunisia', 'TUN', 'Middle East & North Africa',\n        'Lower middle income', 1963, 44.100780488],\n       ['Antigua and Barbuda', 'ATG', 'Latin America & Caribbean',\n        'High income', 1963, 62.992585366],\n       ['Maldives', 'MDV', 'South Asia', 'Upper middle income', 1964,\n        39.806902439],\n       ['Saudi Arabia', 'SAU', 'Middle East & North Africa',\n        'High income', 1964, 47.811268293],\n       ['Sao Tome and Principe', 'STP', 'Sub-Saharan Africa',\n        'Lower middle income', 1964, 52.674756098],\n       ['Maldives', 'MDV', 'South Asia', 'Upper middle income', 1965,\n        40.49602439],\n       ['Lebanon', 'LBN', 'Middle East & North Africa',\n        'Upper middle income', 1965, 64.715],\n       ['Canada', 'CAN', 'North America', 'High income', 1967,\n        72.207804878],\n       ['Lao PDR', 'LAO', 'East Asia & Pacific', 'Lower middle income',\n        1967, 45.316878049],\n       ['Egypt, Arab Rep.', 'EGY', 'Middle East & North Africa',\n        'Lower middle income', 1968, 51.568682927],\n       ['Iran, Islamic Rep.', 'IRN', 'Middle East & North Africa',\n        'Upper middle income', 1969, 50.158341463],\n       ['Samoa', 'WSM', 'East Asia & Pacific', 'Upper middle income',\n        1970, 54.969512195],\n       ['Botswana', 'BWA', 'Sub-Saharan Africa', 'Upper middle income',\n        1970, 54.443463415],\n       ['Nepal', 'NPL', 'South Asia', 'Low income', 1970, 40.504439024],\n       ['Bermuda', 'BMU', 'North America', 'High income', 1970, 70.29],\n       ['Mexico', 'MEX', 'Latin America & Caribbean',\n        'Upper middle income', 1971, 61.819170732],\n       ['Trinidad and Tobago', 'TTO', 'Latin America & Caribbean',\n        'High income', 1971, 65.260390244],\n       ['St. Lucia', 'LCA', 'Latin America & Caribbean',\n        'Upper middle income', 1971, 63.443170732],\n       ['Sudan', 'SDN', 'Sub-Saharan Africa', 'Lower middle income',\n        1971, 52.570146341],\n       ['Nepal', 'NPL', 'South Asia', 'Low income', 1971, 41.088146341],\n       ['Canada', 'CAN', 'North America', 'High income', 1972,\n        72.933902439],\n       ['Swaziland', 'SWZ', 'Sub-Saharan Africa', 'Lower middle income',\n        1972, 49.151121951],\n       ['Bhutan', 'BTN', 'South Asia', 'Lower middle income', 1973,\n        39.287487805],\n       ['Peru', 'PER', 'Latin America & Caribbean',\n        'Upper middle income', 1973, 55.758],\n       ['United States', 'USA', 'North America', 'High income', 1974,\n        71.956097561],\n       ['Romania', 'ROU', 'Europe & Central Asia', 'Upper middle income',\n        1974, 69.499756098],\n       ['New Caledonia', 'NCL', 'East Asia & Pacific', 'High income',\n        1975, 65.273170732],\n       ['Uruguay', 'URY', 'Latin America & Caribbean', 'High income',\n        1975, 69.142829268],\n       ['Papua New Guinea', 'PNG', 'East Asia & Pacific',\n        'Lower middle income', 1975, 49.270512195],\n       ['United States', 'USA', 'North America', 'High income', 1976,\n        72.856097561],\n       ['Afghanistan', 'AFG', 'South Asia', 'Low income', 1976,\n        39.575707317],\n       ['Bhutan', 'BTN', 'South Asia', 'Lower middle income', 1977,\n        42.600829268],\n       ['Lao PDR', 'LAO', 'East Asia & Pacific', 'Lower middle income',\n        1977, 48.208390244],\n       ['Dominican Republic', 'DOM', 'Latin America & Caribbean',\n        'Upper middle income', 1977, 61.878902439],\n       ['Guatemala', 'GTM', 'Latin America & Caribbean',\n        'Lower middle income', 1978, 56.391512195],\n       ['Ireland', 'IRL', 'Europe & Central Asia', 'High income', 1978,\n        72.104756098],\n       ['Ecuador', 'ECU', 'Latin America & Caribbean',\n        'Upper middle income', 1978, 61.925195122],\n       ['Ecuador', 'ECU', 'Latin America & Caribbean',\n        'Upper middle income', 1979, 62.506902439],\n       ['United States', 'USA', 'North America', 'High income', 1979,\n        73.804878049],\n       ['Sudan', 'SDN', 'Sub-Saharan Africa', 'Lower middle income',\n        1979, 54.149073171],\n       ['Bangladesh', 'BGD', 'South Asia', 'Lower middle income', 1979,\n        52.887292683],\n       ['Egypt, Arab Rep.', 'EGY', 'Middle East & North Africa',\n        'Lower middle income', 1979, 57.641365854],\n       ['Cyprus', 'CYP', 'Europe & Central Asia', 'High income', 1980,\n        74.756804878],\n       ['Qatar', 'QAT', 'Middle East & North Africa', 'High income',\n        1980, 72.791658537],\n       ['Bermuda', 'BMU', 'North America', 'High income', 1980,\n        72.304634146],\n       ['Puerto Rico', 'PRI', 'Latin America & Caribbean', 'High income',\n        1980, 73.702292683],\n       ['Myanmar', 'MMR', 'East Asia & Pacific', 'Lower middle income',\n        1981, 55.334390244],\n       ['Qatar', 'QAT', 'Middle East & North Africa', 'High income',\n        1981, 73.087365854],\n       ['St. Martin (French part)', 'MAF', 'Latin America & Caribbean',\n        'High income', 1982, 73.219512195],\n       ['Kenya', 'KEN', 'Sub-Saharan Africa', 'Lower middle income',\n        1982, 58.799756098],\n       ['Greenland', 'GRL', 'Europe & Central Asia', 'High income', 1982,\n        63.192682927],\n       ['Indonesia', 'IDN', 'East Asia & Pacific', 'Lower middle income',\n        1983, 60.82295122],\n       ['Algeria', 'DZA', 'Middle East & North Africa',\n        'Upper middle income', 1984, 63.117121951],\n       ['Central African Republic', 'CAF', 'Sub-Saharan Africa',\n        'Low income', 1984, 49.825634146],\n       ['New Caledonia', 'NCL', 'East Asia & Pacific', 'High income',\n        1984, 68.063414634],\n       ['Somalia', 'SOM', 'Sub-Saharan Africa', 'Low income', 1984,\n        45.984365854],\n       ['France', 'FRA', 'Europe & Central Asia', 'High income', 1984,\n        75.0],\n       ['Lao PDR', 'LAO', 'East Asia & Pacific', 'Lower middle income',\n        1984, 50.544243902],\n       ['India', 'IND', 'South Asia', 'Lower middle income', 1985,\n        55.860902439],\n       ['Philippines', 'PHL', 'East Asia & Pacific',\n        'Lower middle income', 1985, 63.798560976],\n       ['Kosovo', 'XKX', 'Europe & Central Asia', 'Lower middle income',\n        1985, 66.797560976],\n       ['United Arab Emirates', 'ARE', 'Middle East & North Africa',\n        'High income', 1985, 70.076634146],\n       ['Uzbekistan', 'UZB', 'Europe & Central Asia',\n        'Lower middle income', 1986, 66.942439024],\n       ['United States', 'USA', 'North America', 'High income', 1987,\n        74.765853659],\n       ['South Sudan', 'SSD', 'Sub-Saharan Africa', 'Low income', 1987,\n        41.50802439],\n       ['Bhutan', 'BTN', 'South Asia', 'Lower middle income', 1988,\n        50.898487805],\n       ['Canada', 'CAN', 'North America', 'High income', 1988,\n        76.809268293],\n       ['Niger', 'NER', 'Sub-Saharan Africa', 'Low income', 1988,\n        42.962073171],\n       ['Tunisia', 'TUN', 'Middle East & North Africa',\n        'Lower middle income', 1988, 67.607902439],\n       ['Guatemala', 'GTM', 'Latin America & Caribbean',\n        'Lower middle income', 1989, 61.704658537],\n       ['Sri Lanka', 'LKA', 'South Asia', 'Lower middle income', 1989,\n        69.534341463],\n       ['Uruguay', 'URY', 'Latin America & Caribbean', 'High income',\n        1990, 72.539536585],\n       ['United States', 'USA', 'North America', 'High income', 1990,\n        75.214634146],\n       ['United States', 'USA', 'North America', 'High income', 1991,\n        75.365853659],\n       ['Saudi Arabia', 'SAU', 'Middle East & North Africa',\n        'High income', 1991, 69.592195122],\n       ['Philippines', 'PHL', 'East Asia & Pacific',\n        'Lower middle income', 1991, 65.481268293],\n       ['Syrian Arab Republic', 'SYR', 'Middle East & North Africa',\n        'Lower middle income', 1991, 70.396],\n       ['Libya', 'LBY', 'Middle East & North Africa',\n        'Upper middle income', 1992, 69.275073171],\n       ['Albania', 'ALB', 'Europe & Central Asia', 'Upper middle income',\n        1992, 71.900804878],\n       ['Bangladesh', 'BGD', 'South Asia', 'Lower middle income', 1993,\n        60.418804878],\n       ['United States', 'USA', 'North America', 'High income', 1993,\n        75.419512195],\n       ['Cameroon', 'CMR', 'Sub-Saharan Africa', 'Lower middle income',\n        1993, 53.524829268],\n       ['Aruba', 'ABW', 'Latin America & Caribbean', 'High income', 1994,\n        73.535756098],\n       ['Ghana', 'GHA', 'Sub-Saharan Africa', 'Lower middle income',\n        1994, 57.594829268],\n       ['Canada', 'CAN', 'North America', 'High income', 1994,\n        77.86195122],\n       ['Pakistan', 'PAK', 'South Asia', 'Lower middle income', 1995,\n        61.485292683],\n       ['Rwanda', 'RWA', 'Sub-Saharan Africa', 'Low income', 1995,\n        31.634512195],\n       ['Somalia', 'SOM', 'Sub-Saharan Africa', 'Low income', 1996,\n        48.077609756],\n       ['Sierra Leone', 'SLE', 'Sub-Saharan Africa', 'Low income', 1998,\n        37.046804878],\n       ['United Arab Emirates', 'ARE', 'Middle East & North Africa',\n        'High income', 1998, 73.920219512],\n       ['Oman', 'OMN', 'Middle East & North Africa', 'High income', 1999,\n        71.910390244],\n       ['Virgin Islands (U.S.)', 'VIR', 'Latin America & Caribbean',\n        'High income', 1999, 77.306170732],\n       ['Spain', 'ESP', 'Europe & Central Asia', 'High income', 1999,\n        78.717073171],\n       ['Swaziland', 'SWZ', 'Sub-Saharan Africa', 'Lower middle income',\n        2001, 47.434097561],\n       ['Spain', 'ESP', 'Europe & Central Asia', 'High income', 2001,\n        79.368292683],\n       ['United Arab Emirates', 'ARE', 'Middle East & North Africa',\n        'High income', 2003, 75.217219512],\n       ['United States', 'USA', 'North America', 'High income', 2004,\n        77.487804878],\n       ['Lebanon', 'LBN', 'Middle East & North Africa',\n        'Upper middle income', 2006, 77.246707317],\n       ['Indonesia', 'IDN', 'East Asia & Pacific', 'Lower middle income',\n        2006, 67.367487805],\n       ['Kazakhstan', 'KAZ', 'Europe & Central Asia',\n        'Upper middle income', 2006, 66.16097561],\n       ['Canada', 'CAN', 'North America', 'High income', 2006,\n        80.292682927],\n       ['Mauritania', 'MRT', 'Sub-Saharan Africa', 'Lower middle income',\n        2007, 61.139731707],\n       ['Puerto Rico', 'PRI', 'Latin America & Caribbean', 'High income',\n        2008, 77.906463415],\n       ['Togo', 'TGO', 'Sub-Saharan Africa', 'Low income', 2008,\n        56.005902439],\n       ['Bangladesh', 'BGD', 'South Asia', 'Lower middle income', 2008,\n        69.277853659],\n       ['Qatar', 'QAT', 'Middle East & North Africa', 'High income',\n        2008, 77.448756098],\n       ['Hungary', 'HUN', 'Europe & Central Asia', 'High income', 2008,\n        73.702439024],\n       ['Fiji', 'FJI', 'East Asia & Pacific', 'Upper middle income',\n        2009, 69.202853659],\n       ['Aruba', 'ABW', 'Latin America & Caribbean', 'High income', 2009,\n        74.818146341],\n       ['Malaysia', 'MYS', 'East Asia & Pacific', 'Upper middle income',\n        2009, 74.038414634],\n       ['Canada', 'CAN', 'North America', 'High income', 2010,\n        81.197560976],\n       ['Nicaragua', 'NIC', 'Latin America & Caribbean',\n        'Lower middle income', 2010, 73.581731707],\n       ['United States', 'USA', 'North America', 'High income', 2011,\n        78.641463415],\n       ['Bangladesh', 'BGD', 'South Asia', 'Lower middle income', 2011,\n        70.47195122],\n       ['Seychelles', 'SYC', 'Sub-Saharan Africa', 'High income', 2011,\n        72.724390244],\n       ['Ireland', 'IRL', 'Europe & Central Asia', 'High income', 2012,\n        80.846341463],\n       ['India', 'IND', 'South Asia', 'Lower middle income', 2012,\n        67.289878049],\n       ['New Zealand', 'NZL', 'East Asia & Pacific', 'High income', 2013,\n        81.407317073],\n       ['Bermuda', 'BMU', 'North America', 'High income', 2013,\n        80.572439024],\n       ['Myanmar', 'MMR', 'East Asia & Pacific', 'Lower middle income',\n        2014, 65.857853659],\n       ['Pakistan', 'PAK', 'South Asia', 'Lower middle income', 2014,\n        66.183365854],\n       ['Czech Republic', 'CZE', 'Europe & Central Asia', 'High income',\n        2014, 78.824390244],\n       ['Malaysia', 'MYS', 'East Asia & Pacific', 'Upper middle income',\n        2014, 74.718292683],\n       ['Canada', 'CAN', 'North America', 'High income', 2014,\n        81.956609756],\n       ['Oman', 'OMN', 'Middle East & North Africa', 'High income', 2014,\n        77.085097561],\n       ['Comoros', 'COM', 'Sub-Saharan Africa', 'Low income', 2015,\n        63.55402439]], dtype=object)\n\n\n\nisinstance(df_mix, np.ndarray) # an ndarray of Python objects\n\nTrue\n\n\nUse a subset by using loc indexing with to_numpy:\n\nsample.loc[:, numeric_cols].to_numpy()\n\narray([[1960.        ,   67.6804878 ],\n       [1960.        ,   69.77073171],\n       [1961.        ,   46.22321951],\n       [1961.        ,   70.97317073],\n       [1961.        ,   49.26980488],\n       [1962.        ,   59.19907317],\n       [1962.        ,   47.08397561],\n       [1962.        ,   65.13821951],\n       [1962.        ,   68.57780488],\n       [1962.        ,   67.86585366],\n       [1963.        ,   62.052     ],\n       [1963.        ,   36.42517073],\n       [1963.        ,   62.4595122 ],\n       [1963.        ,   58.78158537],\n       [1963.        ,   44.10078049],\n       [1963.        ,   62.99258537],\n       [1964.        ,   39.80690244],\n       [1964.        ,   47.81126829],\n       [1964.        ,   52.6747561 ],\n       [1965.        ,   40.49602439],\n       [1965.        ,   64.715     ],\n       [1967.        ,   72.20780488],\n       [1967.        ,   45.31687805],\n       [1968.        ,   51.56868293],\n       [1969.        ,   50.15834146],\n       [1970.        ,   54.9695122 ],\n       [1970.        ,   54.44346341],\n       [1970.        ,   40.50443902],\n       [1970.        ,   70.29      ],\n       [1971.        ,   61.81917073],\n       [1971.        ,   65.26039024],\n       [1971.        ,   63.44317073],\n       [1971.        ,   52.57014634],\n       [1971.        ,   41.08814634],\n       [1972.        ,   72.93390244],\n       [1972.        ,   49.15112195],\n       [1973.        ,   39.2874878 ],\n       [1973.        ,   55.758     ],\n       [1974.        ,   71.95609756],\n       [1974.        ,   69.4997561 ],\n       [1975.        ,   65.27317073],\n       [1975.        ,   69.14282927],\n       [1975.        ,   49.2705122 ],\n       [1976.        ,   72.85609756],\n       [1976.        ,   39.57570732],\n       [1977.        ,   42.60082927],\n       [1977.        ,   48.20839024],\n       [1977.        ,   61.87890244],\n       [1978.        ,   56.39151219],\n       [1978.        ,   72.1047561 ],\n       [1978.        ,   61.92519512],\n       [1979.        ,   62.50690244],\n       [1979.        ,   73.80487805],\n       [1979.        ,   54.14907317],\n       [1979.        ,   52.88729268],\n       [1979.        ,   57.64136585],\n       [1980.        ,   74.75680488],\n       [1980.        ,   72.79165854],\n       [1980.        ,   72.30463415],\n       [1980.        ,   73.70229268],\n       [1981.        ,   55.33439024],\n       [1981.        ,   73.08736585],\n       [1982.        ,   73.21951219],\n       [1982.        ,   58.7997561 ],\n       [1982.        ,   63.19268293],\n       [1983.        ,   60.82295122],\n       [1984.        ,   63.11712195],\n       [1984.        ,   49.82563415],\n       [1984.        ,   68.06341463],\n       [1984.        ,   45.98436585],\n       [1984.        ,   75.        ],\n       [1984.        ,   50.5442439 ],\n       [1985.        ,   55.86090244],\n       [1985.        ,   63.79856098],\n       [1985.        ,   66.79756098],\n       [1985.        ,   70.07663415],\n       [1986.        ,   66.94243902],\n       [1987.        ,   74.76585366],\n       [1987.        ,   41.50802439],\n       [1988.        ,   50.89848781],\n       [1988.        ,   76.80926829],\n       [1988.        ,   42.96207317],\n       [1988.        ,   67.60790244],\n       [1989.        ,   61.70465854],\n       [1989.        ,   69.53434146],\n       [1990.        ,   72.53953658],\n       [1990.        ,   75.21463415],\n       [1991.        ,   75.36585366],\n       [1991.        ,   69.59219512],\n       [1991.        ,   65.48126829],\n       [1991.        ,   70.396     ],\n       [1992.        ,   69.27507317],\n       [1992.        ,   71.90080488],\n       [1993.        ,   60.41880488],\n       [1993.        ,   75.41951219],\n       [1993.        ,   53.52482927],\n       [1994.        ,   73.5357561 ],\n       [1994.        ,   57.59482927],\n       [1994.        ,   77.86195122],\n       [1995.        ,   61.48529268],\n       [1995.        ,   31.63451219],\n       [1996.        ,   48.07760976],\n       [1998.        ,   37.04680488],\n       [1998.        ,   73.92021951],\n       [1999.        ,   71.91039024],\n       [1999.        ,   77.30617073],\n       [1999.        ,   78.71707317],\n       [2001.        ,   47.43409756],\n       [2001.        ,   79.36829268],\n       [2003.        ,   75.21721951],\n       [2004.        ,   77.48780488],\n       [2006.        ,   77.24670732],\n       [2006.        ,   67.3674878 ],\n       [2006.        ,   66.16097561],\n       [2006.        ,   80.29268293],\n       [2007.        ,   61.13973171],\n       [2008.        ,   77.90646342],\n       [2008.        ,   56.00590244],\n       [2008.        ,   69.27785366],\n       [2008.        ,   77.4487561 ],\n       [2008.        ,   73.70243902],\n       [2009.        ,   69.20285366],\n       [2009.        ,   74.81814634],\n       [2009.        ,   74.03841463],\n       [2010.        ,   81.19756098],\n       [2010.        ,   73.58173171],\n       [2011.        ,   78.64146342],\n       [2011.        ,   70.47195122],\n       [2011.        ,   72.72439024],\n       [2012.        ,   80.84634146],\n       [2012.        ,   67.28987805],\n       [2013.        ,   81.40731707],\n       [2013.        ,   80.57243902],\n       [2014.        ,   65.85785366],\n       [2014.        ,   66.18336585],\n       [2014.        ,   78.82439024],\n       [2014.        ,   74.71829268],\n       [2014.        ,   81.95660976],\n       [2014.        ,   77.08509756],\n       [2015.        ,   63.55402439]])\n\n\nThere is a nonnumeric column in our example dataset. Let’s convert income_group to dummy variables. But this can be error prone.\n\ndummies = pd.get_dummies(sample.income_group, prefix='income_group')\ndata_with_dummies = sample.drop('income_group', axis=1).join(dummies)\ndata_with_dummies\n\n\n\n\n\n\n\n\ncountry\ncountry_code\nregion\nyear\nlife_expectancy\nincome_group_High income\nincome_group_Low income\nincome_group_Lower middle income\nincome_group_Upper middle income\n\n\n\n\n187\nPoland\nPOL\nEurope & Central Asia\n1960\n67.680488\n1\n0\n0\n0\n\n\n248\nUnited States\nUSA\nNorth America\n1960\n69.770732\n1\n0\n0\n0\n\n\n444\nPakistan\nPAK\nSouth Asia\n1961\n46.223220\n0\n0\n1\n0\n\n\n274\nAustralia\nAUS\nEast Asia & Pacific\n1961\n70.973171\n1\n0\n0\n0\n\n\n367\nIndonesia\nIDN\nEast Asia & Pacific\n1961\n49.269805\n0\n0\n1\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n14254\nCzech Republic\nCZE\nEurope & Central Asia\n2014\n78.824390\n1\n0\n0\n0\n\n\n14368\nMalaysia\nMYS\nEast Asia & Pacific\n2014\n74.718293\n0\n0\n0\n1\n\n\n14235\nCanada\nCAN\nNorth America\n2014\n81.956610\n1\n0\n0\n0\n\n\n14381\nOman\nOMN\nMiddle East & North Africa\n2014\n77.085098\n1\n0\n0\n0\n\n\n14509\nComoros\nCOM\nSub-Saharan Africa\n2015\n63.554024\n0\n1\n0\n0\n\n\n\n\n140 rows × 9 columns",
    "crumbs": [
      "12. Introduction to Modeling Libraries in Python",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Notes</span>"
    ]
  },
  {
    "objectID": "12_notes.html#creating-model-descriptions-with-patsy",
    "href": "12_notes.html#creating-model-descriptions-with-patsy",
    "title": "3  Notes",
    "section": "3.3 Creating Model Descriptions with Patsy",
    "text": "3.3 Creating Model Descriptions with Patsy\nPatsy is a Python package that allows data transformations using arbitrary Python code.\nPatsy describes statistical models with a string-based “formula syntax”:\ny ~ x0 + x1\nwhere x0, x1 are terms in the design matrix created for the model.\n\noutcome, predictors = patsy.dmatrices(\n    'life_expectancy ~ income_group + region + year', data=sample\n)\n\noutcome is a DesignMatrix object that represents life_expectancy which is the variable we want to predict.\n\noutcome\n\nDesignMatrix with shape (140, 1)\n  life_expectancy\n         67.68049\n         69.77073\n         46.22322\n         70.97317\n         49.26980\n         59.19907\n         47.08398\n         65.13822\n         68.57780\n         67.86585\n         62.05200\n         36.42517\n         62.45951\n         58.78159\n         44.10078\n         62.99259\n         39.80690\n         47.81127\n         52.67476\n         40.49602\n         64.71500\n         72.20780\n         45.31688\n         51.56868\n         50.15834\n         54.96951\n         54.44346\n         40.50444\n         70.29000\n         61.81917\n  [110 rows omitted]\n  Terms:\n    'life_expectancy' (column 0)\n  (to view full data, use np.asarray(this_obj))\n\n\npredictors is a DesignMatrix object that represents the combination of income_group, region, year.\nWe have 140 examples, 11 features:\n\nIntercept (column 0): an array of 1s\nincome group: columns 1 - 4\nregion: columns 4-10\nyear: columns 10\n\nThe extra columns for income_group and regions are from One-Hot encoding, a process by which categorical variables are converted into a form that could be provided to ML algorithms.\n\npredictors\n\nDesignMatrix with shape (140, 11)\n  Columns:\n    ['Intercept',\n     'income_group[T.Low income]',\n     'income_group[T.Lower middle income]',\n     'income_group[T.Upper middle income]',\n     'region[T.Europe & Central Asia]',\n     'region[T.Latin America & Caribbean]',\n     'region[T.Middle East & North Africa]',\n     'region[T.North America]',\n     'region[T.South Asia]',\n     'region[T.Sub-Saharan Africa]',\n     'year']\n  Terms:\n    'Intercept' (column 0)\n    'income_group' (columns 1:4)\n    'region' (columns 4:10)\n    'year' (column 10)\n  (to view full data, use np.asarray(this_obj))\n\n\nThese Patsy DesignMatrix instances are NumPy ndarrays with additional metadata:\n\nnp.asarray(predictors)\n\narray([[1.000e+00, 0.000e+00, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n        1.960e+03],\n       [1.000e+00, 0.000e+00, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n        1.960e+03],\n       [1.000e+00, 0.000e+00, 1.000e+00, ..., 1.000e+00, 0.000e+00,\n        1.961e+03],\n       ...,\n       [1.000e+00, 0.000e+00, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n        2.014e+03],\n       [1.000e+00, 0.000e+00, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n        2.014e+03],\n       [1.000e+00, 1.000e+00, 0.000e+00, ..., 0.000e+00, 1.000e+00,\n        2.015e+03]])\n\n\n\nnp.asarray(outcome)\n\narray([[67.6804878 ],\n       [69.77073171],\n       [46.22321951],\n       [70.97317073],\n       [49.26980488],\n       [59.19907317],\n       [47.08397561],\n       [65.13821951],\n       [68.57780488],\n       [67.86585366],\n       [62.052     ],\n       [36.42517073],\n       [62.4595122 ],\n       [58.78158537],\n       [44.10078049],\n       [62.99258537],\n       [39.80690244],\n       [47.81126829],\n       [52.6747561 ],\n       [40.49602439],\n       [64.715     ],\n       [72.20780488],\n       [45.31687805],\n       [51.56868293],\n       [50.15834146],\n       [54.9695122 ],\n       [54.44346341],\n       [40.50443902],\n       [70.29      ],\n       [61.81917073],\n       [65.26039024],\n       [63.44317073],\n       [52.57014634],\n       [41.08814634],\n       [72.93390244],\n       [49.15112195],\n       [39.2874878 ],\n       [55.758     ],\n       [71.95609756],\n       [69.4997561 ],\n       [65.27317073],\n       [69.14282927],\n       [49.2705122 ],\n       [72.85609756],\n       [39.57570732],\n       [42.60082927],\n       [48.20839024],\n       [61.87890244],\n       [56.39151219],\n       [72.1047561 ],\n       [61.92519512],\n       [62.50690244],\n       [73.80487805],\n       [54.14907317],\n       [52.88729268],\n       [57.64136585],\n       [74.75680488],\n       [72.79165854],\n       [72.30463415],\n       [73.70229268],\n       [55.33439024],\n       [73.08736585],\n       [73.21951219],\n       [58.7997561 ],\n       [63.19268293],\n       [60.82295122],\n       [63.11712195],\n       [49.82563415],\n       [68.06341463],\n       [45.98436585],\n       [75.        ],\n       [50.5442439 ],\n       [55.86090244],\n       [63.79856098],\n       [66.79756098],\n       [70.07663415],\n       [66.94243902],\n       [74.76585366],\n       [41.50802439],\n       [50.89848781],\n       [76.80926829],\n       [42.96207317],\n       [67.60790244],\n       [61.70465854],\n       [69.53434146],\n       [72.53953658],\n       [75.21463415],\n       [75.36585366],\n       [69.59219512],\n       [65.48126829],\n       [70.396     ],\n       [69.27507317],\n       [71.90080488],\n       [60.41880488],\n       [75.41951219],\n       [53.52482927],\n       [73.5357561 ],\n       [57.59482927],\n       [77.86195122],\n       [61.48529268],\n       [31.63451219],\n       [48.07760976],\n       [37.04680488],\n       [73.92021951],\n       [71.91039024],\n       [77.30617073],\n       [78.71707317],\n       [47.43409756],\n       [79.36829268],\n       [75.21721951],\n       [77.48780488],\n       [77.24670732],\n       [67.3674878 ],\n       [66.16097561],\n       [80.29268293],\n       [61.13973171],\n       [77.90646342],\n       [56.00590244],\n       [69.27785366],\n       [77.4487561 ],\n       [73.70243902],\n       [69.20285366],\n       [74.81814634],\n       [74.03841463],\n       [81.19756098],\n       [73.58173171],\n       [78.64146342],\n       [70.47195122],\n       [72.72439024],\n       [80.84634146],\n       [67.28987805],\n       [81.40731707],\n       [80.57243902],\n       [65.85785366],\n       [66.18336585],\n       [78.82439024],\n       [74.71829268],\n       [81.95660976],\n       [77.08509756],\n       [63.55402439]])\n\n\nSuppress the intercept by adding the term + 0 to the model:\n\ny, X = patsy.dmatrices(\n    'life_expectancy ~ income_group + region + year + 0', data=sample\n)\nX\n\nDesignMatrix with shape (140, 11)\n  Columns:\n    ['income_group[High income]',\n     'income_group[Low income]',\n     'income_group[Lower middle income]',\n     'income_group[Upper middle income]',\n     'region[T.Europe & Central Asia]',\n     'region[T.Latin America & Caribbean]',\n     'region[T.Middle East & North Africa]',\n     'region[T.North America]',\n     'region[T.South Asia]',\n     'region[T.Sub-Saharan Africa]',\n     'year']\n  Terms:\n    'income_group' (columns 0:4)\n    'region' (columns 4:10)\n    'year' (column 10)\n  (to view full data, use np.asarray(this_obj))\n\n\nThe Patsy objects can be passed directly into algorithms like numpy.linalg.lstsq, which performs an ordinary least squares regression. The model metadata is retained in the design_info attribute:\n\ncoef, resid, _, _ = np.linalg.lstsq(outcome, predictors)\ncoef\n\n/tmp/ipykernel_133/2774571045.py:1: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\nTo use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n  coef, resid, _, _ = np.linalg.lstsq(outcome, predictors)\n\n\narray([[1.51964589e-02, 9.81119953e-04, 4.41412174e-03, 2.65860943e-03,\n        2.40182610e-03, 2.29746257e-03, 2.26362465e-03, 2.58305159e-03,\n        1.77260076e-03, 1.76133204e-03, 3.02004208e+01]])\n\n\nYou can reattach the model column names to the fitted coefficients to obtain a Series, for example:\n\ncoef = pd.Series(coef.squeeze(), index=outcome.design_info.column_names)\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nInput In [23], in &lt;cell line: 1&gt;()\n----&gt; 1 coef = pd.Series(coef.squeeze(), index=outcome.design_info.column_names)\n\nFile /cloud/lib/lib/python3.9/site-packages/pandas/core/series.py:461, in Series.__init__(self, data, index, dtype, name, copy, fastpath)\n    459     index = default_index(len(data))\n    460 elif is_list_like(data):\n--&gt; 461     com.require_length_match(data, index)\n    463 # create/copy the manager\n    464 if isinstance(data, (SingleBlockManager, SingleArrayManager)):\n\nFile /cloud/lib/lib/python3.9/site-packages/pandas/core/common.py:561, in require_length_match(data, index)\n    557 \"\"\"\n    558 Check the length of data matches the length of the index.\n    559 \"\"\"\n    560 if len(data) != len(index):\n--&gt; 561     raise ValueError(\n    562         \"Length of values \"\n    563         f\"({len(data)}) \"\n    564         \"does not match length of index \"\n    565         f\"({len(index)})\"\n    566     )\n\nValueError: Length of values (11) does not match length of index (1)\n\n\n\n\n3.3.1 Data Transformations in Patsy Formulas\nYou can mix Python code into your Patsy formulas:\n\noutcome, predictors = patsy.dmatrices(\n    'np.round(life_expectancy) ~ income_group + region + year', data=sample\n)\n\noutcome\n\nDesignMatrix with shape (140, 1)\n  np.round(life_expectancy)\n                         68\n                         70\n                         46\n                         71\n                         49\n                         59\n                         47\n                         65\n                         69\n                         68\n                         62\n                         36\n                         62\n                         59\n                         44\n                         63\n                         40\n                         48\n                         53\n                         40\n                         65\n                         72\n                         45\n                         52\n                         50\n                         55\n                         54\n                         41\n                         70\n                         62\n  [110 rows omitted]\n  Terms:\n    'np.round(life_expectancy)' (column 0)\n  (to view full data, use np.asarray(this_obj))\n\n\nSome commonly used variable transformations include standardizing (to mean 0 and variance 1) and centering (subtracting the mean). Patsy has built-in functions for this purpose:\n\noutcome, predictors = patsy.dmatrices(\n    'life_expectancy ~ income_group + region + center(year)', data=sample\n)\n\npredictors\n\nDesignMatrix with shape (140, 11)\n  Columns:\n    ['Intercept',\n     'income_group[T.Low income]',\n     'income_group[T.Lower middle income]',\n     'income_group[T.Upper middle income]',\n     'region[T.Europe & Central Asia]',\n     'region[T.Latin America & Caribbean]',\n     'region[T.Middle East & North Africa]',\n     'region[T.North America]',\n     'region[T.South Asia]',\n     'region[T.Sub-Saharan Africa]',\n     'center(year)']\n  Terms:\n    'Intercept' (column 0)\n    'income_group' (columns 1:4)\n    'region' (columns 4:10)\n    'center(year)' (column 10)\n  (to view full data, use np.asarray(this_obj))\n\n\nThe patsy.build_design_matrices function can apply transformations to new out-of-sample data using the saved information from the original in-sample dataset:\n\noutcome, predictors = patsy.dmatrices(\n    'life_expectancy ~ income_group + region + center(year)', data=sample\n)\n\nnew_df = df.groupby(\"region\").sample(n=20).sort_values(by=\"year\")\n\nnew_predictors = patsy.build_design_matrices([predictors.design_info], new_df)\n\nnew_predictors\n\n[DesignMatrix with shape (140, 11)\n   Columns:\n     ['Intercept',\n      'income_group[T.Low income]',\n      'income_group[T.Lower middle income]',\n      'income_group[T.Upper middle income]',\n      'region[T.Europe & Central Asia]',\n      'region[T.Latin America & Caribbean]',\n      'region[T.Middle East & North Africa]',\n      'region[T.North America]',\n      'region[T.South Asia]',\n      'region[T.Sub-Saharan Africa]',\n      'center(year)']\n   Terms:\n     'Intercept' (column 0)\n     'income_group' (columns 1:4)\n     'region' (columns 4:10)\n     'center(year)' (column 10)\n   (to view full data, use np.asarray(this_obj))]\n\n\nWhen you want to add columns from a dataset by name, you must wrap them in the special I function:\n\noutcome, predictors = patsy.dmatrices(\n    'life_expectancy ~ income_group + region + I(year + year)', data=sample # nonsense\n)",
    "crumbs": [
      "12. Introduction to Modeling Libraries in Python",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Notes</span>"
    ]
  },
  {
    "objectID": "12_notes.html#categorical-data-and-patsy",
    "href": "12_notes.html#categorical-data-and-patsy",
    "title": "3  Notes",
    "section": "3.4 Categorical Data and Patsy",
    "text": "3.4 Categorical Data and Patsy\nWhen you use nonnumeric terms in a Patsy formula, they are converted to dummy variables by default.\n\noutcome, predictors = patsy.dmatrices(\n    'life_expectancy ~ income_group + region + year', data=sample\n)\n\npredictors\n\nDesignMatrix with shape (140, 11)\n  Columns:\n    ['Intercept',\n     'income_group[T.Low income]',\n     'income_group[T.Lower middle income]',\n     'income_group[T.Upper middle income]',\n     'region[T.Europe & Central Asia]',\n     'region[T.Latin America & Caribbean]',\n     'region[T.Middle East & North Africa]',\n     'region[T.North America]',\n     'region[T.South Asia]',\n     'region[T.Sub-Saharan Africa]',\n     'year']\n  Terms:\n    'Intercept' (column 0)\n    'income_group' (columns 1:4)\n    'region' (columns 4:10)\n    'year' (column 10)\n  (to view full data, use np.asarray(this_obj))\n\n\nIf you omit the intercept from the model, then columns for each category value will be included in the model design matrix:\n\noutcome, predictors = patsy.dmatrices(\n    'life_expectancy ~ region + income_group + year + 0', data=sample # will have all levels but only for the FIRST categorical variable\n)\n\npredictors\n\nDesignMatrix with shape (140, 11)\n  Columns:\n    ['region[East Asia & Pacific]',\n     'region[Europe & Central Asia]',\n     'region[Latin America & Caribbean]',\n     'region[Middle East & North Africa]',\n     'region[North America]',\n     'region[South Asia]',\n     'region[Sub-Saharan Africa]',\n     'income_group[T.Low income]',\n     'income_group[T.Lower middle income]',\n     'income_group[T.Upper middle income]',\n     'year']\n  Terms:\n    'region' (columns 0:7)\n    'income_group' (columns 7:10)\n    'year' (column 10)\n  (to view full data, use np.asarray(this_obj))\n\n\nNumeric columns can be interpreted as categorical with the C function:\n\noutcome, predictors = patsy.dmatrices(\n    'life_expectancy ~ income_group + region + C(year)', data=sample\n)\n\npredictors\n\nDesignMatrix with shape (140, 60)\n  Columns:\n    ['Intercept',\n     'income_group[T.Low income]',\n     'income_group[T.Lower middle income]',\n     'income_group[T.Upper middle income]',\n     'region[T.Europe & Central Asia]',\n     'region[T.Latin America & Caribbean]',\n     'region[T.Middle East & North Africa]',\n     'region[T.North America]',\n     'region[T.South Asia]',\n     'region[T.Sub-Saharan Africa]',\n     'C(year)[T.1961]',\n     'C(year)[T.1962]',\n     'C(year)[T.1963]',\n     'C(year)[T.1964]',\n     'C(year)[T.1965]',\n     'C(year)[T.1967]',\n     'C(year)[T.1968]',\n     'C(year)[T.1969]',\n     'C(year)[T.1970]',\n     'C(year)[T.1971]',\n     'C(year)[T.1972]',\n     'C(year)[T.1973]',\n     'C(year)[T.1974]',\n     'C(year)[T.1975]',\n     'C(year)[T.1976]',\n     'C(year)[T.1977]',\n     'C(year)[T.1978]',\n     'C(year)[T.1979]',\n     'C(year)[T.1980]',\n     'C(year)[T.1981]',\n     'C(year)[T.1982]',\n     'C(year)[T.1983]',\n     'C(year)[T.1984]',\n     'C(year)[T.1985]',\n     'C(year)[T.1986]',\n     'C(year)[T.1987]',\n     'C(year)[T.1988]',\n     'C(year)[T.1989]',\n     'C(year)[T.1990]',\n     'C(year)[T.1991]',\n     'C(year)[T.1992]',\n     'C(year)[T.1993]',\n     'C(year)[T.1994]',\n     'C(year)[T.1995]',\n     'C(year)[T.1996]',\n     'C(year)[T.1998]',\n     'C(year)[T.1999]',\n     'C(year)[T.2001]',\n     'C(year)[T.2003]',\n     'C(year)[T.2004]',\n     'C(year)[T.2006]',\n     'C(year)[T.2007]',\n     'C(year)[T.2008]',\n     'C(year)[T.2009]',\n     'C(year)[T.2010]',\n     'C(year)[T.2011]',\n     'C(year)[T.2012]',\n     'C(year)[T.2013]',\n     'C(year)[T.2014]',\n     'C(year)[T.2015]']\n  Terms:\n    'Intercept' (column 0)\n    'income_group' (columns 1:4)\n    'region' (columns 4:10)\n    'C(year)' (columns 10:60)\n  (to view full data, use np.asarray(this_obj))\n\n\nInclude interaction terms of the form key1:key2:\n\noutcome, predictors = patsy.dmatrices(\n    'life_expectancy ~ income_group + region + year + income_group:region', data=sample\n)\n\npredictors\n\nDesignMatrix with shape (140, 29)\n  Columns:\n    ['Intercept',\n     'income_group[T.Low income]',\n     'income_group[T.Lower middle income]',\n     'income_group[T.Upper middle income]',\n     'region[T.Europe & Central Asia]',\n     'region[T.Latin America & Caribbean]',\n     'region[T.Middle East & North Africa]',\n     'region[T.North America]',\n     'region[T.South Asia]',\n     'region[T.Sub-Saharan Africa]',\n     'income_group[T.Low income]:region[T.Europe & Central Asia]',\n     'income_group[T.Lower middle income]:region[T.Europe & Central Asia]',\n     'income_group[T.Upper middle income]:region[T.Europe & Central Asia]',\n     'income_group[T.Low income]:region[T.Latin America & Caribbean]',\n     'income_group[T.Lower middle income]:region[T.Latin America & Caribbean]',\n     'income_group[T.Upper middle income]:region[T.Latin America & Caribbean]',\n     'income_group[T.Low income]:region[T.Middle East & North Africa]',\n     'income_group[T.Lower middle income]:region[T.Middle East & North Africa]',\n     'income_group[T.Upper middle income]:region[T.Middle East & North Africa]',\n     'income_group[T.Low income]:region[T.North America]',\n     'income_group[T.Lower middle income]:region[T.North America]',\n     'income_group[T.Upper middle income]:region[T.North America]',\n     'income_group[T.Low income]:region[T.South Asia]',\n     'income_group[T.Lower middle income]:region[T.South Asia]',\n     'income_group[T.Upper middle income]:region[T.South Asia]',\n     'income_group[T.Low income]:region[T.Sub-Saharan Africa]',\n     'income_group[T.Lower middle income]:region[T.Sub-Saharan Africa]',\n     'income_group[T.Upper middle income]:region[T.Sub-Saharan Africa]',\n     'year']\n  Terms:\n    'Intercept' (column 0)\n    'income_group' (columns 1:4)\n    'region' (columns 4:10)\n    'income_group:region' (columns 10:28)\n    'year' (column 28)\n  (to view full data, use np.asarray(this_obj))",
    "crumbs": [
      "12. Introduction to Modeling Libraries in Python",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Notes</span>"
    ]
  },
  {
    "objectID": "12_notes.html#introduction-to-statsmodels",
    "href": "12_notes.html#introduction-to-statsmodels",
    "title": "3  Notes",
    "section": "3.5 Introduction to statsmodels",
    "text": "3.5 Introduction to statsmodels\nstatsmodels is a Python library for fitting many kinds of statistical models, performing statistical tests, and data exploration and visualization.\n\nLinear models, generalized linear models, and robust linear models\nLinear mixed effects models\nAnalysis of variance (ANOVA) methods\nTime series processes and state space models\nGeneralized method of moments\n\nThe sm.add_constant function can add an intercept column to an existing matrix:\n\npredictor_model = sm.add_constant(predictors)\npredictor_model\n\nThe sm.OLS class can fit an ordinary least squares linear regression. The model’s fit method returns a regression results object containing estimated model parameters and other diagnostics:\n\noutcome, predictors = patsy.dmatrices(\n    'life_expectancy ~ income_group + region + year', data=sample\n)\n\npredictors_org = np.asarray(predictors)\n\nrng = np.random.default_rng(seed=12345)\n\ndef dnorm(mean, variance, size=1):\n    if isinstance(size, int):\n        size = size,\n    return mean + np.sqrt(variance) * rng.standard_normal(*size)\n\npredictors = np.c_[np.hstack(tuple(predictors_org[:, [2]])),\n                   np.hstack(tuple(predictors_org[:, [0]])),\n                   np.hstack(tuple(predictors_org[:, [1]]))]\n\npredictors = np.asarray(predictors)\n\neps = dnorm(0, 0.1, size=len(sample))\nbeta = dnorm(45,82, size=3)\n\noutcome = np.dot(predictors, beta) + eps\n\n\nX_model = sm.add_constant(X)\n\n\nmodel = sm.OLS(y, X)\n\n\nresults = model.fit()\nprint(results.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:        life_expectancy   R-squared:                       0.835\nModel:                            OLS   Adj. R-squared:                  0.823\nMethod:                 Least Squares   F-statistic:                     65.48\nDate:                Fri, 28 Oct 2022   Prob (F-statistic):           1.19e-45\nTime:                        20:29:40   Log-Likelihood:                -420.62\nNo. Observations:                 140   AIC:                             863.2\nDf Residuals:                     129   BIC:                             895.6\nDf Model:                          10                                         \nCovariance Type:            nonrobust                                         \n========================================================================================================\n                                           coef    std err          t      P&gt;|t|      [0.025      0.975]\n--------------------------------------------------------------------------------------------------------\nincome_group[High income]             -601.0677     53.808    -11.171      0.000    -707.527    -494.608\nincome_group[Low income]              -620.7862     53.651    -11.571      0.000    -726.936    -514.637\nincome_group[Lower middle income]     -609.9033     53.643    -11.370      0.000    -716.038    -503.769\nincome_group[Upper middle income]     -606.9144     53.554    -11.333      0.000    -712.872    -500.957\nregion[T.Europe & Central Asia]          5.3402      1.741      3.066      0.003       1.895       8.786\nregion[T.Latin America & Caribbean]      3.2563      1.712      1.903      0.059      -0.130       6.643\nregion[T.Middle East & North Africa]     1.4021      1.675      0.837      0.404      -1.913       4.717\nregion[T.North America]                  6.2191      1.894      3.283      0.001       2.471       9.967\nregion[T.South Asia]                    -4.7101      1.684     -2.797      0.006      -8.042      -1.378\nregion[T.Sub-Saharan Africa]            -5.0216      1.824     -2.753      0.007      -8.630      -1.413\nyear                                     0.3371      0.027     12.481      0.000       0.284       0.391\n==============================================================================\nOmnibus:                       11.871   Durbin-Watson:                   1.996\nProb(Omnibus):                  0.003   Jarque-Bera (JB):               13.271\nSkew:                          -0.586   Prob(JB):                      0.00131\nKurtosis:                       3.950   Cond. No.                     4.96e+05\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 4.96e+05. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\n\nresults = smf.ols('life_expectancy ~ income_group + region + year', data=sample).fit()\nresults.params\n\nIntercept                              -601.067727\nincome_group[T.Low income]              -19.718427\nincome_group[T.Lower middle income]      -8.835585\nincome_group[T.Upper middle income]      -5.846721\nregion[T.Europe & Central Asia]           5.340173\nregion[T.Latin America & Caribbean]       3.256349\nregion[T.Middle East & North Africa]      1.402143\nregion[T.North America]                   6.219114\nregion[T.South Asia]                     -4.710066\nregion[T.Sub-Saharan Africa]             -5.021556\nyear                                      0.337079\ndtype: float64\n\n\n\nresults.tvalues\n\nIntercept                              -11.170682\nincome_group[T.Low income]              -9.011465\nincome_group[T.Lower middle income]     -6.207760\nincome_group[T.Upper middle income]     -4.309918\nregion[T.Europe & Central Asia]          3.066470\nregion[T.Latin America & Caribbean]      1.902550\nregion[T.Middle East & North Africa]     0.836912\nregion[T.North America]                  3.283279\nregion[T.South Asia]                    -2.797053\nregion[T.Sub-Saharan Africa]            -2.753270\nyear                                    12.480643\ndtype: float64\n\n\n\nresults.predict(sample[:5])\n\n187    64.948099\n248    65.827040\n444    46.399354\n274    59.945005\n367    51.109421\ndtype: float64",
    "crumbs": [
      "12. Introduction to Modeling Libraries in Python",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Notes</span>"
    ]
  },
  {
    "objectID": "12_notes.html#estimating-time-series-processes",
    "href": "12_notes.html#estimating-time-series-processes",
    "title": "3  Notes",
    "section": "3.6 Estimating Time Series Processes",
    "text": "3.6 Estimating Time Series Processes\nAnother class of models in statsmodels is for time series analysis.\n\ninit_x = 4\n\nvalues = [init_x, init_x]\nN = 1000\n\n# AR(2) structure with two lags\nb0 = 0.8\nb1 = -0.4\nnoise = dnorm(0, 0.1, N)\nfor i in range(N):\n    new_x = values[-1] * b0 + values[-2] * b1 + noise[i]\n    values.append(new_x)\n    \n# fit with more lags\n\nMAXLAGS = 5\nmodel = AutoReg(values, MAXLAGS)\nresults = model.fit()\nresults.params\n\narray([ 0.01141668,  0.8278958 , -0.42492365, -0.01487685,  0.03721318,\n       -0.03644228])",
    "crumbs": [
      "12. Introduction to Modeling Libraries in Python",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Notes</span>"
    ]
  },
  {
    "objectID": "12_notes.html#introduction-to-scikit-learn",
    "href": "12_notes.html#introduction-to-scikit-learn",
    "title": "3  Notes",
    "section": "3.7 Introduction to scikit-learn",
    "text": "3.7 Introduction to scikit-learn\nscikit-learn is one of the most widely used and trusted general-purpose Python machine learning toolkits.\n\nX_train, X_test, y_train, y_test = train_test_split(\n    sample[['income_group', 'region', 'year']], sample[['life_expectancy']], test_size=0.2, random_state=42)\n\n\n# make sure there are no missing values\n\nprint(X_test.isna().sum())\nprint(y_test.isna().sum())\n\nincome_group    0\nregion          0\nyear            0\ndtype: int64\nlife_expectancy    0\ndtype: int64\n\n\n\n# can also do it with patsy\n\noutcome, predictors = patsy.dmatrices(\n    'life_expectancy ~ income_group + region + year', data=sample\n)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    predictors, outcome, test_size=0.2, random_state=42)\n\ny_train = y_train.squeeze()\ny_test = y_test.squeeze()\n\n\n# impute_value = train['Age'].median()\n# train['Age'] = train['Age'].fillna(impute_value)\n# test['Age'] = test['Age'].fillna(impute_value)\n\n\nmodel = LinearRegression().fit(X_train, y_train)\nmodel.fit(X_train, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\ny_predict = model.predict(X_test)\ny_predict\n\narray([78.63905681, 41.6710372 , 61.07560873, 77.12346239, 56.24225717,\n       60.82576811, 42.99395169, 41.6710372 , 74.1469048 , 75.46981928,\n       75.04709887, 71.69375575, 63.94037441, 51.82387157, 83.87519146,\n       42.66322307, 35.6275935 , 37.94269386, 63.56521171, 80.89863386,\n       50.99752407, 62.57302584, 58.22662891, 51.61205646, 77.52772236,\n       52.57565731, 79.10783413, 73.92796809])\n\n\n\noutcome\n\nDesignMatrix with shape (140, 1)\n  life_expectancy\n         67.68049\n         69.77073\n         46.22322\n         70.97317\n         49.26980\n         59.19907\n         47.08398\n         65.13822\n         68.57780\n         67.86585\n         62.05200\n         36.42517\n         62.45951\n         58.78159\n         44.10078\n         62.99259\n         39.80690\n         47.81127\n         52.67476\n         40.49602\n         64.71500\n         72.20780\n         45.31688\n         51.56868\n         50.15834\n         54.96951\n         54.44346\n         40.50444\n         70.29000\n         61.81917\n  [110 rows omitted]\n  Terms:\n    'life_expectancy' (column 0)\n  (to view full data, use np.asarray(this_obj))",
    "crumbs": [
      "12. Introduction to Modeling Libraries in Python",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Notes</span>"
    ]
  },
  {
    "objectID": "12_notes.html#going-rogue",
    "href": "12_notes.html#going-rogue",
    "title": "3  Notes",
    "section": "3.8 Going rogue",
    "text": "3.8 Going rogue\nUsing yellowbrick:\n\ny is the real avalues\ny hat is predicted\nblack dotted line is fitted line created by the model\ngrey dotted line is if the predicted values == real values\n\n\nmodel = LinearRegression()\nvisualizer = prediction_error(model, X_train, y_train, X_test, y_test)",
    "crumbs": [
      "12. Introduction to Modeling Libraries in Python",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Notes</span>"
    ]
  },
  {
    "objectID": "example_python.html",
    "href": "example_python.html",
    "title": "4  Example Jupyter Notebook",
    "section": "",
    "text": "4.1 NumPy\nimport numpy as np\na = np.arange(15).reshape(3, 5)\na\n\narray([[ 0,  1,  2,  3,  4],\n       [ 5,  6,  7,  8,  9],\n       [10, 11, 12, 13, 14]])",
    "crumbs": [
      "Examples",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>NumPy</span>"
    ]
  },
  {
    "objectID": "example_python.html#matplotlib",
    "href": "example_python.html#matplotlib",
    "title": "4  Example Jupyter Notebook",
    "section": "4.2 Matplotlib",
    "text": "4.2 Matplotlib\n\nimport matplotlib.pyplot as plt\n\nfig = plt.figure()\nx = np.arange(10)\ny = 2.5 * np.sin(x / 20 * np.pi)\nyerr = np.linspace(0.05, 0.2, 10)\n\nplt.errorbar(x, y + 3, yerr=yerr, label='both limits (default)')\nplt.errorbar(x, y + 2, yerr=yerr, uplims=True, label='uplims=True')\nplt.errorbar(x, y + 1, yerr=yerr, uplims=True, lolims=True,\n             label='uplims=True, lolims=True')\n\nupperlimits = [True, False] * 5\nlowerlimits = [False, True] * 5\nplt.errorbar(x, y, yerr=yerr, uplims=upperlimits, lolims=lowerlimits,\n             label='subsets of uplims and lolims')\n\nplt.legend(loc='lower right')\nplt.show(fig)",
    "crumbs": [
      "Examples",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>NumPy</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Python for Data Analysis Book Club",
    "section": "",
    "text": "Welcome\nThis is a companion for the book Python for Data Analysis, 3E by Wes McKinney.\nThis website is being developed by the Data Science Learning Community. Follow along and join the community to participate.\nThis companion follows the Data Science Learning Community Code of Conduct.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#book-club-meetings",
    "href": "index.html#book-club-meetings",
    "title": "Python for Data Analysis Book Club",
    "section": "Book club meetings",
    "text": "Book club meetings\n\nEach week, a volunteer will present a chapter from the book.\n\nThis is the best way to learn the material.\n\nPresentations will usually consist of a review of the material, a discussion, and/or a demonstration of the principles presented in that chapter.\nMore information about how to present is available in the GitHub repo.\nPresentations will be recorded and will be available on the Data Science Learning Community YouTube Channel.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "01_video.html",
    "href": "01_video.html",
    "title": "Video",
    "section": "",
    "text": "Cohort 01",
    "crumbs": [
      "1. Preliminaries",
      "Video"
    ]
  },
  {
    "objectID": "01_video.html#cohort-01",
    "href": "01_video.html#cohort-01",
    "title": "Video",
    "section": "",
    "text": "00:22:16    Isabella Velásquez: For more info on TidyTuesday: https://github.com/rfordatascience/tidytuesday#readme\n00:26:23    shamsuddeen:    https://docs.google.com/spreadsheets/d/1io_R_ZaGtzJcDF5KcY4hbgMBsJVi_4USE4JGsHcv9ug/edit#gid=0\n00:45:23    shamsuddeen:    https://github.com/conda-forge/miniforge\n00:59:07    shamsuddeen:    https://github.com/conda-forge/miniforge\n01:03:03    shamsuddeen:    https://blogs.shmuhammad.com/",
    "crumbs": [
      "1. Preliminaries",
      "Video"
    ]
  },
  {
    "objectID": "02_video.html",
    "href": "02_video.html",
    "title": "Video",
    "section": "",
    "text": "Cohort 01",
    "crumbs": [
      "2. Python Language Basics, IPython, and Jupyter Notebooks",
      "Video"
    ]
  },
  {
    "objectID": "02_video.html#cohort-01",
    "href": "02_video.html#cohort-01",
    "title": "Video",
    "section": "",
    "text": "00:09:36    Isabella Velásquez: https://github.com/r4ds/bookclub-py4da\n00:15:43    Layla Bouzoubaa:    heart heart heart\n00:15:50    Layla Bouzoubaa:    I am lving quarto\n00:16:07    Layla Bouzoubaa:    😻😻😻😻\n00:16:27    shamsuddeen:    Yes\n00:17:49    Jadey Ryan: thank you so much Isabella! so helpful 🥳\n00:21:12    Isabella Velásquez: Get started with Quarto: https://quarto.org/docs/get-started/\n00:21:50    Ron:    Busted ;)\n00:39:01    Isabella Velásquez: import pandas as pd\n\n%pdef pd.read_csv\n00:39:23    Isabella Velásquez: ^ this runs (no parentheses)\n00:41:15    Ron:    For simpler functions %pdef is pretty helpful, for example %pdef pd.DataFrame is only a few lines ;)\n00:41:46    Layla Bouzoubaa:    ahh, perfect! thank you much!\n00:43:13    Layla Bouzoubaa:    wooaahh\n00:52:53    Isabella Velásquez: Error in \"5\" + 6 : non-numeric argument to binary operator\n01:02:27    Ron:    I LOVE f-strings.\n01:06:45    Isabella Velásquez: I have to go, thanks so much, this was great!\n01:07:19    Layla Bouzoubaa:    me as well, have a great weekend all!\n01:08:17    Jim Gruman: thankyou!!!\n01:08:25    Jadey Ryan: thank you!!!",
    "crumbs": [
      "2. Python Language Basics, IPython, and Jupyter Notebooks",
      "Video"
    ]
  },
  {
    "objectID": "03_notes.html",
    "href": "03_notes.html",
    "title": "2  Data Structures and Sequences",
    "section": "",
    "text": "2.1 Tuples\nA tuple is a fixed-length, immutable sequence of Python objects which, once assigned, cannot be changed. The easiest way to create one is with a comma-separated sequence of values wrapped in parentheses:\ntup = (4, 5, 6)\ntup\n\n(4, 5, 6)\nIn many contexts, the parentheses can be omitted\ntup = 4, 5, 6\ntup\n\n(4, 5, 6)\nYou can convert any sequence or iterator to a tuple by invoking\ntuple([4,0,2])\n\ntup = tuple('string')\n\ntup\n\n('s', 't', 'r', 'i', 'n', 'g')\nElements can be accessed with square brackets []\nNote the zero indexing\ntup[0]\n\n's'\nTuples of tuples\nnested_tup = (4,5,6),(7,8)\n\nnested_tup\n\n((4, 5, 6), (7, 8))\nnested_tup[0]\n\n(4, 5, 6)\nnested_tup[1]\n\n(7, 8)\nWhile the objects stored in a tuple may be mutable themselves, once the tuple is created it’s not possible to modify which object is stored in each slot:\ntup = tuple(['foo', [1, 2], True])\n\ntup[2]\n\nTrue\nIf an object inside a tuple is mutable, such as a list, you can modify it in place\ntup[1].append(3)\n\ntup\n\n('foo', [1, 2, 3], True)\nYou can concatenate tuples using the + operator to produce longer tuples:\n(4, None, 'foo') + (6, 0) + ('bar',)\n\n(4, None, 'foo', 6, 0, 'bar')",
    "crumbs": [
      "3. Built-in Data Structures, Functions, and Files",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Structures and Sequences</span>"
    ]
  },
  {
    "objectID": "03_notes.html#tuples",
    "href": "03_notes.html#tuples",
    "title": "2  Data Structures and Sequences",
    "section": "",
    "text": "```{python}\n\ntup[2] = False\n\n```\nTypeError                                 Traceback (most recent call last)\nInput In [9], in &lt;cell line: 1&gt;()\n----&gt; 1 tup[2] = False\n\nTypeError: 'tuple' object does not support item assignment\nTypeError: 'tuple' object does not support item assignment\n\n\n\n\n\n2.1.1 Unpacking tuples\nIf you try to assign to a tuple-like expression of variables, Python will attempt to unpack the value on the righthand side of the equals sign:\n\ntup = (4, 5, 6)\ntup\n\n(4, 5, 6)\n\n\n\na, b, c = tup\n\nc\n\n6\n\n\nEven sequences with nested tuples can be unpacked:\n\ntup = 4, 5, (6,7)\n\na, b, (c, d) = tup\n\nd\n\n7\n\n\nTo easily swap variable names\n\na, b = 1, 4\n\na\n\n1\n\n\n\nb\n\n4\n\n\n\nb, a = a, b\n\na\n\n4\n\n\n\nb\n\n1\n\n\nA common use of variable unpacking is iterating over sequences of tuples or lists\n\nseq = [(1, 2, 3), (4, 5, 6), (7, 8, 9)]\n\nseq\n\n[(1, 2, 3), (4, 5, 6), (7, 8, 9)]\n\n\n\nfor a, b, c in seq:\n     print(f'a={a}, b={b}, c={c}')\n\na=1, b=2, c=3\na=4, b=5, c=6\na=7, b=8, c=9\n\n\n*rest syntax for plucking elements\n\nvalues = 1,2,3,4,5\n\na, b, *rest = values\n\nrest\n\n[3, 4, 5]\n\n\nAs a matter of convention, many Python programmers will use the underscore (_) for unwanted variables:\n\na, b, *_ = values\n\n\n\n2.1.2 Tuple methods\nSince the size and contents of a tuple cannot be modified, it is very light on instance methods. A particularly useful one (also available on lists) is count\n\na = (1,2,2,2,2,3,4,5,7,8,9)\n\na.count(2)\n\n4",
    "crumbs": [
      "3. Built-in Data Structures, Functions, and Files",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Structures and Sequences</span>"
    ]
  },
  {
    "objectID": "03_notes.html#list",
    "href": "03_notes.html#list",
    "title": "2  Data Structures and Sequences",
    "section": "2.2 List",
    "text": "2.2 List\n\nIn contrast with tuples, lists are variable length and their contents can be modified in place.\nLists are mutable.\nLists use [] square brackts or the list function\n\na_list = [2, 3, 7, None]\n\ntup = (\"foo\", \"bar\", \"baz\")\n\nb_list = list(tup)\n\nb_list\n\n['foo', 'bar', 'baz']\n\n\n\nb_list[1] = \"peekaboo\"\n\nb_list\n\n['foo', 'peekaboo', 'baz']\n\n\nLists and tuples are semantically similar (though tuples cannot be modified) and can be used interchangeably in many functions.\n\ngen = range(10)\n\ngen\n\nrange(0, 10)\n\n\n\nlist(gen)\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\n\n2.2.1 Adding and removing list elements\nthe append method\n\nb_list.append(\"dwarf\")\n\nb_list\n\n['foo', 'peekaboo', 'baz', 'dwarf']\n\n\nthe insert method\n\nb_list.insert(1, \"red\")\n\nb_list\n\n['foo', 'red', 'peekaboo', 'baz', 'dwarf']\n\n\ninsert is computationally more expensive than append\nthe pop method, the inverse of insert\n\nb_list.pop(2)\n\n'peekaboo'\n\n\n\nb_list\n\n['foo', 'red', 'baz', 'dwarf']\n\n\nthe remove method\n\nb_list.append(\"foo\")\n\nb_list\n\n['foo', 'red', 'baz', 'dwarf', 'foo']\n\n\n\nb_list.remove(\"foo\")\n\nb_list\n\n['red', 'baz', 'dwarf', 'foo']\n\n\nCheck if a list contains a value using the in keyword:\n\n\"dwarf\" in b_list\n\nTrue\n\n\nThe keyword not can be used to negate an in\n\n\"dwarf\" not in b_list\n\nFalse\n\n\n\n\n2.2.2 Concatenating and combining lists\nsimilar with tuples, use + to concatenate\n\n[4, None, \"foo\"] + [7, 8, (2, 3)]\n\n[4, None, 'foo', 7, 8, (2, 3)]\n\n\nthe extend method\n\nx = [4, None, \"foo\"]\n\nx.extend([7,8,(2,3)])\n\nx\n\n[4, None, 'foo', 7, 8, (2, 3)]\n\n\nlist concatenation by addition is an expensive operation\nusing extend is preferable\n```{python}\neverything = []\nfor chunk in list_of_lists:\n    everything.extend(chunk)\n\n```\nis generally faster than\n```{python}\n\neverything = []\nfor chunk in list_of_lists:\n    everything = everything + chunk\n\n```\n\n\n2.2.3 Sorting\nthe sort method\n\na = [7, 2, 5, 1, 3]\n\na.sort()\n\na\n\n[1, 2, 3, 5, 7]\n\n\nsort options\n\nb = [\"saw\", \"small\", \"He\", \"foxes\", \"six\"]\n\nb.sort(key = len)\n\nb\n\n['He', 'saw', 'six', 'small', 'foxes']\n\n\n\n\n2.2.4 Slicing\nSlicing semantics takes a bit of getting used to, especially if you’re coming from R or MATLAB.\nusing the indexing operator []\n\nseq = [7, 2, 3, 7, 5, 6, 0, 1]\n\nseq[3:5]\n\n[7, 5]\n\n\nalso assigned with a sequence\n\nseq[3:5] = [6,3]\n\nseq\n\n[7, 2, 3, 6, 3, 6, 0, 1]\n\n\nEither the start or stop can be omitted\n\nseq[:5]\n\n[7, 2, 3, 6, 3]\n\n\n\nseq[3:]\n\n[6, 3, 6, 0, 1]\n\n\nNegative indices slice the sequence relative to the end:\n\nseq[-4:]\n\n[3, 6, 0, 1]\n\n\nA step can also be used after a second colon to, say, take every other element:\n\nseq[::2]\n\n[7, 3, 3, 0]\n\n\nA clever use of this is to pass -1, which has the useful effect of reversing a list or tuple:\n\nseq[::-1]\n\n[1, 0, 6, 3, 6, 3, 2, 7]",
    "crumbs": [
      "3. Built-in Data Structures, Functions, and Files",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Structures and Sequences</span>"
    ]
  },
  {
    "objectID": "03_notes.html#dictionary",
    "href": "03_notes.html#dictionary",
    "title": "2  Data Structures and Sequences",
    "section": "2.3 Dictionary",
    "text": "2.3 Dictionary\n\nThe dictionary or dict may be the most important built-in Python data structure.\nOne approach for creating a dictionary is to use curly braces {} and colons to separate keys and values:\n\nempty_dict = {}\n\nd1 = {\"a\": \"some value\", \"b\": [1, 2, 3, 4]}\n\nd1\n\n{'a': 'some value', 'b': [1, 2, 3, 4]}\n\n\naccess, insert, or set elements\n\nd1[7] = \"an integer\"\n\nd1\n\n{'a': 'some value', 'b': [1, 2, 3, 4], 7: 'an integer'}\n\n\nand as before\n\n\"b\" in d1\n\nTrue\n\n\nthe del and pop methods\n\ndel d1[7]\n\nd1\n\n{'a': 'some value', 'b': [1, 2, 3, 4]}\n\n\n\nret = d1.pop(\"a\")\n\nret\n\n'some value'\n\n\nThe keys and values methods\n\nlist(d1.keys())\n\n['b']\n\n\n\nlist(d1.values())\n\n[[1, 2, 3, 4]]\n\n\nthe items method\n\nlist(d1.items())\n\n[('b', [1, 2, 3, 4])]\n\n\nthe update method to merge one dictionary into another\n\nd1.update({\"b\": \"foo\", \"c\": 12})\n\nd1\n\n{'b': 'foo', 'c': 12}\n\n\n### Creating dictionaries from sequences\n\nlist(range(5))\n\n[0, 1, 2, 3, 4]\n\n\n\ntuples = zip(range(5), reversed(range(5)))\n\ntuples\n\nmapping = dict(tuples)\n\nmapping\n\n{0: 4, 1: 3, 2: 2, 3: 1, 4: 0}\n\n\n\n2.3.1 Default values\nimagine categorizing a list of words by their first letters as a dictionary of lists\n\nwords = [\"apple\", \"bat\", \"bar\", \"atom\", \"book\"]\n\nby_letter = {}\n\nfor word in words:\n        letter = word[0]\n        if letter not in by_letter:\n            by_letter[letter] = [word]\n        else:\n            by_letter[letter].append(word)\n\nby_letter\n\n{'a': ['apple', 'atom'], 'b': ['bat', 'bar', 'book']}\n\n\nThe setdefault dictionary method can be used to simplify this workflow. The preceding for loop can be rewritten as:\n\nby_letter = {}\n\nfor word in words:\n        letter = word[0]\n        by_letter.setdefault(letter, []).append(word)\n\nby_letter\n\n{'a': ['apple', 'atom'], 'b': ['bat', 'bar', 'book']}\n\n\nThe built-in collectionsmodule has a useful class, defaultdict, which makes this even easier.\n\nfrom collections import defaultdict\n\nby_letter = defaultdict(list)\n\nfor word in words:\n        by_letter[word[0]].append(word)\n\nby_letter\n\ndefaultdict(list, {'a': ['apple', 'atom'], 'b': ['bat', 'bar', 'book']})\n\n\n\n\n2.3.2 Valid dictionary key types\nkeys generally have to be immutable objects like scalars or tuples for hashability\nTo use a list as a key, one option is to convert it to a tuple, which can be hashed as long as its elements also can be:\n\nd = {}\n\nd[tuple([1,2,3])] = 5\n\nd\n\n{(1, 2, 3): 5}",
    "crumbs": [
      "3. Built-in Data Structures, Functions, and Files",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Structures and Sequences</span>"
    ]
  },
  {
    "objectID": "03_notes.html#set",
    "href": "03_notes.html#set",
    "title": "2  Data Structures and Sequences",
    "section": "2.4 Set",
    "text": "2.4 Set\n\ncan be created in two ways: via the set function or via a set literal with curly braces:\n\nset([2, 2, 2, 1, 3, 3])\n\n{2,2,1,3,3}\n\n{1, 2, 3}\n\n\nSets support mathematical set operations like union, intersection, difference, and symmetric difference.\nThe union of these two sets:\n\na = {1, 2, 3, 4, 5}\n\nb = {3, 4, 5, 6, 7, 8}\n\na.union(b)\n\na | b\n\n{1, 2, 3, 4, 5, 6, 7, 8}\n\n\nThe &operator or the intersection method\n\na.intersection(b)\n\na & b\n\n{3, 4, 5}\n\n\nA table of commonly used set methods\nAll of the logical set operations have in-place counterparts, which enable you to replace the contents of the set on the left side of the operation with the result. For very large sets, this may be more efficient\n\nc = a.copy()\n\nc |= b\n\nc\n\n{1, 2, 3, 4, 5, 6, 7, 8}\n\n\n\nd = a.copy()\n\nd &= b\n\nd\n\n{3, 4, 5}\n\n\nset elements generally must be immutable, and they must be hashable\nyou can convert them to tuples\nYou can also check if a set is a subset of (is contained in) or a superset of (contains all elements of) another set\n\na_set = {1, 2, 3, 4, 5}\n\n{1, 2, 3}.issubset(a_set)\n\nTrue\n\n\n\na_set.issuperset({1, 2, 3})\n\nTrue",
    "crumbs": [
      "3. Built-in Data Structures, Functions, and Files",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Structures and Sequences</span>"
    ]
  },
  {
    "objectID": "03_notes.html#built-in-sequence-functions",
    "href": "03_notes.html#built-in-sequence-functions",
    "title": "2  Data Structures and Sequences",
    "section": "2.5 Built-In Sequence Functions",
    "text": "2.5 Built-In Sequence Functions\n\n2.5.1 enumerate\nenumerate returns a sequence of (i, value) tuples\n\n\n2.5.2 sorted\nsorted returns a new sorted list\n\nsorted([7,1,2,9,3,6,5,0,22])\n\n[0, 1, 2, 3, 5, 6, 7, 9, 22]\n\n\n\n\n2.5.3 zip\nzip “pairs” up the elements of a number of lists, tuples, or other sequences to create a list of tuples\n\nseq1 = [\"foo\", \"bar\", \"baz\"]\n\nseq2 = [\"one\", \"two\", \"three\"]\n\nzipped = zip(seq1, seq2)\n\nlist(zipped)\n\n[('foo', 'one'), ('bar', 'two'), ('baz', 'three')]\n\n\nzip can take an arbitrary number of sequences, and the number of elements it produces is determined by the shortest sequence\n\nseq3 = [False, True]\n\nlist(zip(seq1, seq2, seq3))\n\n[('foo', 'one', False), ('bar', 'two', True)]\n\n\nA common use of zip is simultaneously iterating over multiple sequences, possibly also combined with enumerate\n\nfor index, (a, b) in enumerate(zip(seq1, seq2)):\n    print(f\"{index}: {a}, {b}\")\n\n0: foo, one\n1: bar, two\n2: baz, three\n\n\nreversed iterates over the elements of a sequence in reverse order\n\nlist(reversed(range(10)))\n\n[9, 8, 7, 6, 5, 4, 3, 2, 1, 0]",
    "crumbs": [
      "3. Built-in Data Structures, Functions, and Files",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Structures and Sequences</span>"
    ]
  },
  {
    "objectID": "03_notes.html#list-set-and-dictionary-comprehensions",
    "href": "03_notes.html#list-set-and-dictionary-comprehensions",
    "title": "2  Data Structures and Sequences",
    "section": "2.6 List, Set, and Dictionary Comprehensions",
    "text": "2.6 List, Set, and Dictionary Comprehensions\n[expr for value in collection if condition]\nFor example, given a list of strings, we could filter out strings with length 2 or less and convert them to uppercase like this\n\nstrings = [\"a\", \"as\", \"bat\", \"car\", \"dove\", \"python\"]\n\n[x.upper() for x in strings if len(x) &gt; 2]\n\n['BAT', 'CAR', 'DOVE', 'PYTHON']\n\n\nA dictionary comprehension looks like this\ndict_comp = {key-expr: value-expr for value in collection\n             if condition}\nSuppose we wanted a set containing just the lengths of the strings contained in the collection\n\nunique_lengths = {len(x) for x in strings}\n\nunique_lengths\n\n{1, 2, 3, 4, 6}\n\n\nwe could create a lookup map of these strings for their locations in the list\n\nloc_mapping = {value: index for index, value in enumerate(strings)}\n\nloc_mapping\n\n{'a': 0, 'as': 1, 'bat': 2, 'car': 3, 'dove': 4, 'python': 5}",
    "crumbs": [
      "3. Built-in Data Structures, Functions, and Files",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Structures and Sequences</span>"
    ]
  },
  {
    "objectID": "03_notes.html#nested-list-comprehensions",
    "href": "03_notes.html#nested-list-comprehensions",
    "title": "2  Data Structures and Sequences",
    "section": "2.7 Nested list comprehensions",
    "text": "2.7 Nested list comprehensions\nSuppose we have a list of lists containing some English and Spanish names. We want to get a single list containing all names with two or more a’s in them\n\nall_data = [[\"John\", \"Emily\", \"Michael\", \"Mary\", \"Steven\"],\n            [\"Maria\", \"Juan\", \"Javier\", \"Natalia\", \"Pilar\"]]\n\nresult = [name for names in all_data for name in names\n          if name.count(\"a\") &gt;= 2]\n\nresult\n\n['Maria', 'Natalia']\n\n\nHere is another example where we “flatten” a list of tuples of integers into a simple list of integers\n\nsome_tuples = [(1, 2, 3), (4, 5, 6), (7, 8, 9)]\n\nflattened = [x for tup in some_tuples for x in tup]\n\nflattened\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9]",
    "crumbs": [
      "3. Built-in Data Structures, Functions, and Files",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Structures and Sequences</span>"
    ]
  },
  {
    "objectID": "03_notes.html#namespaces-scope-and-local-functions",
    "href": "03_notes.html#namespaces-scope-and-local-functions",
    "title": "2  Data Structures and Sequences",
    "section": "3.1 Namespaces, Scope, and Local Functions",
    "text": "3.1 Namespaces, Scope, and Local Functions\nA more descriptive name describing a variable scope in Python is a namespace.\nConsider the following function\n\na = []\n\ndef func():\n    for i in range(5):\n        a.append(i)\n\nWhen func() is called, the empty list a is created, five elements are appended, and then a is destroyed when the function exits.\n\nfunc()\n\nfunc()\n\na\n\n[0, 1, 2, 3, 4, 0, 1, 2, 3, 4]",
    "crumbs": [
      "3. Built-in Data Structures, Functions, and Files",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Structures and Sequences</span>"
    ]
  },
  {
    "objectID": "03_notes.html#returing-multiple-values",
    "href": "03_notes.html#returing-multiple-values",
    "title": "2  Data Structures and Sequences",
    "section": "3.2 Returing Multiple Values",
    "text": "3.2 Returing Multiple Values\nWhat’s happening here is that the function is actually just returning one object, a tuple, which is then being unpacked into the result variables.\n\ndef f():\n    a = 5\n    b = 6\n    c = 7\n    return a, b, c\n\na, b, c = f()\n\na\n\n5",
    "crumbs": [
      "3. Built-in Data Structures, Functions, and Files",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Structures and Sequences</span>"
    ]
  },
  {
    "objectID": "03_notes.html#functions-are-objects",
    "href": "03_notes.html#functions-are-objects",
    "title": "2  Data Structures and Sequences",
    "section": "3.3 Functions are Objects",
    "text": "3.3 Functions are Objects\nSuppose we were doing some data cleaning and needed to apply a bunch of transformations to the following list of strings:\n\nstates = [\"   Alabama \", \"Georgia!\", \"Georgia\", \"georgia\", \"FlOrIda\",\n          \"south   carolina##\", \"West virginia?\"]\n\nimport re\n\ndef clean_strings(strings):\n    result = []\n    for value in strings:\n        value = value.strip()\n        value = re.sub(\"[!#?]\", \"\", value)\n        value = value.title()\n        result.append(value)\n    return result\n\nclean_strings(states)\n\n['Alabama',\n 'Georgia',\n 'Georgia',\n 'Georgia',\n 'Florida',\n 'South   Carolina',\n 'West Virginia']\n\n\nAnother approach\n\ndef remove_punctuation(value):\n    return re.sub(\"[!#?]\", \"\", value)\n\nclean_ops = [str.strip, remove_punctuation, str.title]\n\ndef clean_strings(strings, ops):\n    result = []\n    for value in strings:\n        for func in ops:\n            value = func(value)\n        result.append(value)\n    return result\n\nclean_strings(states, clean_ops)\n\n['Alabama',\n 'Georgia',\n 'Georgia',\n 'Georgia',\n 'Florida',\n 'South   Carolina',\n 'West Virginia']\n\n\nYou can use functions as arguments to other functions like the built-in map function\n\nfor x in map(remove_punctuation, states):\n    print(x)\n\n   Alabama \nGeorgia\nGeorgia\ngeorgia\nFlOrIda\nsouth   carolina\nWest virginia",
    "crumbs": [
      "3. Built-in Data Structures, Functions, and Files",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Structures and Sequences</span>"
    ]
  },
  {
    "objectID": "03_notes.html#anonymous-lambda-functions",
    "href": "03_notes.html#anonymous-lambda-functions",
    "title": "2  Data Structures and Sequences",
    "section": "3.4 Anonymous Lambda Functions",
    "text": "3.4 Anonymous Lambda Functions\na way of writing functions consisting of a single statement\nsuppose you wanted to sort a collection of strings by the number of distinct letters in each string\n\nstrings = [\"foo\", \"card\", \"bar\", \"aaaaaaa\", \"ababdo\"]\n\nstrings.sort(key=lambda x: len(set(x)))\n\nstrings\n\n['aaaaaaa', 'foo', 'bar', 'card', 'ababdo']",
    "crumbs": [
      "3. Built-in Data Structures, Functions, and Files",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Structures and Sequences</span>"
    ]
  },
  {
    "objectID": "03_notes.html#generator-expressions",
    "href": "03_notes.html#generator-expressions",
    "title": "2  Data Structures and Sequences",
    "section": "4.1 Generator expressions",
    "text": "4.1 Generator expressions\nThis is a generator analogue to list, dictionary, and set comprehensions. To create one, enclose what would otherwise be a list comprehension within parentheses instead of brackets:\n\ngen = (x ** 2 for x in range(100))\n\ngen\n\n&lt;generator object &lt;genexpr&gt; at 0x000001A3294B9C10&gt;\n\n\nGenerator expressions can be used instead of list comprehensions as function arguments in some cases:\n\nsum(x ** 2 for x in range(100))\n\n328350\n\n\n\ndict((i, i ** 2) for i in range(5))\n\n{0: 0, 1: 1, 2: 4, 3: 9, 4: 16}",
    "crumbs": [
      "3. Built-in Data Structures, Functions, and Files",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Structures and Sequences</span>"
    ]
  },
  {
    "objectID": "03_notes.html#itertools-module",
    "href": "03_notes.html#itertools-module",
    "title": "2  Data Structures and Sequences",
    "section": "4.2 itertools module",
    "text": "4.2 itertools module\nitertools module has a collection of generators for many common data algorithms.\ngroupby takes any sequence and a function, grouping consecutive elements in the sequence by return value of the function\n\nimport itertools\n\ndef first_letter(x):\n    return x[0]\n\nnames = [\"Alan\", \"Adam\", \"Jackie\", \"Lily\", \"Katie\", \"Molly\"]\n\nfor letter, names in itertools.groupby(names, first_letter):\n    print(letter, list(names))\n\nA ['Alan', 'Adam']\nJ ['Jackie']\nL ['Lily']\nK ['Katie']\nM ['Molly']\n\n\nTable of other itertools functions",
    "crumbs": [
      "3. Built-in Data Structures, Functions, and Files",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Structures and Sequences</span>"
    ]
  },
  {
    "objectID": "03_notes.html#exceptions-in-ipython",
    "href": "03_notes.html#exceptions-in-ipython",
    "title": "2  Data Structures and Sequences",
    "section": "5.1 Exceptions in IPython",
    "text": "5.1 Exceptions in IPython\nIf an exception is raised while you are %run-ing a script or executing any statement, IPython will by default print a full call stack trace. Having additional context by itself is a big advantage over the standard Python interpreter",
    "crumbs": [
      "3. Built-in Data Structures, Functions, and Files",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Structures and Sequences</span>"
    ]
  },
  {
    "objectID": "03_notes.html#byte-and-unicode-with-files",
    "href": "03_notes.html#byte-and-unicode-with-files",
    "title": "2  Data Structures and Sequences",
    "section": "6.1 Byte and Unicode with Files",
    "text": "6.1 Byte and Unicode with Files\nThe default behavior for Python files (whether readable or writable) is text mode, which means that you intend to work with Python strings (i.e., Unicode).",
    "crumbs": [
      "3. Built-in Data Structures, Functions, and Files",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Structures and Sequences</span>"
    ]
  },
  {
    "objectID": "03_video.html",
    "href": "03_video.html",
    "title": "Video",
    "section": "",
    "text": "Cohort 01",
    "crumbs": [
      "3. Built-in Data Structures, Functions, and Files",
      "Video"
    ]
  },
  {
    "objectID": "03_video.html#cohort-01",
    "href": "03_video.html#cohort-01",
    "title": "Video",
    "section": "",
    "text": "00:30:39    Karim Badr: Is there a difference between append and extend?\n00:30:57    Ron:    Append is for adding a single element\n00:31:11    Karim Badr: Thanks!\n00:36:34    Ron:    Some discussion of this here: https://stackoverflow.com/questions/11364533/why-are-slice-and-range-upper-bound-exclusive\n00:36:49    Ron:    if you are wondering (like I am) why it is like that :)",
    "crumbs": [
      "3. Built-in Data Structures, Functions, and Files",
      "Video"
    ]
  },
  {
    "objectID": "04_video.html",
    "href": "04_video.html",
    "title": "Video",
    "section": "",
    "text": "Cohort 01",
    "crumbs": [
      "4. NumPy Basics: Arrays and Vectorized Computation",
      "Video"
    ]
  },
  {
    "objectID": "04_video.html#cohort-01",
    "href": "04_video.html#cohort-01",
    "title": "Video",
    "section": "",
    "text": "00:37:17    Serena DeStefani:   Sorry, I have to go today because I have some work left to do. But this session is very nice and hopefully I will be back! Thank you\n00:53:43    Karim Badr: Quick google search about transpose vs swapaxes and other forms for this: https://blog.fearcat.in/a?ID=01750-160525d9-9c23-4590-9020-6a3f90d3bdbf",
    "crumbs": [
      "4. NumPy Basics: Arrays and Vectorized Computation",
      "Video"
    ]
  },
  {
    "objectID": "05_notes.html",
    "href": "05_notes.html",
    "title": "Notes",
    "section": "",
    "text": "Introduction",
    "crumbs": [
      "5. Getting Started with pandas",
      "Notes"
    ]
  },
  {
    "objectID": "05_notes.html#introduction",
    "href": "05_notes.html#introduction",
    "title": "Notes",
    "section": "",
    "text": "Note\n\n\n\nThis is a long chapter, these notes are intended as a tour of main ideas!\n\n\n\n\n\nPanda bus tour!\n\n\n\nPandas is a major tool in Python data analysis\nWorks with Numpy, adding support for tabular / heterogenous data",
    "crumbs": [
      "5. Getting Started with pandas",
      "Notes"
    ]
  },
  {
    "objectID": "05_notes.html#import-conventions",
    "href": "05_notes.html#import-conventions",
    "title": "Notes",
    "section": "Import conventions:",
    "text": "Import conventions:\n\nimport numpy as np\nimport pandas as pd",
    "crumbs": [
      "5. Getting Started with pandas",
      "Notes"
    ]
  },
  {
    "objectID": "05_notes.html#pandas-primary-data-structures",
    "href": "05_notes.html#pandas-primary-data-structures",
    "title": "Notes",
    "section": "Panda’s primary data structures",
    "text": "Panda’s primary data structures\n\nSeries: One dimensional object containing a sequence of values of the same type.\nDataFrame: Tabular data, similar (and inspired by) R dataframe.\nOther structures will be introduced as they arise, e.g. Index and Groupby objects.\n\n\nSeries\n\nobj = pd.Series([4,7,-4,3], index = [\"A\",\"B\",\"C\",\"D\"])\nobj\n\nA    4\nB    7\nC   -4\nD    3\ndtype: int64\n\n\nThe index is optional, if not specified it will default to 0 through N-1\n\nSelection\nSelect elements or sub-Series by labels, sets of labels, boolean arrays …\n\nobj['A']\n\n4\n\n\n\nobj[['A','C']]\n\nA    4\nC   -4\ndtype: int64\n\n\n\nobj[obj &gt; 3]\n\nA    4\nB    7\ndtype: int64\n\n\n\n\nOther things you can do\n\nNumpy functions and Numpy-like operations work as expected:\n\n\nobj*3\n\nA    12\nB    21\nC   -12\nD     9\ndtype: int64\n\n\n\nnp.exp(obj)\n\nA      54.598150\nB    1096.633158\nC       0.018316\nD      20.085537\ndtype: float64\n\n\n\nSeries can be created from and converted to a dictionary\n\n\nobj.to_dict()\n\n{'A': 4, 'B': 7, 'C': -4, 'D': 3}\n\n\n\nSeries can be converted to numpy array:\n\n\nobj.to_numpy()\n\narray([ 4,  7, -4,  3], dtype=int64)\n\n\n\n\n\nDataFrame\n\nRepresents table of data\nHas row index index and column index column\nCommon way to create is from a dictionary, but see Table 5.1 for more!\n\n\ntest = pd.DataFrame({\"cars\":['Chevy','Ford','Dodge','BMW'],'MPG':[14,15,16,12], 'Year':[1979, 1980, 2001, 2020]})\ntest\n\n\n\n\n\n\n\n\ncars\nMPG\nYear\n\n\n\n\n0\nChevy\n14\n1979\n\n\n1\nFord\n15\n1980\n\n\n2\nDodge\n16\n2001\n\n\n3\nBMW\n12\n2020\n\n\n\n\n\n\n\n\nIf you want a non-default index, it can be specified just like with Series.\nhead(n) / tail(n) - return the first / last n rows, 5 by default\n\n\nSelecting\n\nCan retrieve columns or sets of columns by using obj[...]:\n\n\ntest['cars']\n\n0    Chevy\n1     Ford\n2    Dodge\n3      BMW\nName: cars, dtype: object\n\n\nNote that we got a Series here.\n\ntest[['cars','MPG']]\n\n\n\n\n\n\n\n\ncars\nMPG\n\n\n\n\n0\nChevy\n14\n\n\n1\nFord\n15\n\n\n2\nDodge\n16\n\n\n3\nBMW\n12\n\n\n\n\n\n\n\n\nDot notation can also be used (test.cars) as long as the column names are valid identifiers\nRows can be retrieved with iloc[...] and loc[...]:\n\nloc retrieves by index\niloc retrieves by position.\n\n\n\n\nModifying / Creating Columns\n\nColumns can be modified (and created) by assignment:\n\n\ntest['MPG^2'] = test['MPG']**2\ntest\n\n\n\n\n\n\n\n\ncars\nMPG\nYear\nMPG^2\n\n\n\n\n0\nChevy\n14\n1979\n196\n\n\n1\nFord\n15\n1980\n225\n\n\n2\nDodge\n16\n2001\n256\n\n\n3\nBMW\n12\n2020\n144\n\n\n\n\n\n\n\n\ndel keyword can be used to drop columns, or drop method can be used to do so non-destructively\n\n\n\n\nIndex object\n\nIndex objects are used for holding axis labels and other metadata\n\n\ntest.index\n\nRangeIndex(start=0, stop=4, step=1)\n\n\n\nCan change the index, in this case replacing the default:\n\n\n# Create index from one of the columns\ntest.index = test['cars']  \n\n # remove 'cars' column since i am using as an index now.  s\ntest=test.drop('cars', axis = \"columns\")  # or axis  = 1\ntest\n\n\n\n\n\n\n\n\nMPG\nYear\nMPG^2\n\n\ncars\n\n\n\n\n\n\n\nChevy\n14\n1979\n196\n\n\nFord\n15\n1980\n225\n\n\nDodge\n16\n2001\n256\n\n\nBMW\n12\n2020\n144\n\n\n\n\n\n\n\n\nNote the axis keyword argument above, many DataFrame methods use this.\nAbove I changed a column into an index. Often you want to go the other way, this can be done with reset_index:\n\n\ntest.reset_index()  # Note this doesn't actually change test\n\n\n\n\n\n\n\n\ncars\nMPG\nYear\nMPG^2\n\n\n\n\n0\nChevy\n14\n1979\n196\n\n\n1\nFord\n15\n1980\n225\n\n\n2\nDodge\n16\n2001\n256\n\n\n3\nBMW\n12\n2020\n144\n\n\n\n\n\n\n\n\nColumns are an index as well:\n\n\ntest.columns\n\nIndex(['MPG', 'Year', 'MPG^2'], dtype='object')\n\n\n\nIndexes act like immutable sets, see Table 5.2 in book for Index methods and properties",
    "crumbs": [
      "5. Getting Started with pandas",
      "Notes"
    ]
  },
  {
    "objectID": "05_notes.html#essential-functionality",
    "href": "05_notes.html#essential-functionality",
    "title": "Notes",
    "section": "Essential Functionality",
    "text": "Essential Functionality\n\nReindexing and dropping\n\nreindex creats a new object with the values arranged according to the new index. Missing values are used if necessary, or you can use optional fill methods. You can use iloc and loc to reindex as well.\n\n\ns = pd.Series([1,2,3,4,5], index = list(\"abcde\"))\ns2 = s.reindex(list(\"abcfu\"))  #  not a song by GAYLE \ns2\n\na    1.0\nb    2.0\nc    3.0\nf    NaN\nu    NaN\ndtype: float64\n\n\n\nMissing values and can be tested for with isna or notna methods\n\n\npd.isna(s2)\n\na    False\nb    False\nc    False\nf     True\nu     True\ndtype: bool\n\n\n\ndrop , illustrated above can drop rows or columns. In addition to using axis you can use columns or index. Again these make copies.\n\n\ntest.drop(columns = 'MPG')\n\n\n\n\n\n\n\n\nYear\nMPG^2\n\n\ncars\n\n\n\n\n\n\nChevy\n1979\n196\n\n\nFord\n1980\n225\n\n\nDodge\n2001\n256\n\n\nBMW\n2020\n144\n\n\n\n\n\n\n\n\ntest.drop(index = ['Ford', 'BMW'])\n\n\n\n\n\n\n\n\nMPG\nYear\nMPG^2\n\n\ncars\n\n\n\n\n\n\n\nChevy\n14\n1979\n196\n\n\nDodge\n16\n2001\n256\n\n\n\n\n\n\n\n\n\nIndexing, Selection and Filtering\n\nSeries\n\nFor Series, indexing is similar to Numpy, except you can use the index as well as integers.\n\n\nobj = pd.Series(np.arange(4.), index=[\"a\", \"b\", \"c\", \"d\"])\nobj[0:3]\n\na    0.0\nb    1.0\nc    2.0\ndtype: float64\n\n\n\nobj['a':'c']\n\na    0.0\nb    1.0\nc    2.0\ndtype: float64\n\n\n\nobj[obj&lt;2]\n\na    0.0\nb    1.0\ndtype: float64\n\n\n\nobj[['a','d']]\n\na    0.0\nd    3.0\ndtype: float64\n\n\n\nHowever, preferred way is to use loc for selection by index and iloc for selection by position. This is to avoid the issue where the index is itself integers.\n\n\nobj.loc[['a','d']]\n\na    0.0\nd    3.0\ndtype: float64\n\n\n\nobj.iloc[1]\n\n1.0\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote if a range or a set of indexes is used, a Series is returned. If a single item is requested, you get just that item.\n\n\n\n\nDataFrame\n\nSelecting with df[...] for a DataFrame retrieves one or more columns as we have seen, if you select a single column you get a Series\nThere are some special cases, indexing with a boolean selects rows, as does selecting with a slice:\n\n\ntest[0:1]\n\n\n\n\n\n\n\n\nMPG\nYear\nMPG^2\n\n\ncars\n\n\n\n\n\n\n\nChevy\n14\n1979\n196\n\n\n\n\n\n\n\n\ntest[test['MPG'] &lt; 15]\n\n\n\n\n\n\n\n\nMPG\nYear\nMPG^2\n\n\ncars\n\n\n\n\n\n\n\nChevy\n14\n1979\n196\n\n\nBMW\n12\n2020\n144\n\n\n\n\n\n\n\n\niloc and loc can be used to select rows as illustrated before, but can also be used to select columns or subsets of rows/columns\n\n\ntest.loc[:,['Year','MPG']]\n\n\n\n\n\n\n\n\nYear\nMPG\n\n\ncars\n\n\n\n\n\n\nChevy\n1979\n14\n\n\nFord\n1980\n15\n\n\nDodge\n2001\n16\n\n\nBMW\n2020\n12\n\n\n\n\n\n\n\n\ntest.loc['Ford','MPG']\n\n15\n\n\n\nThese work with slices and booleans as well! The following says “give me all the rows with MPG more then 15, and the columns starting from Year”\n\n\ntest.loc[test['MPG'] &gt; 15, 'Year':]\n\n\n\n\n\n\n\n\nYear\nMPG^2\n\n\ncars\n\n\n\n\n\n\nDodge\n2001\n256\n\n\n\n\n\n\n\n\nIndexing options are fully illustrated in the book and Table 5.4\nBe careful with chained indexing:\n\n\ntest[test['MPG']&gt; 15].loc[:,'MPG'] = 18\n\nC:\\Users\\jryan\\AppData\\Local\\Temp\\ipykernel_13388\\2484144822.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  test[test['MPG']&gt; 15].loc[:,'MPG'] = 18\n\n\nHere we are assigning to a ‘slice’, which is probably not what is intended. You will get a warning and a recommendation to fix it by using one loc:\n\ntest.loc[test['MPG']&gt; 15 ,'MPG'] = 18\ntest\n\n\n\n\n\n\n\n\nMPG\nYear\nMPG^2\n\n\ncars\n\n\n\n\n\n\n\nChevy\n14\n1979\n196\n\n\nFord\n15\n1980\n225\n\n\nDodge\n18\n2001\n256\n\n\nBMW\n12\n2020\n144\n\n\n\n\n\n\n\n\n\n\n\n\n\nRule of Thumb\n\n\n\nAvoid chained indexing when doing assignments\n\n\n\n\n\nArithmetic and Data Alignment\n\nPandas can make it simpler to work with objects that have different indexes, usually ‘doing the right thing’\n\n\ns1 = pd.Series([7.3, -2.5, 3.4, 1.5], index=[\"a\", \"c\", \"d\", \"e\"])\ns2 = pd.Series([-2.1, 3.6, -1.5, 4, 3.1], index=[\"a\", \"c\", \"e\", \"f\", \"g\"])\ns1+s2\n\na    5.2\nc    1.1\nd    NaN\ne    0.0\nf    NaN\ng    NaN\ndtype: float64\n\n\n\nFills can be specified by using methods:\n\n\ns1.add(s2, fill_value = 0)\n\na    5.2\nc    1.1\nd    3.4\ne    0.0\nf    4.0\ng    3.1\ndtype: float64\n\n\n\nSee Table 5.5 for list of these methods.\nYou can also do arithmetic between DataFrames and Series in a way that is similar to Numpy.\n\n\n\nFunction Application and Mapping\n\nNumpy ufuncs also work with Pandas objects.\n\n\nframe = pd.DataFrame(np.random.standard_normal((4, 3)),\n                         columns=list(\"bde\"),\n                         index=[\"Utah\", \"Ohio\", \"Texas\", \"Oregon\"])\nframe\n\n\n\n\n\n\n\n\nb\nd\ne\n\n\n\n\nUtah\n0.798548\n-1.455476\n0.507618\n\n\nOhio\n-0.405775\n-0.132380\n-0.563721\n\n\nTexas\n-0.404526\n0.703566\n1.661291\n\n\nOregon\n-0.359775\n-0.450894\n-0.712254\n\n\n\n\n\n\n\n\nnp.abs(frame)\n\n\n\n\n\n\n\n\nb\nd\ne\n\n\n\n\nUtah\n0.798548\n1.455476\n0.507618\n\n\nOhio\n0.405775\n0.132380\n0.563721\n\n\nTexas\n0.404526\n0.703566\n1.661291\n\n\nOregon\n0.359775\n0.450894\n0.712254\n\n\n\n\n\n\n\n\napply can be used to apply a function on 1D arrays to each column or row:\n\n\nframe.apply(np.max, axis = 'rows') #'axis' is optional here, default is rows\n\nb    0.798548\nd    0.703566\ne    1.661291\ndtype: float64\n\n\nApplying accross columns is common, especially to combine different columns in some way:\n\nframe['max'] = frame.apply(np.max, axis = 'columns')\nframe\n\n\n\n\n\n\n\n\nb\nd\ne\nmax\n\n\n\n\nUtah\n0.798548\n-1.455476\n0.507618\n0.798548\n\n\nOhio\n-0.405775\n-0.132380\n-0.563721\n-0.132380\n\n\nTexas\n-0.404526\n0.703566\n1.661291\n1.661291\n\n\nOregon\n-0.359775\n-0.450894\n-0.712254\n-0.359775\n\n\n\n\n\n\n\n\nMany more examples of this in the book.\n\n\n\nSorting and Ranking\n\nsort_index will sort with the index (on either axis for DataFrame)\nsort_values is used to sort by values or a particular column\n\n\ntest.sort_values('MPG')\n\n\n\n\n\n\n\n\nMPG\nYear\nMPG^2\n\n\ncars\n\n\n\n\n\n\n\nBMW\n12\n2020\n144\n\n\nChevy\n14\n1979\n196\n\n\nFord\n15\n1980\n225\n\n\nDodge\n18\n2001\n256\n\n\n\n\n\n\n\n\nrank will assign ranks from on through the number of data points.",
    "crumbs": [
      "5. Getting Started with pandas",
      "Notes"
    ]
  },
  {
    "objectID": "05_notes.html#summarizing-and-computing-descriptive-statistics",
    "href": "05_notes.html#summarizing-and-computing-descriptive-statistics",
    "title": "Notes",
    "section": "Summarizing and Computing Descriptive Statistics",
    "text": "Summarizing and Computing Descriptive Statistics\n\ndf = pd.DataFrame([[1.4, np.nan], [7.1, -4.5],\n                      [np.nan, np.nan], [0.75, -1.3]],\n                      index=[\"a\", \"b\", \"c\", \"d\"],\n                      columns=[\"one\", \"two\"])\ndf\n\n\n\n\n\n\n\n\none\ntwo\n\n\n\n\na\n1.40\nNaN\n\n\nb\n7.10\n-4.5\n\n\nc\nNaN\nNaN\n\n\nd\n0.75\n-1.3\n\n\n\n\n\n\n\nSome Examples:\nSum over rows:\n\ndf.sum()\n\none    9.25\ntwo   -5.80\ndtype: float64\n\n\nSum over columns:\n\n# Sum Rows\ndf.sum(axis=\"columns\")\n\na    1.40\nb    2.60\nc    0.00\nd   -0.55\ndtype: float64\n\n\nExtremely useful is describe:\n\ndf.describe()\n\n\n\n\n\n\n\n\none\ntwo\n\n\n\n\ncount\n3.000000\n2.000000\n\n\nmean\n3.083333\n-2.900000\n\n\nstd\n3.493685\n2.262742\n\n\nmin\n0.750000\n-4.500000\n\n\n25%\n1.075000\n-3.700000\n\n\n50%\n1.400000\n-2.900000\n\n\n75%\n4.250000\n-2.100000\n\n\nmax\n7.100000\n-1.300000\n\n\n\n\n\n\n\nBook chapter contains many more examples and a full list of summary statistics and related methods.",
    "crumbs": [
      "5. Getting Started with pandas",
      "Notes"
    ]
  },
  {
    "objectID": "05_notes.html#summary",
    "href": "05_notes.html#summary",
    "title": "Notes",
    "section": "Summary",
    "text": "Summary\n\nPrimary Panda’s data structures:\n\nSeries\nDataFrame\n\nMany ways to access and transform these objects. Key ones are:\n\n[] : access an element(s) of a Series or columns(s) of a DataFrame\nloc[r ,c] : access a row / column / cell by the index.\niloc[i, j] : access ar row / column / cell by the integer position.\n\nOnline reference.\n\n\n\n\n\n\n\nSuggestion\n\n\n\nWork though the chapter’s code and try stuff!",
    "crumbs": [
      "5. Getting Started with pandas",
      "Notes"
    ]
  },
  {
    "objectID": "05_notes.html#references",
    "href": "05_notes.html#references",
    "title": "Notes",
    "section": "References",
    "text": "References\n\nChapter’s code.\nPanda reference.",
    "crumbs": [
      "5. Getting Started with pandas",
      "Notes"
    ]
  },
  {
    "objectID": "05_notes.html#next-chapter",
    "href": "05_notes.html#next-chapter",
    "title": "Notes",
    "section": "Next Chapter",
    "text": "Next Chapter\n\nLoading and writing data sets!",
    "crumbs": [
      "5. Getting Started with pandas",
      "Notes"
    ]
  },
  {
    "objectID": "05_video.html",
    "href": "05_video.html",
    "title": "Video",
    "section": "",
    "text": "Cohort 01",
    "crumbs": [
      "5. Getting Started with pandas",
      "Video"
    ]
  },
  {
    "objectID": "05_video.html#cohort-01",
    "href": "05_video.html#cohort-01",
    "title": "Video",
    "section": "",
    "text": "00:43:47    Karim Badr: Great summary!\n00:44:18    Oluwafemi Oyedele:  Thank you !!!\n00:44:23    Jim Gruman: Thank You!!!!\n00:44:24    shamsuddeen:    thank you\n00:44:30    Jadey Ryan: thank you Ron!\n00:44:56    Layla Bouzoubaa:    Jim i love you hex stickers in the frame! i'm going to steal that idea 😄\n00:46:06    Serena DeStefani:   😂",
    "crumbs": [
      "5. Getting Started with pandas",
      "Video"
    ]
  },
  {
    "objectID": "06_notes.html",
    "href": "06_notes.html",
    "title": "Notes",
    "section": "",
    "text": "Reading and Writing Data in Text Format",
    "crumbs": [
      "6. Data Loading, Storage, and File Formats",
      "Notes"
    ]
  },
  {
    "objectID": "06_notes.html#reading-and-writing-data-in-text-format",
    "href": "06_notes.html#reading-and-writing-data-in-text-format",
    "title": "Notes",
    "section": "",
    "text": "read_csv Arguments\nTable 6.1 lists the various data types pandas can read.\nEach function can be called with pd.read_* (for example, pd.read_csv).\n\n\n\n\n\n\nNote\n\n\n\nWes points out that the number of arguments can be overwhelming. pd.read_csv has about 50. The pandas documentation is a good resource for finding the right arguments.\n\n\nTable 6.2 lists frequently used options in pd.read_csv.\nLet’s import the Palmer Penguins dataset to explore this function and some of the csv arguments. Note: I added random numbers for month and day to demonstrate date parsing.\n\nimport pandas as pd\n\npenguins = pd.read_csv(\"data/penguins.csv\")\n\npenguins.head(5)\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nmonth\nday\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n4\n10\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n3\n6\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n7\n22\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2\n13\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n8\n21\n2007\n\n\n\n\n\n\n\n\nIndex Columns\nIndexing gets column names from the file or from this argument\n\npenguins_indexed = pd.read_csv(\"data/penguins.csv\", index_col = \"species\")\npenguins_indexed.head(5)\n\n\n\n\n\n\n\n\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nmonth\nday\nyear\n\n\nspecies\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n4\n10\n2007\n\n\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n3\n6\n2007\n\n\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n7\n22\n2007\n\n\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2\n13\n2007\n\n\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n8\n21\n2007\n\n\n\n\n\n\n\n\n\nInfer or Convert Data Type\nType inference and data conversion converts values (including missing) to a user-defined value.\nIf you data uses another string value as the missing placeholder, you can add it to na_values.\n\npenguins_NA = pd.read_csv(\n  \"data/penguins.csv\", \n  na_values = [\"male\"]\n  )\n  \npenguins_NA.head(5)\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nmonth\nday\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nNaN\n4\n10\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n3\n6\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n7\n22\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2\n13\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n8\n21\n2007\n\n\n\n\n\n\n\n\n\nParse Date and Time\nDate and time parsing combines date and time from multiple columns into a single column\n\npenguins_dates = pd.read_csv(\n  \"data/penguins.csv\", \n  parse_dates = {\"date\": [\"month\", \"day\", \"year\"]}\n  )\n  \npenguins_dates[\"date\"] = pd.to_datetime(\n  penguins_dates.date, \n  format = \"%m%d%Y\"\n  )\n  \nprint(penguins_dates.date.head(5))\n\nprint(penguins_dates.date.dtypes)\n\n0   2007-04-10\n1   2007-03-06\n2   2007-07-22\n3   2007-02-13\n4   2007-08-21\nName: date, dtype: datetime64[ns]\ndatetime64[ns]\n\n\n\n\nIterate Through Large Files\nIterating allows iteration over chunks of very large files\nUsing nrows to read in only 5 rows:\n\npd.read_csv(\"data/penguins.csv\", nrows = 5\n  )\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nmonth\nday\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n4\n10\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n3\n6\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n7\n22\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2\n13\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n8\n21\n2007\n\n\n\n\n\n\n\nUsing chunksize and the TextFileReader to aggregate and summarize the data by species:\n\nchunker = pd.read_csv(\"data/penguins.csv\", chunksize = 10)\n\nprint(type(chunker))\n\ntot = pd.Series([], dtype = 'int64')\nfor piece in chunker:\n    tot = tot.add(piece[\"species\"].value_counts(), fill_value = 0)\n\ntot\n\n&lt;class 'pandas.io.parsers.readers.TextFileReader'&gt;\n\n\nAdelie       152.0\nChinstrap     68.0\nGentoo       124.0\ndtype: float64\n\n\n\n\nImport Semi-Clean Data\nUnclean data issues skips rows, comments, punctuation, etc.\nWe can import a subset of the columns using usecols and change their names (header = 0; names = [list]).\n\npenguins_custom = pd.read_csv(\n  \"data/penguins.csv\", \n  usecols = [0,1,6],\n  header = 0, \n  names = [\"Species\", \"Island\", \"Sex\"]\n  )\n\npenguins_custom.head(5)\n\n\n\n\n\n\n\n\nSpecies\nIsland\nSex\n\n\n\n\n0\nAdelie\nTorgersen\nmale\n\n\n1\nAdelie\nTorgersen\nfemale\n\n\n2\nAdelie\nTorgersen\nfemale\n\n\n3\nAdelie\nTorgersen\nNaN\n\n\n4\nAdelie\nTorgersen\nfemale\n\n\n\n\n\n\n\n\n\n\nWriting Data to Text Format\nTo write to a csv file, we can use pandas DataFrame’s to_csv method with index = False so the row numbers are not stored in the first column. Missing values are written as empty strings, we can specify a placeholder with na_rep = \"NA\":\n\npenguins_custom.to_csv(\n  \"data/penguins_custom.csv\", \n  index = False,\n  na_rep = \"NA\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorking with Other Delimited Formats\n\nReading\nIn case your tabular data makes pandas trip up and you need a little extra manual processing, you can use Python’s built in csv module.\nLet’s read in the penguins dataset the hard, manual way.\n\nimport csv\n\npenguin_reader = csv.reader(penguins)\n\nprint(penguin_reader)\n\n&lt;_csv.reader object at 0x0000026704996340&gt;\n\n\nNow we have the _csv_reader object.\nNext, Wes iterated through the reader to print the lines, which seems to only give me the row with my headings.\n\nfor line in penguin_reader:\n  print(line)\n\n['species']\n['island']\n['bill_length_mm']\n['bill_depth_mm']\n['flipper_length_mm']\n['body_mass_g']\n['sex']\n['month']\n['day']\n['year']\n\n\nWe’ll keep following along to wrangle it into a form we can use:\n\nwith open(\"data/penguins.csv\") as penguin_reader:\n  lines = list(csv.reader(penguin_reader))\n  \nheader, values = lines[0], lines[1:]\n\nprint(header)\nprint(values[5])\n\n['species', 'island', 'bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g', 'sex', 'month', 'day', 'year']\n['Adelie', 'Torgersen', '39.3', '20.6', '190', '3650', 'male', '3', '3', '2007']\n\n\nNow we have two lists: header and values. We use a dictionary of data columns and the expression zip(*values). This combination of dictionary comprehension and expression is generally faster than iterating through a loop. However, Wes warns that this can use a lot of memory on large files.\n\npenguin_dict = {h: v for h, v in zip(header, zip(*values))}\n\n# too big to print and I'm not sure how to print a select few key-value pairs\n\n\n\n\n\n\n\nRecall\n\n\n\nFor a reminder on dictionary comprehensions, see Chapter 3.\n\n\nNow to finally get this into a usable dataframe we’ll use pandas DataFrame from_dict method!\n\npenguin_df = pd.DataFrame.from_dict(penguin_dict)\npenguin_df.head(5)\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nmonth\nday\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181\n3750\nmale\n4\n10\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186\n3800\nfemale\n3\n6\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18\n195\n3250\nfemale\n7\n22\n2007\n\n\n3\nAdelie\nTorgersen\nNA\nNA\nNA\nNA\nNA\n2\n13\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193\n3450\nfemale\n8\n21\n2007\n\n\n\n\n\n\n\n\n\ncsv.Dialect\nSince there are many kinds of delimited files, string quoting conventions, and line terminators, you may find yourself wanting to define a “Dialect” to read in your delimited file. The options available are found in Table 6.3.\nYou can either define a csv.Dialect subclass or pass dialect parameters to csv.reader.\n\n# option 1\n\n## define a dialect subclass\n\nclass my_dialect(csv.Dialect):\n    lineterminator = \"\\n\"\n    delimiter = \";\"\n    quotechar = '\"'\n    quoting = csv.QUOTE_MINIMAL\n    \n## use the subclass\n\nreader = csv.reader(penguins, dialect = my_dialect)\n\n# option 2\n\n## pass just dialect parameters\n\nreader = csv.reader(penguins, delimiter = \",\")\n\n\n\n\n\n\n\nRecap for when to use what?\n\n\n\nFor most data, pandas read_* functions, plus the overwhelming number of options, will likely get you close to what you need.\nIf there are additional, minor wrangling needs, you can try using Python’s csv.reader with either a csv.Dialect subclass or just by passing in dialect parameters.\nIf you have complicated or multicharacter delimiters, you’ll likely need to import the string module and use the split method or regular expression method re.split.\n\n\n\n\nWriting\ncsv.writer is the companion to csv.reader with the same dialect and format options. The first argument in open is the path and filename you want to write to and the second argument \"w\" makes the file writeable.\n\n\n\n\n\n\nNote\n\n\n\nPython documentation notes that newline=\"\" should be specified in case there are newlines embedded inside quoted fields to ensure they are interpreted correctly.\n\n\n\nwith open(\"data/write_data.csv\", \"w\", newline = \"\") as f:\n    writer = csv.writer(f, dialect = my_dialect)\n    writer.writerow((\"one\", \"two\", \"three\"))\n    writer.writerow((\"1\", \"2\", \"3\"))\n    writer.writerow((\"4\", \"5\", \"6\"))\n    writer.writerow((\"7\", \"8\", \"9\"))\n\n\n\nJavaScript Object Notation (JSON) Data\nStandard format for HTTP requests between web browsers, applications, and APIs. Its almost valid Python code:\n\nInstead of NaN, it uses null\nDoesn’t allow trailing commas at end of lists\nData types: objects (dictionaries), arrays (lists), strings, numbers, booleans, and nulls.\n\nWe’ll make up a simple file of my pets’ names, types, and sex to demonstrate JSON data loading and writing.\n\nImport the json module and use json.loads to convert a JSON string to Python. There are multiple ways to convert JSON objects to a DataFrame.\n\nimport json\n\nobj = \"\"\"\n{\"name\": \"Jadey\",\n \"pets\": [{\"name\": \"Mai\", \"type\": \"cat\", \"sex\": \"Female\"},\n          {\"name\": \"Tai\", \"type\": \"cat\", \"sex\": \"Male\"},\n          {\"name\": \"Skye\", \"type\": \"cat\", \"sex\": \"Female\"}]\n}\n\"\"\"\n\njson_to_py = json.loads(obj)\n\nprint(json_to_py)\ntype(json_to_py)\n\n{'name': 'Jadey', 'pets': [{'name': 'Mai', 'type': 'cat', 'sex': 'Female'}, {'name': 'Tai', 'type': 'cat', 'sex': 'Male'}, {'name': 'Skye', 'type': 'cat', 'sex': 'Female'}]}\n\n\ndict\n\n\nSince this imported the object as a dictionary, we can use pd.DataFrame to create a DataFrame of the pets’ names, type, and sex.\n\npets_df = pd.DataFrame(json_to_py[\"pets\"], columns = [\"name\", \"type\", \"sex\"])\n\nprint(type(pets_df))\npets_df\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n\n\n\n\n\n\n\n\n\nname\ntype\nsex\n\n\n\n\n0\nMai\ncat\nFemale\n\n\n1\nTai\ncat\nMale\n\n\n2\nSkye\ncat\nFemale\n\n\n\n\n\n\n\nUse json.dumps to convert from Python (class: dictionary) back to JSON (class: string).\n\npy_to_json = json.dumps(json_to_py)\n\nprint(\"json_to_py type:\", type(json_to_py))\nprint(\"py_to_json type:\", type(py_to_json))\npy_to_json\n\njson_to_py type: &lt;class 'dict'&gt;\npy_to_json type: &lt;class 'str'&gt;\n\n\n'{\"name\": \"Jadey\", \"pets\": [{\"name\": \"Mai\", \"type\": \"cat\", \"sex\": \"Female\"}, {\"name\": \"Tai\", \"type\": \"cat\", \"sex\": \"Male\"}, {\"name\": \"Skye\", \"type\": \"cat\", \"sex\": \"Female\"}]}'\n\n\nWe can use pandas pd.read_json function and to_json DataFrame method to read and write JSON files.\n\npets_df.to_json(\"data/pets.json\")\n\nWe can easily import a JSON file using pandas.read_json.\n\npet_data = pd.read_json(\"data/pets.json\")\npet_data\n\n\n\n\n\n\n\n\nname\ntype\nsex\n\n\n\n\n0\nMai\ncat\nFemale\n\n\n1\nTai\ncat\nMale\n\n\n2\nSkye\ncat\nFemale\n\n\n\n\n\n\n\n\n\n\nWeb Scraping\n\nHTML\npd.read_html uses libraries to read and write HTML and XML:\n\nTry: xlml [faster]\nCatch: beautifulsoup4 and html5lib [better equipped for malformed files]\n\nIf you want to specify which parsing engine is used, you can use the flavor argument.\n\ntables = pd.read_html(\n  \"https://www.fdic.gov/resources/resolutions/bank-failures/failed-bank-list/\", \n  flavor = \"html5lib\"\n  )\n\nprint(\"Table Length:\", len(tables))\n\n# since this outputs a list of tables, we can grab just the first table\n\ntables[0].head(5)\n\nTable Length: 1\n\n\n\n\n\n\n\n\n\nBank NameBank\nCityCity\nStateSt\nCertCert\nAcquiring InstitutionAI\nClosing DateClosing\nFundFund\n\n\n\n\n0\nAlmena State Bank\nAlmena\nKS\n15426\nEquity Bank\nOctober 23, 2020\n10538\n\n\n1\nFirst City Bank of Florida\nFort Walton Beach\nFL\n16748\nUnited Fidelity Bank, fsb\nOctober 16, 2020\n10537\n\n\n2\nThe First State Bank\nBarboursville\nWV\n14361\nMVB Bank, Inc.\nApril 3, 2020\n10536\n\n\n3\nEricson State Bank\nEricson\nNE\n18265\nFarmers and Merchants Bank\nFebruary 14, 2020\n10535\n\n\n4\nCity National Bank of New Jersey\nNewark\nNJ\n21111\nIndustrial Bank\nNovember 1, 2019\n10534\n\n\n\n\n\n\n\n\n\nXML\nXML format is more general than HTML, but they are structurally similar. See pandas documentation for pd.read_xml.\nThis snippet of an xml file is from Microsoft.\n&lt;catalog&gt;\n   &lt;book id=\"bk101\"&gt;\n      &lt;author&gt;Gambardella, Matthew&lt;/author&gt;\n      &lt;title&gt;XML Developer's Guide&lt;/title&gt;\n      &lt;genre&gt;Computer&lt;/genre&gt;\n      &lt;price&gt;44.95&lt;/price&gt;\n      &lt;publish_date&gt;2000-10-01&lt;/publish_date&gt;\n      &lt;description&gt;An in-depth look at creating applications \n      with XML.&lt;/description&gt;\n   &lt;/book&gt;\n\nbooks = pd.read_xml(\"data/books.xml\")\n\nbooks.head(5)\n\n\n\n\n\n\n\n\nid\nauthor\ntitle\ngenre\nprice\npublish_date\ndescription\n\n\n\n\n0\nbk101\nGambardella, Matthew\nXML Developer's Guide\nComputer\n44.95\n2000-10-01\nAn in-depth look at creating applications \\n ...\n\n\n1\nbk102\nRalls, Kim\nMidnight Rain\nFantasy\n5.95\n2000-12-16\nA former architect battles corporate zombies, ...\n\n\n2\nbk103\nCorets, Eva\nMaeve Ascendant\nFantasy\n5.95\n2000-11-17\nAfter the collapse of a nanotechnology \\n ...\n\n\n3\nbk104\nCorets, Eva\nOberon's Legacy\nFantasy\n5.95\n2001-03-10\nIn post-apocalypse England, the mysterious \\n ...\n\n\n4\nbk105\nCorets, Eva\nThe Sundered Grail\nFantasy\n5.95\n2001-09-10\nThe two daughters of Maeve, half-sisters, \\n ...\n\n\n\n\n\n\n\nIf you’d like to manually parse a file, Wes demonstrates this process in the textbook, before demonstrating how the following steps are turned into one line of code using pd.read_xml.\n\nfrom lxml import objectify\nUse lxml.objectify,\nCreate a dictionary of tag names to data values\nCnvert that list of dictionaries into a DataFrame.\n\n\n\n\nBinary Data Formats\n\nPickle\nPython has a built-in pickle module that converts pandas objects into the pickle format (serializes the data into a byte stream), which is generally readable only in Python.\nMore information can be found in Python documentation.\nHere’s a demo to show pickling and unpickling the penguins dataset.\n\nprint(\"Unpickled penguins type:\", type(penguins))\n\npenguins.to_pickle(\"data/penguins_pickle\")\n\n# do some machine learning\n\npickled_penguins = pd.read_pickle(\"data/penguins_pickle\")\npickled_penguins\n\nUnpickled penguins type: &lt;class 'pandas.core.frame.DataFrame'&gt;\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nmonth\nday\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n4\n10\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n3\n6\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n7\n22\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2\n13\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n8\n21\n2007\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n339\nChinstrap\nDream\n55.8\n19.8\n207.0\n4000.0\nmale\n6\n4\n2009\n\n\n340\nChinstrap\nDream\n43.5\n18.1\n202.0\n3400.0\nfemale\n6\n8\n2009\n\n\n341\nChinstrap\nDream\n49.6\n18.2\n193.0\n3775.0\nmale\n4\n8\n2009\n\n\n342\nChinstrap\nDream\n50.8\n19.0\n210.0\n4100.0\nmale\n8\n24\n2009\n\n\n343\nChinstrap\nDream\n50.2\n18.7\n198.0\n3775.0\nfemale\n2\n11\n2009\n\n\n\n\n344 rows × 10 columns\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\npickle is recommended only as a short-term storage format (i.e. loading and unloading your machine learning models) because the format may not be stable over time. Also, the module is not secure – pickle data can be maliciously tampered with. Python docs recommend signing data with hmac to ensure it hasn’t been tampered with.\n\n\n\n\nMicrosoft Excel Files\npd.ExcelFile class or pd.read_excel functions use packages xlrd (for older .xlx files) and openpyxl (for newer .xlsx files), which must be installed separately from pandas.\nconda install xlrd openpyxl\npd.read_excel takes most of the same arguments as pd.read_csv.\n\npenguins_excel = pd.read_excel(\n  \"data/penguins.xlsx\", \n  index_col = \"species\",\n  parse_dates = {\"date\": [\"month\", \"day\", \"year\"]}\n)\n\npenguins_excel.head(5)\n\n\n\n\n\n\n\n\ndate\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\nspecies\n\n\n\n\n\n\n\n\n\n\n\nAdelie\n2007-04-10\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n\n\nAdelie\n2007-03-06\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n\n\nAdelie\n2007-07-22\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n\n\nAdelie\n2007-02-13\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nAdelie\n2007-08-21\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n\n\n\n\n\n\n\nTo read multiple sheets, use pd.ExcelFile.\n\npenguins_sheets = pd.ExcelFile(\"data/penguins_sheets.xlsx\")\n\nprint(\"Available sheet names:\", penguins_sheets.sheet_names)\n\npenguins_sheets\n\nAvailable sheet names: ['chinstrap', 'gentoo', 'adelie']\n\n\n&lt;pandas.io.excel._base.ExcelFile at 0x26705acdd90&gt;\n\n\nThen we can parse all sheets into a dictionary by specifying the sheet_name argument as None. Or, we can read in a subset of sheets.\n\nsheets = penguins_sheets.parse(sheet_name = None)\n\nsheets\n\n{'chinstrap':       species island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n 0   Chinstrap  Dream            46.5           17.9                192   \n 1   Chinstrap  Dream            50.0           19.5                196   \n 2   Chinstrap  Dream            51.3           19.2                193   \n 3   Chinstrap  Dream            45.4           18.7                188   \n 4   Chinstrap  Dream            52.7           19.8                197   \n ..        ...    ...             ...            ...                ...   \n 63  Chinstrap  Dream            55.8           19.8                207   \n 64  Chinstrap  Dream            43.5           18.1                202   \n 65  Chinstrap  Dream            49.6           18.2                193   \n 66  Chinstrap  Dream            50.8           19.0                210   \n 67  Chinstrap  Dream            50.2           18.7                198   \n \n     body_mass_g     sex  month  day  year  \n 0          3500  female      7    4  2007  \n 1          3900    male      9    6  2007  \n 2          3650    male      4   15  2007  \n 3          3525  female      6   10  2007  \n 4          3725    male      8   19  2007  \n ..          ...     ...    ...  ...   ...  \n 63         4000    male      6    4  2009  \n 64         3400  female      6    8  2009  \n 65         3775    male      4    8  2009  \n 66         4100    male      8   24  2009  \n 67         3775  female      2   11  2009  \n \n [68 rows x 10 columns],\n 'gentoo':     species  island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n 0    Gentoo  Biscoe            46.1           13.2              211.0   \n 1    Gentoo  Biscoe            50.0           16.3              230.0   \n 2    Gentoo  Biscoe            48.7           14.1              210.0   \n 3    Gentoo  Biscoe            50.0           15.2              218.0   \n 4    Gentoo  Biscoe            47.6           14.5              215.0   \n ..      ...     ...             ...            ...                ...   \n 119  Gentoo  Biscoe             NaN            NaN                NaN   \n 120  Gentoo  Biscoe            46.8           14.3              215.0   \n 121  Gentoo  Biscoe            50.4           15.7              222.0   \n 122  Gentoo  Biscoe            45.2           14.8              212.0   \n 123  Gentoo  Biscoe            49.9           16.1              213.0   \n \n      body_mass_g     sex  month  day  year  \n 0         4500.0  female      3    8  2007  \n 1         5700.0    male      2    4  2007  \n 2         4450.0  female      7    1  2007  \n 3         5700.0    male      9   15  2007  \n 4         5400.0    male     11   19  2007  \n ..           ...     ...    ...  ...   ...  \n 119          NaN     NaN     12   11  2009  \n 120       4850.0  female      7   20  2009  \n 121       5750.0    male      9   18  2009  \n 122       5200.0  female     12   11  2009  \n 123       5400.0    male      6   15  2009  \n \n [124 rows x 10 columns],\n 'adelie':     species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n 0    Adelie  Torgersen            39.1           18.7              181.0   \n 1    Adelie  Torgersen            39.5           17.4              186.0   \n 2    Adelie  Torgersen            40.3           18.0              195.0   \n 3    Adelie  Torgersen             NaN            NaN                NaN   \n 4    Adelie  Torgersen            36.7           19.3              193.0   \n ..      ...        ...             ...            ...                ...   \n 147  Adelie      Dream            36.6           18.4              184.0   \n 148  Adelie      Dream            36.0           17.8              195.0   \n 149  Adelie      Dream            37.8           18.1              193.0   \n 150  Adelie      Dream            36.0           17.1              187.0   \n 151  Adelie      Dream            41.5           18.5              201.0   \n \n      body_mass_g     sex  month  day  year  \n 0         3750.0    male      4   10  2007  \n 1         3800.0  female      3    6  2007  \n 2         3250.0  female      7   22  2007  \n 3            NaN     NaN      2   13  2007  \n 4         3450.0  female      8   21  2007  \n ..           ...     ...    ...  ...   ...  \n 147       3475.0  female     11    4  2009  \n 148       3450.0  female      5   21  2009  \n 149       3750.0    male      8   15  2009  \n 150       3700.0  female      1   16  2009  \n 151       4000.0    male      5    8  2009  \n \n [152 rows x 10 columns]}\n\n\nThen we can subset one of the sheets as a pandas DataFrame object.\n\nchinstrap = sheets[\"chinstrap\"].head(5)\nchinstrap\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nmonth\nday\nyear\n\n\n\n\n0\nChinstrap\nDream\n46.5\n17.9\n192\n3500\nfemale\n7\n4\n2007\n\n\n1\nChinstrap\nDream\n50.0\n19.5\n196\n3900\nmale\n9\n6\n2007\n\n\n2\nChinstrap\nDream\n51.3\n19.2\n193\n3650\nmale\n4\n15\n2007\n\n\n3\nChinstrap\nDream\n45.4\n18.7\n188\n3525\nfemale\n6\n10\n2007\n\n\n4\nChinstrap\nDream\n52.7\n19.8\n197\n3725\nmale\n8\n19\n2007\n\n\n\n\n\n\n\nWrite one sheet to using to_excel:\n\nchinstrap.to_excel(\"data/chinstrap.xlsx\")\n\nIf you want to write to multiple sheets, create an ExcelWriter class and then write the data to it:\n\ngentoo = sheets[\"gentoo\"].head(5)\n\nwriter = pd.ExcelWriter(\"data/chinstrap_gentoo.xlsx\")\n\nchinstrap.to_excel(writer, sheet_name = \"chinstrap\")\n\ngentoo.to_excel(writer, sheet_name = \"gentoo\")\n\nwriter.save()\n\n\n\nHDF5 Format\nHierarchical data format (HDF) is used in Python, C, Java, Julia, MATLAB, and others for storing big scientific array data (multiple datasets and metadata within one file). HDF5 can be used to efficiently read/write chunks of large arrays.\nThe PyTables package must first be installed.\nconda install pytables\n\npip install tables # the package is called \"tables\" in PyPI\npandas provides an dictionary-like-class for HDF5 files called HDFStore:\n\nstore = pd.HDFStore(\"data/pets.h5\")\n\nstore[\"pets\"] = pets_df\nstore[\"pets\"]\n\n\n\n\n\n\n\n\nname\ntype\nsex\n\n\n\n\n0\nMai\ncat\nFemale\n\n\n1\nTai\ncat\nMale\n\n\n2\nSkye\ncat\nFemale\n\n\n\n\n\n\n\nHDFStore can store data as a fixed or as a table schema. Table allows querying but is generally slower.\n\npets_df.to_hdf(\"data/petnames.h5\", \"pets\", format = \"table\")\npd.read_hdf(\"data/petnames.h5\", \"pets\", where=[\"columns = name\"])\n\n\n\n\n\n\n\n\nname\n\n\n\n\n0\nMai\n\n\n1\nTai\n\n\n2\nSkye\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhen should I use HDF5?\n\n\n\nWes recommends using HDF5 for write-once, read-many datasets that are worked with locally. If your data is stored on remote servers, then you may try other binary formats designed for distributed storage (for example, Apache Parquet).\n\n\n\n\n\nInteracting with Web APIs\nTo access data from APIs, Wes suggests using the requests package.\nconda install requests\nLet’s pull from this free zoo animal API.\n\nimport requests\n\nurl = \"https://zoo-animal-api.herokuapp.com/animals/rand\"\n\nresp = requests.get(url)\n\nresp.raise_for_status()\n\nprint(\"HTTP status\", resp)\n\nanimal = resp.json()\nanimal\n\nanimal_df = pd.DataFrame([animal]) # important to wrap the dictionary object into a list\nanimal_df\n\nHTTP status &lt;Response [200]&gt;\n\n\n\n\n\n\n\n\n\nname\nlatin_name\nanimal_type\nactive_time\nlength_min\nlength_max\nweight_min\nweight_max\nlifespan\nhabitat\ndiet\ngeo_range\nimage_link\nid\n\n\n\n\n0\nBrazilian Porcupine\nCoendou prehensilis\nMammal\nNocturnal\n1.5\n1.7\n9\n11\n17\nTropical rainforest\nLeaves, bark, fruits, shoots and small animals\nNorthern and eastern South America\nhttps://upload.wikimedia.org/wikipedia/commons...\n45\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIt is important to note that the dictionary is wrapped into a list. If it isn’t, then you will get the following error: ValueError: If using all scalar values, you must pass an index.\n\n\n\n\nInteracting with Databases\nSome popular SQL-based relational databases are: SQL Server, PostgreSQL, MySQL, SQLite3. We can use pandas to load the results of a SQL query into a DataFrame.\nImport sqlite3 and create a database.\n\nimport sqlite3\n\ncon = sqlite3.connect(\"data/data.sqlite\")\n\nThis creates a table.\n\nquery = \"\"\"\n  CREATE TABLE states\n  (Capital VARCHAR(20), State VARCHAR(20),\n  x1 REAL, x2 INTEGER\n);\"\"\"\n\ncon.execute(query)\n\ncon.commit()\n\nThis inserts the rows of data:\n\ndata = [(\"Atlanta\", \"Georgia\", 1.25, 6), (\"Seattle\", \"Washington\", 2.6, 3), (\"Sacramento\", \"California\", 1.7, 5)]\n        \nstmt = \"INSERT INTO states VALUES(?, ?, ?, ?)\"\n\ncon.executemany(stmt, data)\n\ncon.commit()\n\nNow we can look at the data:\n\ncursor = con.execute(\"SELECT * FROM states\")\n\nrows = cursor.fetchall()\n\nrows\n\n[('Atlanta', 'Georgia', 1.25, 6),\n ('Seattle', 'Washington', 2.6, 3),\n ('Sacramento', 'California', 1.7, 5),\n ('Atlanta', 'Georgia', 1.25, 6),\n ('Seattle', 'Washington', 2.6, 3),\n ('Sacramento', 'California', 1.7, 5),\n ('Atlanta', 'Georgia', 1.25, 6),\n ('Seattle', 'Washington', 2.6, 3),\n ('Sacramento', 'California', 1.7, 5),\n ('Atlanta', 'Georgia', 1.25, 6),\n ('Seattle', 'Washington', 2.6, 3),\n ('Sacramento', 'California', 1.7, 5),\n ('Atlanta', 'Georgia', 1.25, 6),\n ('Seattle', 'Washington', 2.6, 3),\n ('Sacramento', 'California', 1.7, 5),\n ('Atlanta', 'Georgia', 1.25, 6),\n ('Seattle', 'Washington', 2.6, 3),\n ('Sacramento', 'California', 1.7, 5),\n ('Atlanta', 'Georgia', 1.25, 6),\n ('Seattle', 'Washington', 2.6, 3),\n ('Sacramento', 'California', 1.7, 5)]\n\n\nTo get the data into a pandas DataFrame, we’ll need to provide column names in the cursor.description.\n\nprint(cursor.description)\n\npd.DataFrame(rows, columns = [x[0] for x in cursor.description])\n\n(('Capital', None, None, None, None, None, None), ('State', None, None, None, None, None, None), ('x1', None, None, None, None, None, None), ('x2', None, None, None, None, None, None))\n\n\n\n\n\n\n\n\n\nCapital\nState\nx1\nx2\n\n\n\n\n0\nAtlanta\nGeorgia\n1.25\n6\n\n\n1\nSeattle\nWashington\n2.60\n3\n\n\n2\nSacramento\nCalifornia\n1.70\n5\n\n\n3\nAtlanta\nGeorgia\n1.25\n6\n\n\n4\nSeattle\nWashington\n2.60\n3\n\n\n5\nSacramento\nCalifornia\n1.70\n5\n\n\n6\nAtlanta\nGeorgia\n1.25\n6\n\n\n7\nSeattle\nWashington\n2.60\n3\n\n\n8\nSacramento\nCalifornia\n1.70\n5\n\n\n9\nAtlanta\nGeorgia\n1.25\n6\n\n\n10\nSeattle\nWashington\n2.60\n3\n\n\n11\nSacramento\nCalifornia\n1.70\n5\n\n\n12\nAtlanta\nGeorgia\n1.25\n6\n\n\n13\nSeattle\nWashington\n2.60\n3\n\n\n14\nSacramento\nCalifornia\n1.70\n5\n\n\n15\nAtlanta\nGeorgia\n1.25\n6\n\n\n16\nSeattle\nWashington\n2.60\n3\n\n\n17\nSacramento\nCalifornia\n1.70\n5\n\n\n18\nAtlanta\nGeorgia\n1.25\n6\n\n\n19\nSeattle\nWashington\n2.60\n3\n\n\n20\nSacramento\nCalifornia\n1.70\n5\n\n\n\n\n\n\n\nAs per usual, Wes likes to show us the manual way first and then the easier version. Using SQLAlchemy, we can must less verbosely create our DataFrame.\n\nimport sqlalchemy as sqla\n\ndb = sqla.create_engine(\"sqlite:///data/data.sqlite\")\n\npd.read_sql(\"SELECT * FROM states\", db)\n\n\n\n\n\n\n\n\nCapital\nState\nx1\nx2\n\n\n\n\n0\nAtlanta\nGeorgia\n1.25\n6\n\n\n1\nSeattle\nWashington\n2.60\n3\n\n\n2\nSacramento\nCalifornia\n1.70\n5\n\n\n3\nAtlanta\nGeorgia\n1.25\n6\n\n\n4\nSeattle\nWashington\n2.60\n3\n\n\n5\nSacramento\nCalifornia\n1.70\n5\n\n\n6\nAtlanta\nGeorgia\n1.25\n6\n\n\n7\nSeattle\nWashington\n2.60\n3\n\n\n8\nSacramento\nCalifornia\n1.70\n5\n\n\n9\nAtlanta\nGeorgia\n1.25\n6\n\n\n10\nSeattle\nWashington\n2.60\n3\n\n\n11\nSacramento\nCalifornia\n1.70\n5\n\n\n12\nAtlanta\nGeorgia\n1.25\n6\n\n\n13\nSeattle\nWashington\n2.60\n3\n\n\n14\nSacramento\nCalifornia\n1.70\n5\n\n\n15\nAtlanta\nGeorgia\n1.25\n6\n\n\n16\nSeattle\nWashington\n2.60\n3\n\n\n17\nSacramento\nCalifornia\n1.70\n5\n\n\n18\nAtlanta\nGeorgia\n1.25\n6\n\n\n19\nSeattle\nWashington\n2.60\n3\n\n\n20\nSacramento\nCalifornia\n1.70\n5",
    "crumbs": [
      "6. Data Loading, Storage, and File Formats",
      "Notes"
    ]
  },
  {
    "objectID": "06_video.html",
    "href": "06_video.html",
    "title": "Video",
    "section": "",
    "text": "Cohort 01",
    "crumbs": [
      "6. Data Loading, Storage, and File Formats",
      "Video"
    ]
  },
  {
    "objectID": "06_video.html#cohort-01",
    "href": "06_video.html#cohort-01",
    "title": "Video",
    "section": "",
    "text": "00:04:53    phanikumar tata:    hello everyone , got some issues with my camera\n00:57:00    Jim Gruman: thank you Jadey!!\n00:57:30    phanikumar tata:    Great presentation\n00:57:47    phanikumar tata:    can you\n00:58:14    phanikumar tata:    you show you how u r doing .QMD plese\n00:58:33    phanikumar tata:    Quearto to run Python\n00:58:37    Isabella Velásquez: quarto --help\n01:00:22    Isabella Velásquez: https://www.youtube.com/watch?v=MaEjLS2ouGg&list=PL3x6DOfs2NGh7IQIQ_pXNkjLVKa-7lgCw&index=2\n01:00:50    phanikumar tata:    do u have a link\n01:00:56    phanikumar tata:    Thanks a lot\n01:02:08    phanikumar tata:    Oh u need to install reticulate on top of Quearto\n01:03:06    Isabella Velásquez: pip install nbformat\n01:04:33    Isabella Velásquez: you do if you're using RStudio. If you're using Jupyter Lab or VS Code, you do not\n01:04:54    Isabella Velásquez: actually, you may not need reticulate on RStudio - have to double check\n01:07:51    phanikumar tata:    some times\n01:07:59    phanikumar tata:    .lock file would fail it\n01:08:02    phanikumar tata:    to work",
    "crumbs": [
      "6. Data Loading, Storage, and File Formats",
      "Video"
    ]
  },
  {
    "objectID": "07_video.html",
    "href": "07_video.html",
    "title": "Video",
    "section": "",
    "text": "Cohort 01",
    "crumbs": [
      "7. Data Cleaning and Preparation",
      "Video"
    ]
  },
  {
    "objectID": "07_video.html#cohort-01",
    "href": "07_video.html#cohort-01",
    "title": "Video",
    "section": "",
    "text": "00:10:09    Isabella Velásquez: ahhh I had to set my interpreter in RStudio (via Global options&gt;Python)! that's why it worked from the terminal but not from the Render button.",
    "crumbs": [
      "7. Data Cleaning and Preparation",
      "Video"
    ]
  },
  {
    "objectID": "08_video.html",
    "href": "08_video.html",
    "title": "Video",
    "section": "",
    "text": "Cohort 01",
    "crumbs": [
      "8. Data Wrangling: Join, Combine, and Reshape",
      "Video"
    ]
  },
  {
    "objectID": "08_video.html#cohort-01",
    "href": "08_video.html#cohort-01",
    "title": "Video",
    "section": "",
    "text": "00:27:19    Isabella Velásquez: dplyr has suffixes! I have never used them: https://dplyr.tidyverse.org/reference/mutate-joins.html\n00:58:49    Layla Bouzoubaa:    i think there is something similar with r\n00:59:11    Ron:    https://dataprep.ai/\n00:59:13    Layla Bouzoubaa:    actually i think janitor pkg has a func that does reporting",
    "crumbs": [
      "8. Data Wrangling: Join, Combine, and Reshape",
      "Video"
    ]
  },
  {
    "objectID": "09_video.html",
    "href": "09_video.html",
    "title": "Video",
    "section": "",
    "text": "Cohort 01",
    "crumbs": [
      "9. Plotting and Visualization",
      "Video"
    ]
  },
  {
    "objectID": "09_video.html#cohort-01",
    "href": "09_video.html#cohort-01",
    "title": "Video",
    "section": "",
    "text": "00:26:25    Layla Bouzoubaa:    dev.new\n00:26:26    Jadey Ryan: dev.new()\n00:26:58    Jadey Ryan: grDevices\n00:27:53    Layla Bouzoubaa:    could be an issue to report\n00:40:08    shamsuddeen:    ggpy: https://github.com/yhat/ggpy\n00:41:31    shamsuddeen:    plotnine : https://github.com/has2k1/plotnine\n01:01:35    Jadey Ryan: thank you!!",
    "crumbs": [
      "9. Plotting and Visualization",
      "Video"
    ]
  },
  {
    "objectID": "10_video.html",
    "href": "10_video.html",
    "title": "Video",
    "section": "",
    "text": "Cohort 01",
    "crumbs": [
      "10. Data Aggregation and Group Operations",
      "Video"
    ]
  },
  {
    "objectID": "10_video.html#cohort-01",
    "href": "10_video.html#cohort-01",
    "title": "Video",
    "section": "",
    "text": "00:02:37    Oluwafemi Oyedele:  Hi, Jim, Hi Ron, Good Evening!!!\n00:09:47    Ron:    command shift enter on mac, not sure on windows\n00:41:50    Oluwafemi Oyedele:  Thank you !!!",
    "crumbs": [
      "10. Data Aggregation and Group Operations",
      "Video"
    ]
  },
  {
    "objectID": "11_video.html",
    "href": "11_video.html",
    "title": "Video",
    "section": "",
    "text": "Cohort 01",
    "crumbs": [
      "11. Time Series",
      "Video"
    ]
  },
  {
    "objectID": "12_video.html",
    "href": "12_video.html",
    "title": "Video",
    "section": "",
    "text": "Cohort 01",
    "crumbs": [
      "12. Introduction to Modeling Libraries in Python",
      "Video"
    ]
  },
  {
    "objectID": "12_video.html#cohort-01",
    "href": "12_video.html#cohort-01",
    "title": "Video",
    "section": "",
    "text": "00:32:55    shamsuddeen:    I think we will finish early today. I will demo using VScode for translating R code to Python using Copilot for few minutes after the presentation\n00:35:45    Ron:    AR = Autoregression\n00:36:22    Ron:    Sounds like 'getting angry at yourself' ;)\n00:40:55    Ron:    Wow that yellowbrick looks great!",
    "crumbs": [
      "12. Introduction to Modeling Libraries in Python",
      "Video"
    ]
  },
  {
    "objectID": "13_video.html",
    "href": "13_video.html",
    "title": "Video",
    "section": "",
    "text": "Cohort 01",
    "crumbs": [
      "13. Data Analysis Examples",
      "Video"
    ]
  },
  {
    "objectID": "13_video.html#cohort-01",
    "href": "13_video.html#cohort-01",
    "title": "Video",
    "section": "",
    "text": "00:28:14    Ron:    https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.argsort.html\n00:38:36    Ron:    I am switching computers carry on brb :)\n00:40:47    Ronald Legere:  back\n00:53:27    Layla Bouzoubaa:    https://course.fast.ai/Resources/book.html",
    "crumbs": [
      "13. Data Analysis Examples",
      "Video"
    ]
  },
  {
    "objectID": "example_quarto.html",
    "href": "example_quarto.html",
    "title": "Example Quarto Document",
    "section": "",
    "text": "Quarto\nQuarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org.",
    "crumbs": [
      "Examples",
      "Example Quarto Document"
    ]
  },
  {
    "objectID": "example_quarto.html#running-code",
    "href": "example_quarto.html#running-code",
    "title": "Example Quarto Document",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n2\n\n\nYou can add options to executable code like this\n\n\n4\n\n\nThe echo: false option disables the printing of code (only output is displayed).",
    "crumbs": [
      "Examples",
      "Example Quarto Document"
    ]
  },
  {
    "objectID": "how-to.html",
    "href": "how-to.html",
    "title": "How to add to the book",
    "section": "",
    "text": "Set up Quarto\nThis book is made with Quarto. Please see the Get Started chapter of the Quarto documentation to learn how to install and run Quarto in your IDE.",
    "crumbs": [
      "Appendices",
      "How to add to the book"
    ]
  },
  {
    "objectID": "how-to.html#add-to-book",
    "href": "how-to.html#add-to-book",
    "title": "How to add to the book",
    "section": "Add to book",
    "text": "Add to book\nOnce you have everything set up, forked the repo, and cloned to your computer, you can add a new chapter to the book.\nCreate a new file in the repository folder. For example, to create a new file called 01_exercises.qmd, navigate to the folder then create one using touch 01_exercises.qmd. If you are using VSCode, you can use the Quarto plug-in. You can use plain .md files, Quarto .qmd, or Jupyter .ipynb files in this book. Check out the files under Examples to see the various options.\nWrite in what you would like in the file.\nThen, in the _quarto.yml file, under chapters, add a part with your chapter. The file listed after part is the first page of chapter; the ones under chapters will be subpages.\n  - part: 01_main.qmd\n      chapters: \n      - 01_notes.qmd\n      - 01_video.qmd\n      - 01_exercises.qmd",
    "crumbs": [
      "Appendices",
      "How to add to the book"
    ]
  },
  {
    "objectID": "how-to.html#render-the-book",
    "href": "how-to.html#render-the-book",
    "title": "How to add to the book",
    "section": "Render the book",
    "text": "Render the book\nOnce you have added and edited your files, don’t forget to render the book. Run this in the terminal:\nquarto render --to html",
    "crumbs": [
      "Appendices",
      "How to add to the book"
    ]
  },
  {
    "objectID": "how-to.html#push-up-to-github",
    "href": "how-to.html#push-up-to-github",
    "title": "How to add to the book",
    "section": "Push up to GitHub",
    "text": "Push up to GitHub\nPush your changes to your forked repo and then create a pull request for the DSLC admins to merge your changes.\ngit add .\ngit commit -m \"Message here\"\ngit push",
    "crumbs": [
      "Appendices",
      "How to add to the book"
    ]
  },
  {
    "objectID": "01_main.html",
    "href": "01_main.html",
    "title": "1. Preliminaries",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "1. Preliminaries"
    ]
  },
  {
    "objectID": "02_main.html",
    "href": "02_main.html",
    "title": "2. Python Language Basics, IPython, and Jupyter Notebooks",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "2. Python Language Basics, IPython, and Jupyter Notebooks"
    ]
  },
  {
    "objectID": "03_main.html",
    "href": "03_main.html",
    "title": "3. Built-in Data Structures, Functions, and Files",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "3. Built-in Data Structures, Functions, and Files"
    ]
  },
  {
    "objectID": "03_main.html#learning-objectives",
    "href": "03_main.html#learning-objectives",
    "title": "3. Built-in Data Structures, Functions, and Files",
    "section": "",
    "text": "The data structures tuples, lists, dictionaries, and sets\nFunctions\nErrors and Exception Handling\nFiles and the Operating System",
    "crumbs": [
      "3. Built-in Data Structures, Functions, and Files"
    ]
  },
  {
    "objectID": "05_main.html",
    "href": "05_main.html",
    "title": "5. Getting Started with pandas",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "5. Getting Started with pandas"
    ]
  },
  {
    "objectID": "05_main.html#learning-objectives",
    "href": "05_main.html#learning-objectives",
    "title": "5. Getting Started with pandas",
    "section": "",
    "text": "Learn about Pandas two major data structures: Series and DataFrame\nLearn some essential functionality!",
    "crumbs": [
      "5. Getting Started with pandas"
    ]
  },
  {
    "objectID": "06_main.html",
    "href": "06_main.html",
    "title": "6. Data Loading, Storage, and File Formats",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "6. Data Loading, Storage, and File Formats"
    ]
  },
  {
    "objectID": "08_main.html",
    "href": "08_main.html",
    "title": "8. Data Wrangling: Join, Combine, and Reshape",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "8. Data Wrangling: Join, Combine, and Reshape"
    ]
  },
  {
    "objectID": "10_main.html",
    "href": "10_main.html",
    "title": "10. Data Aggregation and Group Operations",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "10. Data Aggregation and Group Operations"
    ]
  },
  {
    "objectID": "11_main.html",
    "href": "11_main.html",
    "title": "11. Time Series",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "11. Time Series"
    ]
  },
  {
    "objectID": "12_main.html",
    "href": "12_main.html",
    "title": "12. Introduction to Modeling Libraries in Python",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "12. Introduction to Modeling Libraries in Python"
    ]
  },
  {
    "objectID": "13_main.html",
    "href": "13_main.html",
    "title": "13. Data Analysis Examples",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "13. Data Analysis Examples"
    ]
  },
  {
    "objectID": "09_main.html",
    "href": "09_main.html",
    "title": "9. Plotting and Visualization",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "9. Plotting and Visualization"
    ]
  },
  {
    "objectID": "09_main.html#learning-objectives",
    "href": "09_main.html#learning-objectives",
    "title": "9. Plotting and Visualization",
    "section": "",
    "text": "We are going to learn the basic data visualization technique using matplotlib, pandas and seaborn.",
    "crumbs": [
      "9. Plotting and Visualization"
    ]
  },
  {
    "objectID": "09_main.html#import-the-necessary-library",
    "href": "09_main.html#import-the-necessary-library",
    "title": "9. Plotting and Visualization",
    "section": "import the necessary library",
    "text": "import the necessary library\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\n\nDemo_DatasetPrint\n\n\n\n```{python}\n#| eval: false\ndata = np.arange(10)\n```\n\n\n\n\n\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n\n\n\n\n\nCodeOutput\n\n\n\n```{python}\n#| eval: false\nplt.plot(data)\n```",
    "crumbs": [
      "9. Plotting and Visualization"
    ]
  },
  {
    "objectID": "09_main.html#we-can-use-plt.show-function-to-display-the-plot-in-quarto",
    "href": "09_main.html#we-can-use-plt.show-function-to-display-the-plot-in-quarto",
    "title": "9. Plotting and Visualization",
    "section": "We can use plt.show() function to display the plot in quarto",
    "text": "We can use plt.show() function to display the plot in quarto\n\nplt.show()\n\nWhen we are in jupyter notebook we can use %matplotlib notebook so that we can display the plot, but when we are in Ipython we can use %matplotlib to display the plot.",
    "crumbs": [
      "9. Plotting and Visualization"
    ]
  },
  {
    "objectID": "09_main.html#customization-of-the-visualization",
    "href": "09_main.html#customization-of-the-visualization",
    "title": "9. Plotting and Visualization",
    "section": "Customization of the visualization",
    "text": "Customization of the visualization\nWhile libraries like seaborn and pandas’s built-in plotting functions will deal with many of the mundane details of making plots, should you wish to customize them beyond the function options provided, you will need to learn a bit about the matplotlib API.",
    "crumbs": [
      "9. Plotting and Visualization"
    ]
  },
  {
    "objectID": "09_main.html#figures-and-subplots",
    "href": "09_main.html#figures-and-subplots",
    "title": "9. Plotting and Visualization",
    "section": "Figures and Subplots",
    "text": "Figures and Subplots\nPlots in matplotlib reside within a Figure object. You can create a new figure with plt.figure ()\n\nfig = plt.figure()\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\nplt.figure has a number of options; notably, figsize will guarantee the figure has a certain size and aspect ratio if saved to disk.\nYou can’t make a plot with a blank figure. You have to create one or more subplots using add_subplot",
    "crumbs": [
      "9. Plotting and Visualization"
    ]
  },
  {
    "objectID": "09_main.html#add-subplot",
    "href": "09_main.html#add-subplot",
    "title": "9. Plotting and Visualization",
    "section": "Add Subplot",
    "text": "Add Subplot\n\nax1 = fig.add_subplot(2,2, 1)\n\nax1\n\n&lt;AxesSubplot:&gt;\n\n\nThis means that the figure should be 2 × 2, and we’re selecting the first of four subplots (numbered from 1). We can add more subplot",
    "crumbs": [
      "9. Plotting and Visualization"
    ]
  },
  {
    "objectID": "09_main.html#we-can-add-more-subplot",
    "href": "09_main.html#we-can-add-more-subplot",
    "title": "9. Plotting and Visualization",
    "section": "We can add more subplot",
    "text": "We can add more subplot\n\nax2 = fig.add_subplot(2, 2, 2)\n\nax3 = fig.add_subplot(2, 2, 3)\n\nax2\n\nax3\n\n&lt;AxesSubplot:&gt;",
    "crumbs": [
      "9. Plotting and Visualization"
    ]
  },
  {
    "objectID": "09_main.html#adding-axis-methods-to-the-plot",
    "href": "09_main.html#adding-axis-methods-to-the-plot",
    "title": "9. Plotting and Visualization",
    "section": "Adding axis methods to the plot",
    "text": "Adding axis methods to the plot\nThese plot axis objects have various methods that create different types of plots, and it is preferred to use the axis methods over the top-level plotting functions like plt.show(). For example, we could make a line plot with the plot method.\n\nfig = plt.figure()\n\nax1 = fig.add_subplot(2, 2, 1)\n\nax2 = fig.add_subplot(2, 2, 2)\n\nax3 = fig.add_subplot(2, 2, 3)\n\nax3.plot(np.random.standard_normal(50).cumsum(), color=\"black\",\nlinestyle=\"dashed\")\n\n\n\n\n\n\n\n\nWe may notice output like matplotlib.lines.Line2D at when we are creating our visualization. matplotlib returns objects that reference the plot subcomponent that was just added. A lot of the time you can safely ignore this output, or you can put a semicolon at the end of the line to suppress the output.\nThe additional options instruct matplotlib to plot a black dashed line. The objects returned by fig.add_subplot here are AxesSubplot objects, on which you can directly plot on the other empty subplots by calling each one’s instance method.\n\nax1.hist(np.random.standard_normal(100),bins=20,color=\"black\", alpha=0.3);\n\nax2.scatter(np.arange(30), np.arange(30) + 3*np.random.standard_normal(30));\n\nTo make creating a grid of subplots more convenient, matplotlib includes a plt.subplots method that creates a new figure and returns a NumPy array containing the created subplot objects:\n\naxes = plt.subplots(2, 3)\n\naxes\n\n(&lt;Figure size 672x480 with 6 Axes&gt;,\n array([[&lt;AxesSubplot:&gt;, &lt;AxesSubplot:&gt;, &lt;AxesSubplot:&gt;],\n        [&lt;AxesSubplot:&gt;, &lt;AxesSubplot:&gt;, &lt;AxesSubplot:&gt;]], dtype=object))\n\n\n\n\n\n\n\n\n\nThe axes array can then be indexed like a two-dimensional array; for example, axes[0, 1] refers to the subplot in the top row at the center",
    "crumbs": [
      "9. Plotting and Visualization"
    ]
  },
  {
    "objectID": "09_main.html#scatter-plot",
    "href": "09_main.html#scatter-plot",
    "title": "9. Plotting and Visualization",
    "section": "Scatter Plot",
    "text": "Scatter Plot\n\nplt.style.use('_mpl-gallery')\n\n# make the data\nnp.random.seed(3)\nx = 4 + np.random.normal(0, 2, 24)\ny = 4 + np.random.normal(0, 2, len(x))\n# size and color:\nsizes = np.random.uniform(15, 80, len(x))\ncolors = np.random.uniform(15, 80, len(x))\n\n# plot\nfig, ax = plt.subplots()\n\nax.scatter(x, y, s=sizes, c=colors, vmin=0, vmax=100)\n\nax.set(xlim=(0, 8), xticks=np.arange(1, 8),\n       ylim=(0, 8), yticks=np.arange(1, 8))\n\nplt.show()",
    "crumbs": [
      "9. Plotting and Visualization"
    ]
  },
  {
    "objectID": "09_main.html#bar-plot",
    "href": "09_main.html#bar-plot",
    "title": "9. Plotting and Visualization",
    "section": "Bar Plot",
    "text": "Bar Plot\n\nplt.style.use('_mpl-gallery')\n\n# make data:\nnp.random.seed(3)\nx = 0.5 + np.arange(8)\ny = np.random.uniform(2, 7, len(x))\n\n# plot\nfig, ax = plt.subplots()\n\nax.bar(x, y, width=1, edgecolor=\"white\", linewidth=0.7)\n\nax.set(xlim=(0, 8), xticks=np.arange(1, 8),\n       ylim=(0, 8), yticks=np.arange(1, 8))\n\nplt.show()",
    "crumbs": [
      "9. Plotting and Visualization"
    ]
  },
  {
    "objectID": "09_main.html#box-plot",
    "href": "09_main.html#box-plot",
    "title": "9. Plotting and Visualization",
    "section": "Box Plot",
    "text": "Box Plot\n\nplt.style.use('_mpl-gallery')\n\n# make data:\nnp.random.seed(10)\nD = np.random.normal((3, 5, 4), (1.25, 1.00, 1.25), (100, 3))\n\n# plot\nfig, ax = plt.subplots()\nVP = ax.boxplot(D, positions=[2, 4, 6], widths=1.5, patch_artist=True,\n                showmeans=False, showfliers=False,\n                medianprops={\"color\": \"white\", \"linewidth\": 0.5},\n                boxprops={\"facecolor\": \"C0\", \"edgecolor\": \"white\",\n                          \"linewidth\": 0.5},\n                whiskerprops={\"color\": \"C0\", \"linewidth\": 1.5},\n                capprops={\"color\": \"C0\", \"linewidth\": 1.5})\n\nax.set(xlim=(0, 8), xticks=np.arange(1, 8),\n       ylim=(0, 8), yticks=np.arange(1, 8))\n\nplt.show()\n\n\n\n\n\n\n\n\nWe can learn more with the matplotlib documentation\n\nTable.1: Matplotlib.pyplot.subplots options\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nnrows\nNumber of rows of subplots\n\n\nncols\nNumber of columns of subplots\n\n\nsharex\nAll subplots should use the same x-axis ticks (adjusting the xlim will affect all subplots)\n\n\nsharey\nAll subplots should use the same y-axis ticks (adjusting the ylim will affect all subplots)\n\n\nsubplot_kw\nDictionary of keywords passed to add_subplot call used to create each subplot\n\n\nfig_kw\nAdditional keywords to subplots are used when creating the figure, such as plt.subplots (2,2, figsize=(8,6))",
    "crumbs": [
      "9. Plotting and Visualization"
    ]
  },
  {
    "objectID": "09_main.html#adjusting-the-spacing-around-subplots",
    "href": "09_main.html#adjusting-the-spacing-around-subplots",
    "title": "9. Plotting and Visualization",
    "section": "Adjusting the spacing around subplots",
    "text": "Adjusting the spacing around subplots\nBy default, matplotlib leaves a certain amount of padding around the outside of the subplots and in spacing between subplots. This spacing is all specified relative to the height and width of the plot, so that if you resize the plot either programmatically or manually using the GUI window, the plot will dynamically adjust itself. You can change the spacing using the subplots_adjust method on Figure objects:\nsubplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=None)\nwspace and hspace control the percent of the figure width and figure height, respectively, to use as spacing between subplots.\n\nfig, axes = plt.subplots(2, 2, sharex=True, sharey=True)\nfor i in range(2):\n    for j in range(2):\n        axes[i, j].hist(np.random.standard_normal(500), bins=50,\n                        color=\"black\", alpha=0.5)\nfig.subplots_adjust(wspace=0, hspace=0)",
    "crumbs": [
      "9. Plotting and Visualization"
    ]
  },
  {
    "objectID": "09_main.html#colors-markers-and-line-styles",
    "href": "09_main.html#colors-markers-and-line-styles",
    "title": "9. Plotting and Visualization",
    "section": "Colors, Markers, and Line Styles",
    "text": "Colors, Markers, and Line Styles\nmatplotlib’s line plot function accepts arrays of x and y coordinates and optional color styling options. For example, to plot x versus y with green dashes, you would execute:\n\nax.plot(x, y, linestyle=\"--\", color=\"green\")\n\n\nax = fig.add_subplot()\n\nax.plot(np.random.standard_normal(30).cumsum(), color=\"black\",\nlinestyle=\"dashed\", marker=\"o\")\n\nplt.show()\n\nline plots, you will notice that subsequent points are linearly interpolated by default. This can be altered with the drawstyle option.\n\nfig = plt.figure()\n\nax = fig.add_subplot()\n\ndata = np.random.standard_normal(30).cumsum()\n\nax.plot(data, color=\"black\", linestyle=\"dashed\", label=\"Default\");\nax.plot(data, color=\"black\", linestyle=\"dashed\",\ndrawstyle=\"steps-post\", label=\"steps-post\");\nax.legend()\n\n&lt;matplotlib.legend.Legend at 0x2c090478040&gt;",
    "crumbs": [
      "9. Plotting and Visualization"
    ]
  },
  {
    "objectID": "09_main.html#ticks-labels-and-legends",
    "href": "09_main.html#ticks-labels-and-legends",
    "title": "9. Plotting and Visualization",
    "section": "Ticks, Labels, and Legends",
    "text": "Ticks, Labels, and Legends\nMost kinds of plot decorations can be accessed through methods on matplotlib axes objects. This includes methods like xlim, xticks, and xticklabels. These control the plot range, tick locations, and tick labels, respectively. They can be used in two ways:\n\nCalled with no arguments returns the current parameter value (e.g., ax.xlim() returns the current x-axis plotting range)\nCalled with parameters sets the parameter value (e.g., ax.xlim([0, 10]) sets the x-axis range to 0 to 10)",
    "crumbs": [
      "9. Plotting and Visualization"
    ]
  },
  {
    "objectID": "09_main.html#setting-the-title-axis-labels-ticks-and-tick-labels",
    "href": "09_main.html#setting-the-title-axis-labels-ticks-and-tick-labels",
    "title": "9. Plotting and Visualization",
    "section": "Setting the title, axis labels, ticks, and tick labels",
    "text": "Setting the title, axis labels, ticks, and tick labels\n\nfig, ax = plt.subplots()\n\nax.plot(np.random.standard_normal(1000).cumsum());\n\nplt.show()\n\n\n\n\n\n\n\n\nTo change the x-axis ticks, it’s easiest to use set_xticks and set_xticklabels. The former instructs matplotlib where to place the ticks along the data range; by default these locations will also be the labels. But we can set any other values as the labels using set_xticklabels:\nThe rotation option sets the x tick labels at a 30-degree rotation. Lastly, set_xlabel gives a name to the x-axis, and set_title is the subplot title.",
    "crumbs": [
      "9. Plotting and Visualization"
    ]
  },
  {
    "objectID": "09_main.html#adding-legends",
    "href": "09_main.html#adding-legends",
    "title": "9. Plotting and Visualization",
    "section": "Adding legends",
    "text": "Adding legends\nLegends are another critical element for identifying plot elements. There are a couple of ways to add one. The easiest is to pass the label argument when adding each piece of the plot:\n\nfig, ax = plt.subplots()\n\nax.plot(np.random.randn(1000).cumsum(), color=\"black\", label=\"one\");\nax.plot(np.random.randn(1000).cumsum(), color=\"black\", linestyle=\"dashed\",\nlabel=\"two\");\nax.plot(np.random.randn(1000).cumsum(), color=\"black\", linestyle=\"dotted\",\nlabel=\"three\");\n\n\nax.legend()\n\n&lt;matplotlib.legend.Legend at 0x2c0904e2b90&gt;\n\n\n\n\n\n\n\n\n\nThe legend method has several other choices for the location loc argument. See the docstring (with ax.legend?) for more information. The loc legend option tells matplotlib where to place the plot. The default is “best”, which tries to choose a location that is most out of the way. To exclude one or more elements from the legend, pass no label or label=“nolegend”.",
    "crumbs": [
      "9. Plotting and Visualization"
    ]
  },
  {
    "objectID": "09_main.html#saving-plots-to-file",
    "href": "09_main.html#saving-plots-to-file",
    "title": "9. Plotting and Visualization",
    "section": "Saving Plots to File",
    "text": "Saving Plots to File\nYou can save the active figure to file using the figure object’s savefig instance method. For example, to save an SVG version of a figure, you need only type:\n\nfig.savefig(\"figpath.png\", dpi=400)\n\n\nTable 9.2: Some fig.savefig options\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nfname\nString containing a filepath or a Python file-like object. The figure format is inferred from the file extension (e.g., .pdf for PDF or .png for PNG).\n\n\ndpi\nThe figure resolution in dots per inch; defaults to 100 in IPython or 72 in Jupyter out of the box but can be configured.\n\n\nfacecolor, edgecolor\nThe color of the figure background outside of the subplots; \"w\" (white), by default.\n\n\nformat\nThe explicit file format to use (\"png\", \"pdf\", \"svg\", \"ps\", \"eps\", ...).\n\n\n\n\nmatplotlib Configuration\nmatplotlib comes configured with color schemes and defaults that are geared primarily toward preparing figures for publication. Fortunately, nearly all of the default behavior can be customized via global parameters governing figure size, subplot spacing, colors, font sizes, grid styles, and so on. One way to modify the configuration programmatically from Python is to use the rc method; for example, to set the global default figure size to be 10 × 10, you could enter:\nThe first argument to rc is the component you wish to customize, such as \"figure\", \"axes\", \"xtick\", \"ytick\", \"grid\", \"legend\", or many others. After that can follow a sequence of keyword arguments indicating the new parameters. A convenient way to write down the options in your program is as a dictionary:\n\nplt.rc(\"font\", family=\"monospace\", weight=\"bold\", size=8)\n\n\nplt.rc(\"figure\", figsize=(10, 10))\n\n\nplt.rc(\"font\", family=\"monospace\", weight=\"bold\", size=8)\n\nFor more extensive customization and to see a list of all the options, matplotlib comes with a configuration file matplotlibrc in the matplotlib/mpl-data directory. If you customize this file and place it in your home directory titled .matplotlibrc, it will be loaded each time you use matplotlib.\nAll of the current configuration settings are found in the plt.rcParams dictionary, and they can be restored to their default values by calling the plt.rcdefaults() function.\nThe first argument to rc is the component you wish to customize, such as “figure”, “axes”, “xtick”, “ytick”, “grid”, “legend”, or many others. After that can follow a sequence of keyword arguments indicating the new parameters. A convenient way to write down the options in your program is as a dictionary:",
    "crumbs": [
      "9. Plotting and Visualization"
    ]
  },
  {
    "objectID": "09_main.html#plotting-with-pandas-and-seaborn",
    "href": "09_main.html#plotting-with-pandas-and-seaborn",
    "title": "9. Plotting and Visualization",
    "section": "Plotting with pandas and seaborn",
    "text": "Plotting with pandas and seaborn\nmatplotlib can be a fairly low-level tool. You assemble a plot from its base components: the data display (i.e., the type of plot: line, bar, box, scatter, contour, etc.), legend, title, tick labels, and other annotations. In pandas, we may have multiple columns of data, along with row and column labels. pandas itself has built-in methods that simplify creating visualizations from DataFrame and Series objects. Another library is seaborn, a high-level statistical graphics library built on matplotlib. seaborn simplifies creating many common visualization types.",
    "crumbs": [
      "9. Plotting and Visualization"
    ]
  },
  {
    "objectID": "09_main.html#plotting-with-pandas",
    "href": "09_main.html#plotting-with-pandas",
    "title": "9. Plotting and Visualization",
    "section": "Plotting with Pandas",
    "text": "Plotting with Pandas",
    "crumbs": [
      "9. Plotting and Visualization"
    ]
  },
  {
    "objectID": "09_main.html#line-plots",
    "href": "09_main.html#line-plots",
    "title": "9. Plotting and Visualization",
    "section": "Line Plots",
    "text": "Line Plots\n\nimport pandas as pd\nimport numpy as np\n\ns = pd.Series(np.random.standard_normal(10).cumsum(), index=np.arange(0,\n 100, 10))\n\ns.plot()\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\n\n\n\nThe Series object’s index is passed to matplotlib for plotting on the x-axis, though you can disable this by passing use_index=False. The x-axis ticks and limits can be adjusted with the xticks and xlim options, and the y-axis respectively with yticks and ylim.\n\nTable 9.3: Series.plot method arguments\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nlabel\nLabel for plot legend\n\n\nax\nmatplotlib subplot object to plot on; if nothing passed, uses active matplotlib subplot\n\n\nstyle\nStyle string, like \"ko--\", to be passed to matplotlib\n\n\nalpha\nThe plot fill opacity (from 0 to 1)\n\n\nkind\nCan be \"area\", \"bar\", \"barh\", \"density\", \"hist\", \"kde\", \"line\", or \"pie\"; defaults to \"line\"\n\n\nfigsize\nSize of the figure object to create\n\n\nlogx\nPass True for logarithmic scaling on the x axis; pass \"sym\" for symmetric logarithm that permits negative values\n\n\nlogy\nPass True for logarithmic scaling on the y axis; pass \"sym\" for symmetric logarithm that permits negative values\n\n\ntitle\nTitle to use for the plot\n\n\nuse_index\nUse the object index for tick labels\n\n\nrot\nRotation of tick labels (0 through 360)\n\n\nxticks\nValues to use for x-axis ticks\n\n\nyticks\nValues to use for y-axis ticks\n\n\nxlim\nx-axis limits (e.g., [0, 10])\n\n\nylim\ny-axis limits\n\n\ngrid\nDisplay axis grid (off by default)",
    "crumbs": [
      "9. Plotting and Visualization"
    ]
  },
  {
    "objectID": "09_main.html#line-graph",
    "href": "09_main.html#line-graph",
    "title": "9. Plotting and Visualization",
    "section": "Line Graph",
    "text": "Line Graph\n\ndf = pd.DataFrame(np.random.standard_normal((10, 4)).cumsum(0),\ncolumns=[\"A\", \"B\", \"C\", \"D\"],\nindex=np.arange(0, 100, 10))\n\nplt.style.use('grayscale')\n\ndf.plot()\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\n\n\n\nHere I used plt.style.use('grayscale') to switch to a color scheme more suitable for black and white publication, since some readers will not be able to see the full color plots. The plot attribute contains a “family” of methods for different plot types. For example, df.plot() is equivalent to df.plot.line()",
    "crumbs": [
      "9. Plotting and Visualization"
    ]
  },
  {
    "objectID": "09_main.html#bar-plots",
    "href": "09_main.html#bar-plots",
    "title": "9. Plotting and Visualization",
    "section": "Bar Plots",
    "text": "Bar Plots\nThe plot.bar() and plot.barh() make vertical and horizontal bar plots, respectively. In this case, the Series or DataFrame index will be used as the x (bar) or y (barh) ticks\n\nfig, axes = plt.subplots(2, 1)\n\ndata = pd.Series(np.random.uniform(size=16), index=list(\"abcdefghijklmnop\"))\n\ndata.plot.bar(ax=axes[0], color=\"black\", alpha=0.7)\n\ndata.plot.barh(ax=axes[1], color=\"black\", alpha=0.7)\n\n&lt;AxesSubplot:&gt;",
    "crumbs": [
      "9. Plotting and Visualization"
    ]
  },
  {
    "objectID": "09_main.html#with-a-dataframe-bar-plots-group-the-values-in-each-row-in-bars-side-by-side-for-each-value.",
    "href": "09_main.html#with-a-dataframe-bar-plots-group-the-values-in-each-row-in-bars-side-by-side-for-each-value.",
    "title": "9. Plotting and Visualization",
    "section": "With a DataFrame, bar plots group the values in each row in bars, side by side, for each value.",
    "text": "With a DataFrame, bar plots group the values in each row in bars, side by side, for each value.\n\ndf = pd.DataFrame(np.random.uniform(size=(6, 4)),\nindex=[\"one\", \"two\", \"three\", \"four\", \"five\", \"six\"],\ncolumns=pd.Index([\"A\", \"B\", \"C\", \"D\"], name=\"Genus\"))\n\ndf\n \n\ndf.plot.bar()\n\n&lt;AxesSubplot:&gt;",
    "crumbs": [
      "9. Plotting and Visualization"
    ]
  },
  {
    "objectID": "09_main.html#we-create-stacked-bar-plots-from-a-dataframe-by-passing-stackedtrue-resulting-in-the-value-in-each-row-being-stacked-together-horizontally",
    "href": "09_main.html#we-create-stacked-bar-plots-from-a-dataframe-by-passing-stackedtrue-resulting-in-the-value-in-each-row-being-stacked-together-horizontally",
    "title": "9. Plotting and Visualization",
    "section": "We create stacked bar plots from a DataFrame by passing stacked=True, resulting in the value in each row being stacked together horizontally",
    "text": "We create stacked bar plots from a DataFrame by passing stacked=True, resulting in the value in each row being stacked together horizontally\n\ndf.plot.barh(stacked=True, alpha=0.5)\n\n&lt;AxesSubplot:&gt;",
    "crumbs": [
      "9. Plotting and Visualization"
    ]
  },
  {
    "objectID": "09_main.html#visualizing-categorical-data-with-seaborn",
    "href": "09_main.html#visualizing-categorical-data-with-seaborn",
    "title": "9. Plotting and Visualization",
    "section": "Visualizing categorical data with Seaborn",
    "text": "Visualizing categorical data with Seaborn\nPlotting functions in seaborn take a data argument, which can be a pandas DataFrame. The other arguments refer to column names.\n\nimport seaborn as sns\ntips = sns.load_dataset(\"tips\")\nsns.catplot(data=tips, x=\"day\", y=\"total_bill\")\n\n&lt;seaborn.axisgrid.FacetGrid at 0x2c08fd8c2b0&gt;\n\n\n\n\n\n\n\n\n\n\nsns.catplot(data=tips, x=\"day\", y=\"total_bill\", hue=\"sex\", kind=\"swarm\")\n\n&lt;seaborn.axisgrid.FacetGrid at 0x2c092177ac0&gt;",
    "crumbs": [
      "9. Plotting and Visualization"
    ]
  },
  {
    "objectID": "09_main.html#boxplot",
    "href": "09_main.html#boxplot",
    "title": "9. Plotting and Visualization",
    "section": "Boxplot",
    "text": "Boxplot\n\nsns.catplot(data=tips, x=\"day\", y=\"total_bill\", kind=\"box\")\n\n&lt;seaborn.axisgrid.FacetGrid at 0x2c0906cbfd0&gt;",
    "crumbs": [
      "9. Plotting and Visualization"
    ]
  },
  {
    "objectID": "09_main.html#barplot",
    "href": "09_main.html#barplot",
    "title": "9. Plotting and Visualization",
    "section": "Barplot",
    "text": "Barplot\n\ntitanic = sns.load_dataset(\"titanic\")\nsns.catplot(data=titanic, x=\"sex\", y=\"survived\", hue=\"class\", kind=\"bar\")\n\n&lt;seaborn.axisgrid.FacetGrid at 0x2c0904e2bf0&gt;",
    "crumbs": [
      "9. Plotting and Visualization"
    ]
  },
  {
    "objectID": "09_main.html#scatter-plot-1",
    "href": "09_main.html#scatter-plot-1",
    "title": "9. Plotting and Visualization",
    "section": "Scatter Plot",
    "text": "Scatter Plot\n\nimport pandas as pd\nimport seaborn as sns\npenguin=pd.read_excel('data/penguins.xlsx')\n\nax = sns.regplot(x=\"bill_length_mm\", y=\"flipper_length_mm\", data=penguin)\n\nax\n\n&lt;AxesSubplot:xlabel='bill_length_mm', ylabel='flipper_length_mm'&gt;\n\n\n\n\n\n\n\n\n\nIn exploratory data analysis, it’s helpful to be able to look at all the scatter plots among a group of variables; this is known as a pairs plot or scatter plot matrix.\n\n sns.pairplot(penguin, diag_kind=\"kde\", plot_kws={\"alpha\": 0.2})\n\n&lt;seaborn.axisgrid.PairGrid at 0x2c0902b90c0&gt;\n\n\n\n\n\n\n\n\n\nThis plot_kws enables us to pass down configuration options to the individual plotting calls on the off-diagonal elements.\nPoint Plot\n\nsns.catplot(data=titanic, x=\"sex\", y=\"survived\", hue=\"class\", kind=\"point\")\n\n&lt;seaborn.axisgrid.FacetGrid at 0x2c08fdda5c0&gt;",
    "crumbs": [
      "9. Plotting and Visualization"
    ]
  },
  {
    "objectID": "09_main.html#line-plot",
    "href": "09_main.html#line-plot",
    "title": "9. Plotting and Visualization",
    "section": "Line Plot",
    "text": "Line Plot\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nsns.set_theme(style=\"whitegrid\")\n\nrs = np.random.RandomState(365)\nvalues = rs.randn(365, 4).cumsum(axis=0)\ndates = pd.date_range(\"1 1 2016\", periods=365, freq=\"D\")\ndata = pd.DataFrame(values, dates, columns=[\"A\", \"B\", \"C\", \"D\"])\ndata = data.rolling(7).mean()\n\nsns.lineplot(data=data, palette=\"tab10\", linewidth=2.5)\n\n&lt;AxesSubplot:&gt;",
    "crumbs": [
      "9. Plotting and Visualization"
    ]
  },
  {
    "objectID": "09_main.html#facet-grids-and-categorical-data",
    "href": "09_main.html#facet-grids-and-categorical-data",
    "title": "9. Plotting and Visualization",
    "section": "Facet Grids and Categorical Data",
    "text": "Facet Grids and Categorical Data\nOne way to visualize data with many categorical variables is to use a facet grid, which is a two-dimensional layout of plots where the data is split across the plots on each axis based on the distinct values of a certain variable. seaborn has a useful built-in function catplot that simplifies making many kinds of faceted plots split by categorical variables\n\nsns.catplot(x=\"species\", y=\"bill_length_mm\", hue=\"sex\", col=\"island\",\nkind=\"bar\", data=penguin)\n\n&lt;seaborn.axisgrid.FacetGrid at 0x2c09fc18280&gt;\n\n\n\n\n\n\n\n\n\ncatplot supports other plot types that may be useful depending on what you are trying to display. For example, box plots (which show the median, quartiles, and outliers) can be an effective visualization type.\n\nsns.catplot(x=\"bill_length_mm\", y=\"island\", kind=\"box\",\ndata=penguin)\n\n&lt;seaborn.axisgrid.FacetGrid at 0x2c0984ba440&gt;",
    "crumbs": [
      "9. Plotting and Visualization"
    ]
  },
  {
    "objectID": "09_main.html#other-python-visualization-tools",
    "href": "09_main.html#other-python-visualization-tools",
    "title": "9. Plotting and Visualization",
    "section": "Other Python Visualization Tools",
    "text": "Other Python Visualization Tools\n\nThere are many other tools for data visualization in python such as [Altair](https://altair-viz.github.io/); [Bokeh](http://bokeh.pydata.org/) and [Plotly](https://plotly.com/python)\nFor creating static graphics for print or web, I recommend using matplotlib and libraries that build on matplotlib, like pandas and seaborn, for your needs.",
    "crumbs": [
      "9. Plotting and Visualization"
    ]
  },
  {
    "objectID": "07_main.html",
    "href": "07_main.html",
    "title": "7. Data Cleaning and Preparation",
    "section": "",
    "text": "Learning Objectives\nimport pandas as pd\nimport numpy as np\n\nfood = pd.read_csv(\"https://openmv.net/file/food-consumption.csv\")\n\nprint(food.head(5))\n\n   Country  Real coffee  Instant coffee  Tea  Sweetener  Biscuits  \\\n0  Germany           90              49   88       19.0      57.0   \n1    Italy           82              10   60        2.0      55.0   \n2   France           88              42   63        4.0      76.0   \n3  Holland           96              62   98       32.0      62.0   \n4  Belgium           94              38   48       11.0      74.0   \n\n   Powder soup  Tin soup  Potatoes  Frozen fish  ...  Apples  Oranges  \\\n0           51        19        21           27  ...      81       75   \n1           41         3         2            4  ...      67       71   \n2           53        11        23           11  ...      87       84   \n3           67        43         7           14  ...      83       89   \n4           37        23         9           13  ...      76       76   \n\n   Tinned fruit  Jam  Garlic  Butter  Margarine  Olive oil  Yoghurt  \\\n0            44   71      22      91         85         74     30.0   \n1             9   46      80      66         24         94      5.0   \n2            40   45      88      94         47         36     57.0   \n3            61   81      15      31         97         13     53.0   \n4            42   57      29      84         80         83     20.0   \n\n   Crisp bread  \n0           26  \n1           18  \n2            3  \n3           15  \n4            5  \n\n[5 rows x 21 columns]\ndataset: The relative consumption of certain food items in European and Scandinavian countries. The numbers represent the percentage of the population consuming that food type",
    "crumbs": [
      "7. Data Cleaning and Preparation"
    ]
  },
  {
    "objectID": "07_main.html#learning-objectives",
    "href": "07_main.html#learning-objectives",
    "title": "7. Data Cleaning and Preparation",
    "section": "",
    "text": "Know which tools to use for missing data\nKnow how to filter out missing data\nUnderstand methods to fill in missing values\nKnow when and how to transform data\nKnow how to use certain numpy functions to handle outliers, permute, and take random samples\nKnow how to manipulate strings\nUnderstand some useful methods for regular expressions\nLearn about some helpful methods in pandas to explore strings\nUnderstand how to handle categorical data more optimally",
    "crumbs": [
      "7. Data Cleaning and Preparation"
    ]
  },
  {
    "objectID": "07_main.html#handling-missing-data",
    "href": "07_main.html#handling-missing-data",
    "title": "7. Data Cleaning and Preparation",
    "section": "7.1 Handling Missing Data",
    "text": "7.1 Handling Missing Data\nSome things to note:\n\nALL DESCRIPTIVE STATISTICS ON pandas OBJECTS EXLUDE MISSING DATA - BY DEFAULT\nNaN is used for missing values of type: float64\nValues like NaN are called sentinel values\n\na value that is not part of the input but indicates a special meaning; a signal value\nNaN for missing integers, -1 as a value to be inserted in a function that computes only non-negative integers, etc.\n\n\n\nprint(food.Yoghurt.isna())\n\n0     False\n1     False\n2     False\n3     False\n4     False\n5     False\n6     False\n7     False\n8     False\n9     False\n10    False\n11    False\n12    False\n13     True\n14    False\n15    False\nName: Yoghurt, dtype: bool\n\n\nWe do have an NaN in our midst!\n\n# descriptive stats\nprint(np.mean(food['Yoghurt']), \"\\n versus\", np.average(food['Yoghurt']))\n\n20.533333333333335 \n versus nan\n\n\nDifferent results! Why?? According to numpy documentation:\nnp.mean always calculates the arithmetic mean along a specified axis. The first argument requires the type to be of int64 so will take the mean of those that fit. The average is taken over the flattened array by default. np.average computes the weighted average along the specified axis.\nsum(food.Yoghurt) –&gt; nan\nfrom average source:\n        avg = avg_as_array = np.multiply(a, wgt,\n                          dtype=result_dtype).sum(axis, **keepdims_kw) / scl\nfrom mean source:\nif type(a) is not mu.ndarray:\n        try:\n            mean = a.mean\n        except AttributeError:\n            pass\n        else:\n            return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n\n    return _methods._mean(a, axis=axis, dtype=dtype,\n                          out=out, **kwargs)\nFYI: the statistics module includes mean()\nSomething weird to consider….\n\nprint(np.nan == np.nan)\n\n# apparently, according to the floating-point standard, NaN is not equal to itself!\n\nFalse\n\n\nI digress…\n\nFiltering Missing Data\n\n# method dropna\nprint(\"`dropna`: option to include `how = all` to only remove rows where every value is NaN \\n\",food.Yoghurt.dropna().tail(), \"\\n\",\n\"`fillna`: pass fillna a dictionary (fillna({1: 0.5, 2: 0})) to specify a different value for each column\\n\", food.Yoghurt.fillna(0).tail(), \"\\n\",\n\"`isna`\\n\", food.Yoghurt.isna().tail(), \"\\n\",\n\"`notna`\\n\", food.Yoghurt.notna().tail())\n\n`dropna`: option to include `how = all` to only remove rows where every value is NaN \n 10     2.0\n11    11.0\n12     2.0\n14    16.0\n15     3.0\nName: Yoghurt, dtype: float64 \n `fillna`: pass fillna a dictionary (fillna({1: 0.5, 2: 0})) to specify a different value for each column\n 11    11.0\n12     2.0\n13     0.0\n14    16.0\n15     3.0\nName: Yoghurt, dtype: float64 \n `isna`\n 11    False\n12    False\n13     True\n14    False\n15    False\nName: Yoghurt, dtype: bool \n `notna`\n 11     True\n12     True\n13    False\n14     True\n15     True\nName: Yoghurt, dtype: bool",
    "crumbs": [
      "7. Data Cleaning and Preparation"
    ]
  },
  {
    "objectID": "07_main.html#data-transformation",
    "href": "07_main.html#data-transformation",
    "title": "7. Data Cleaning and Preparation",
    "section": "7.2 Data Transformation",
    "text": "7.2 Data Transformation\n\nRemoving Duplicates\nCheck to see is duplicates exists:\n\nfood.duplicated()\n\n0     False\n1     False\n2     False\n3     False\n4     False\n5     False\n6     False\n7     False\n8     False\n9     False\n10    False\n11    False\n12    False\n13    False\n14    False\n15    False\ndtype: bool\n\n\nIf you were to have duplicates, you can use the function drop_duplicates().\n*NOTE: by default, drop_duplicates will only return the first observed value*\n\ndup_food = food[['Yoghurt','Yoghurt']]\ndup_food.columns = ['a','b']\n\n\n# index 11,12 are dropped - dont understand this at all\ndup_food.drop_duplicates()\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n30.0\n30.0\n\n\n1\n5.0\n5.0\n\n\n2\n57.0\n57.0\n\n\n3\n53.0\n53.0\n\n\n4\n20.0\n20.0\n\n\n5\n31.0\n31.0\n\n\n6\n11.0\n11.0\n\n\n7\n6.0\n6.0\n\n\n8\n13.0\n13.0\n\n\n9\n48.0\n48.0\n\n\n10\n2.0\n2.0\n\n\n13\nNaN\nNaN\n\n\n14\n16.0\n16.0\n\n\n15\n3.0\n3.0\n\n\n\n\n\n\n\n\n# index 6, 10 are dropped- also dont understand this at all\ndup_food.drop_duplicates(keep = 'last')\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n30.0\n30.0\n\n\n1\n5.0\n5.0\n\n\n2\n57.0\n57.0\n\n\n3\n53.0\n53.0\n\n\n4\n20.0\n20.0\n\n\n5\n31.0\n31.0\n\n\n7\n6.0\n6.0\n\n\n8\n13.0\n13.0\n\n\n9\n48.0\n48.0\n\n\n11\n11.0\n11.0\n\n\n12\n2.0\n2.0\n\n\n13\nNaN\nNaN\n\n\n14\n16.0\n16.0\n\n\n15\n3.0\n3.0\n\n\n\n\n\n\n\n\n# again 11,12 are dropped - still dont understand - help\ndup_food.drop_duplicates(subset=['a'])\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n30.0\n30.0\n\n\n1\n5.0\n5.0\n\n\n2\n57.0\n57.0\n\n\n3\n53.0\n53.0\n\n\n4\n20.0\n20.0\n\n\n5\n31.0\n31.0\n\n\n6\n11.0\n11.0\n\n\n7\n6.0\n6.0\n\n\n8\n13.0\n13.0\n\n\n9\n48.0\n48.0\n\n\n10\n2.0\n2.0\n\n\n13\nNaN\nNaN\n\n\n14\n16.0\n16.0\n\n\n15\n3.0\n3.0\n\n\n\n\n\n\n\n\n\nTransforming Data with a Function or Mapping\nSince mapping a function over a series has already been covered, this section will only go over a few more helpful ways to map.\n\ndefine your own function - similar to how we would do in apply functions or purrr:map()\n\nfood_sub = food[:5][['Country','Yoghurt']]\ncountry_yogurt = {\n  'Germany':'Quark',\n  'Italy':'Yomo',\n  'France':'Danone',\n  'Holland':'Campina',\n  'Belgium':'Activia'\n}\n\n\n\ndef get_yogurt(x):\n   return country_yogurt[x]\n\nfood_sub['Brand'] = food_sub['Country'].map(get_yogurt)\n\nfood_sub['Country'].map(get_yogurt)\n\n0      Quark\n1       Yomo\n2     Danone\n3    Campina\n4    Activia\nName: Country, dtype: object\n\n\n\n\nReplace Values\n\nprint(\"using `replace`: \\n\", food_sub.replace([30],50), '\\n',\n\"using `replace` for more than one value: \\n\", food_sub.replace([30, 20],[50, 40]))\n\nusing `replace`: \n    Country  Yoghurt    Brand\n0  Germany     50.0    Quark\n1    Italy      5.0     Yomo\n2   France     57.0   Danone\n3  Holland     53.0  Campina\n4  Belgium     20.0  Activia \n using `replace` for more than one value: \n    Country  Yoghurt    Brand\n0  Germany     50.0    Quark\n1    Italy      5.0     Yomo\n2   France     57.0   Danone\n3  Holland     53.0  Campina\n4  Belgium     40.0  Activia\n\n\n\n\nRenaming Axis Indices\nAs we’ve seen, standard indices are labelled as such:\n&gt;&gt;&gt; food_sub.index\nRangeIndex(start=0, stop=5, step=1)\nThat can also be changed with the mapping of a function:\n\nprint(food_sub.index.map(lambda x: x + 10))\nprint('or')\nprint(food_sub.index.map({0:'G', 1:'I', 2:'F', 3:'H', 4:'B'}))\n\nInt64Index([10, 11, 12, 13, 14], dtype='int64')\nor\nIndex(['G', 'I', 'F', 'H', 'B'], dtype='object')\n\n\n\n\nDiscretization and Binning\nIt is common to convert continuous variables into discrete and group them. Let’s group the affinity for yogurt into random bins:\n\nscale = [0, 20, 30, 50, 70]\n# reasonable, ok, interesting, why\n\npd.cut(food.Yoghurt, scale)\n\n0     (20.0, 30.0]\n1      (0.0, 20.0]\n2     (50.0, 70.0]\n3     (50.0, 70.0]\n4      (0.0, 20.0]\n5     (30.0, 50.0]\n6      (0.0, 20.0]\n7      (0.0, 20.0]\n8      (0.0, 20.0]\n9     (30.0, 50.0]\n10     (0.0, 20.0]\n11     (0.0, 20.0]\n12     (0.0, 20.0]\n13             NaN\n14     (0.0, 20.0]\n15     (0.0, 20.0]\nName: Yoghurt, dtype: category\nCategories (4, interval[int64, right]): [(0, 20] &lt; (20, 30] &lt; (30, 50] &lt; (50, 70]]\n\n\n\nscaled = pd.cut(food.Yoghurt.values, scale)\nscaled.categories\n\npd.value_counts(scaled)\n\n(0, 20]     10\n(30, 50]     2\n(50, 70]     2\n(20, 30]     1\ndtype: int64\n\n\nApply the labels to the bins to have it make more sense:\n\nscale_names = ['reasonable', 'ok', 'interesting', 'why']\npd.value_counts(pd.cut(food.Yoghurt.values, scale, labels = scale_names))\n\nreasonable     10\ninteresting     2\nwhy             2\nok              1\ndtype: int64\n\n\nFinally, let pandas do the work for you by supplying a number of bins and a precision point. It will bin your data equally while limiting the decimal point based on the value of precision\n\npd.qcut(food.Yoghurt.values, 4, precision = 2)\n\n[(13.0, 30.5], (1.99, 5.5], (30.5, 57.0], (30.5, 57.0], (13.0, 30.5], ..., (5.5, 13.0], (1.99, 5.5], NaN, (13.0, 30.5], (1.99, 5.5]]\nLength: 16\nCategories (4, interval[float64, right]): [(1.99, 5.5] &lt; (5.5, 13.0] &lt; (13.0, 30.5] &lt; (30.5, 57.0]]\n\n\n\n\nDetecting and Filtering Outliers\nWe often have to face the decision of how to handle outliers. We can choose to exclude them or to transform them.\n\n# let's say any country who's percentage of yogurt consumption is over 50% is an outlier\n\nyog = food.Yoghurt\nyog[yog.abs() &gt; 50]\n\n2    57.0\n3    53.0\nName: Yoghurt, dtype: float64\n\n\nMore interestingly, what if we wanted to know if the consumption of ANY food was over 50% ?\n\nfood2 = food.drop('Country', axis = 'columns')\nfood2[(food2.abs() &gt; 95).any(axis = 'columns')]\n\n\n\n\n\n\n\n\nReal coffee\nInstant coffee\nTea\nSweetener\nBiscuits\nPowder soup\nTin soup\nPotatoes\nFrozen fish\nFrozen veggies\nApples\nOranges\nTinned fruit\nJam\nGarlic\nButter\nMargarine\nOlive oil\nYoghurt\nCrisp bread\n\n\n\n\n3\n96\n62\n98\n32.0\n62.0\n67\n43\n7\n14\n14\n83\n89\n61\n81\n15\n31\n97\n13\n53.0\n15\n\n\n5\n97\n61\n86\n28.0\n79.0\n73\n12\n7\n26\n23\n85\n94\n83\n20\n91\n94\n94\n84\n31.0\n24\n\n\n6\n27\n86\n99\n22.0\n91.0\n55\n76\n17\n20\n24\n76\n68\n89\n91\n11\n95\n94\n57\n11.0\n28\n\n\n10\n97\n13\n93\n31.0\nNaN\n43\n43\n39\n54\n45\n56\n78\n53\n75\n9\n68\n32\n48\n2.0\n93\n\n\n11\n96\n17\n92\n35.0\n66.0\n32\n17\n11\n51\n42\n81\n72\n50\n64\n11\n92\n91\n30\n11.0\n34\n\n\n13\n98\n12\n84\n20.0\n64.0\n27\n10\n8\n18\n12\n50\n57\n22\n37\n15\n96\n94\n17\nNaN\n64\n\n\n15\n30\n52\n99\n11.0\n80.0\n75\n18\n2\n5\n3\n57\n52\n46\n89\n5\n97\n25\n31\n3.0\n9\n\n\n\n\n\n\n\n\n\nPermutation and Random Sampling\n\nPermuting = random reordering\n\nnp.random.permutation = takes the length of the axis you want to permute\n\nRandom sampling = each sample has an equal probability of being chosen\n\nLet’s randomly reorder yogurt affinity:\n\nprint(np.random.permutation(5))\n\nfood.take(np.random.permutation(5))\n\n[2 3 0 1 4]\n\n\n\n\n\n\n\n\n\nCountry\nReal coffee\nInstant coffee\nTea\nSweetener\nBiscuits\nPowder soup\nTin soup\nPotatoes\nFrozen fish\n...\nApples\nOranges\nTinned fruit\nJam\nGarlic\nButter\nMargarine\nOlive oil\nYoghurt\nCrisp bread\n\n\n\n\n2\nFrance\n88\n42\n63\n4.0\n76.0\n53\n11\n23\n11\n...\n87\n84\n40\n45\n88\n94\n47\n36\n57.0\n3\n\n\n1\nItaly\n82\n10\n60\n2.0\n55.0\n41\n3\n2\n4\n...\n67\n71\n9\n46\n80\n66\n24\n94\n5.0\n18\n\n\n3\nHolland\n96\n62\n98\n32.0\n62.0\n67\n43\n7\n14\n...\n83\n89\n61\n81\n15\n31\n97\n13\n53.0\n15\n\n\n0\nGermany\n90\n49\n88\n19.0\n57.0\n51\n19\n21\n27\n...\n81\n75\n44\n71\n22\n91\n85\n74\n30.0\n26\n\n\n4\nBelgium\n94\n38\n48\n11.0\n74.0\n37\n23\n9\n13\n...\n76\n76\n42\n57\n29\n84\n80\n83\n20.0\n5\n\n\n\n\n5 rows × 21 columns\n\n\n\nThis method can be helpful when using iloc indexing!\n\nfood.take(np.random.permutation(5), axis = 'columns')\n\n\n\n\n\n\n\n\nCountry\nSweetener\nInstant coffee\nReal coffee\nTea\n\n\n\n\n0\nGermany\n19.0\n49\n90\n88\n\n\n1\nItaly\n2.0\n10\n82\n60\n\n\n2\nFrance\n4.0\n42\n88\n63\n\n\n3\nHolland\n32.0\n62\n96\n98\n\n\n4\nBelgium\n11.0\n38\n94\n48\n\n\n5\nLuxembourg\n28.0\n61\n97\n86\n\n\n6\nEngland\n22.0\n86\n27\n99\n\n\n7\nPortugal\n2.0\n26\n72\n77\n\n\n8\nAustria\n15.0\n31\n55\n61\n\n\n9\nSwitzerland\n25.0\n72\n73\n85\n\n\n10\nSweden\n31.0\n13\n97\n93\n\n\n11\nDenmark\n35.0\n17\n96\n92\n\n\n12\nNorway\n13.0\n17\n92\n83\n\n\n13\nFinland\n20.0\n12\n98\n84\n\n\n14\nSpain\nNaN\n40\n70\n40\n\n\n15\nIreland\n11.0\n52\n30\n99\n\n\n\n\n\n\n\nLet’s try taking a random subset without replacement:\n\n\nfood.sample(n =5)\n# you can always add `replace=True` if you want replacement\n\n\n\n\n\n\n\n\nCountry\nReal coffee\nInstant coffee\nTea\nSweetener\nBiscuits\nPowder soup\nTin soup\nPotatoes\nFrozen fish\n...\nApples\nOranges\nTinned fruit\nJam\nGarlic\nButter\nMargarine\nOlive oil\nYoghurt\nCrisp bread\n\n\n\n\n14\nSpain\n70\n40\n40\nNaN\n62.0\n43\n2\n14\n23\n...\n59\n77\n30\n38\n86\n44\n51\n91\n16.0\n13\n\n\n1\nItaly\n82\n10\n60\n2.0\n55.0\n41\n3\n2\n4\n...\n67\n71\n9\n46\n80\n66\n24\n94\n5.0\n18\n\n\n9\nSwitzerland\n73\n72\n85\n25.0\n31.0\n69\n10\n17\n19\n...\n79\n70\n46\n61\n64\n82\n48\n61\n48.0\n30\n\n\n4\nBelgium\n94\n38\n48\n11.0\n74.0\n37\n23\n9\n13\n...\n76\n76\n42\n57\n29\n84\n80\n83\n20.0\n5\n\n\n13\nFinland\n98\n12\n84\n20.0\n64.0\n27\n10\n8\n18\n...\n50\n57\n22\n37\n15\n96\n94\n17\nNaN\n64\n\n\n\n\n5 rows × 21 columns\n\n\n\n\n\nComputing Indicator/Dummy Vars\nThis kind of transformation is really helpful for machine learning. It converts categorical variables into indicator or dummy variable through a transformation that results in 0’s and 1’s.\n\npd.get_dummies(food['Country'])\n\n\n\n\n\n\n\n\nAustria\nBelgium\nDenmark\nEngland\nFinland\nFrance\nGermany\nHolland\nIreland\nItaly\nLuxembourg\nNorway\nPortugal\nSpain\nSweden\nSwitzerland\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n\n\n2\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n5\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\n6\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n7\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\n8\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n9\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n10\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n11\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n12\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n13\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n14\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n15\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n\n\nThis example is not the most helpful since this set of countries are unique but I hope you get the idea..\nThis is topic will make more sense in Ch.13 when data analysis examples are worked out.",
    "crumbs": [
      "7. Data Cleaning and Preparation"
    ]
  },
  {
    "objectID": "07_main.html#extension-data-types",
    "href": "07_main.html#extension-data-types",
    "title": "7. Data Cleaning and Preparation",
    "section": "7.3 Extension Data Types",
    "text": "7.3 Extension Data Types\nExtension types addresses some of the shortcomings brought on by numpy such as:\n\nexpensive string computations\nmissing data conversions\nlack of support for time related objects\n\n\ns = pd.Series([1, 2, 3, None])\ns.dtype\n\ndtype('float64')\n\n\n\ns = pd.Series([1, 2, 3, None], dtype=pd.Int64Dtype())\ns\nprint(s.dtype)\n\nInt64\n\n\nNote that this extension type indicates missing with &lt;NA&gt;\n\nprint(s.isna())\n\n0    False\n1    False\n2    False\n3     True\ndtype: bool\n\n\n&lt;NA&gt; uses the pandas.NA sentinal value\n\ns[3] is pd.NA\n\nTrue\n\n\nTypes can be set with astype()\n\ndf = pd.DataFrame({\"A\": [1, 2, None, 4],\n\"B\": [\"one\", \"two\", \"three\", None],\n\"C\": [False, None, False, True]})\n\ndf[\"A\"] = df[\"A\"].astype(\"Int64\")\ndf[\"B\"] = df[\"B\"].astype(\"string\")\ndf[\"C\"] = df[\"C\"].astype(\"boolean\")\n\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n1\none\nFalse\n\n\n1\n2\ntwo\n&lt;NA&gt;\n\n\n2\n&lt;NA&gt;\nthree\nFalse\n\n\n3\n4\n&lt;NA&gt;\nTrue\n\n\n\n\n\n\n\nFind a table of extension types here",
    "crumbs": [
      "7. Data Cleaning and Preparation"
    ]
  },
  {
    "objectID": "07_main.html#string-manipulation",
    "href": "07_main.html#string-manipulation",
    "title": "7. Data Cleaning and Preparation",
    "section": "7.4 String Manipulation",
    "text": "7.4 String Manipulation\nFunctions that are built in:\n\nsplit() : break a string into pieces\njoin()\nstrip() : trim whitespace\nin(): good for locating a substring\ncount() : returns the number of occurrences of a substring\nreplace() : substitute occurrences of one pattern for another\n\nSee more function here\n\nlb = \" layla is smart, witty, charming, and... \"\nlb.split(\" \")\nlb.strip()\n'-'.join(lb)\n'smart' in lb\nlb.count(',')\nlb.replace('...', ' bad at python.')\n\n' layla is smart, witty, charming, and bad at python. '\n\n\n\nRegular Expressions\nRegEx is not easy. It takes some getting used to. It is really useful for programatically applying any of the string functions to particular pattern.\nI often refer to this handy [cheat sheet](https://raw.githubusercontent.com/rstudio/cheatsheets/main/strings.pdf)\nTo use regular expression in python, you must import the module re:\n\nimport re\n\ntext = \"layla has lived in philadelphia county, miami-dade county, and rockdale county\"\n\n# split on whitespace\nre.split(r\"\\s+\", text)\n\n['layla',\n 'has',\n 'lived',\n 'in',\n 'philadelphia',\n 'county,',\n 'miami-dade',\n 'county,',\n 'and',\n 'rockdale',\n 'county']\n\n\nTo avoid repeating a common expression, you can compile it and store it as it’s own object.\nregex = re.compile(r\"\\s+\")\nDon’t forget: there are certain characters you must escape before using like: ‘\\,., +, :’ and more\nWhat if I wanted to get the counties?\n\nregex = re.compile(r\"\\w+(?=\\s+county)\")\n\nregex.findall(text)\n\n['philadelphia', 'dade', 'rockdale']\n\n\n\n\nString Functions\n\ndata = {\"Dave\": \"dave@google.com\", \"Steve\": \"steve@gmail.com\",\n\"Rob\": \"rob@gmail.com\", \"Wes\": np.nan}\n# convert to series\ndata = pd.Series(data)\ndata\n\nDave     dave@google.com\nSteve    steve@gmail.com\nRob        rob@gmail.com\nWes                  NaN\ndtype: object\n\n\nTo get certain information, we can apply string functions from Series array-oriented methods:\n\n# does the string contain something\ndata.str.contains(\"gmail\")\n# change the extension tryp\ndata_as_string_ext = data.astype('string')\ndata_as_string_ext\n\nDave     dave@google.com\nSteve    steve@gmail.com\nRob        rob@gmail.com\nWes                 &lt;NA&gt;\ndtype: string\n\n\n\n# vectorized element retrieval\npattern = r\"([A-Z0-9._%+-]+)@([A-Z0-9.-]+)\\.([A-Z]{2,4})\"\ndata.str.findall(pattern, flags=re.IGNORECASE).str[0]\n\nDave     (dave, google, com)\nSteve    (steve, gmail, com)\nRob        (rob, gmail, com)\nWes                      NaN\ndtype: object",
    "crumbs": [
      "7. Data Cleaning and Preparation"
    ]
  },
  {
    "objectID": "07_main.html#categorical-data",
    "href": "07_main.html#categorical-data",
    "title": "7. Data Cleaning and Preparation",
    "section": "7.5 Categorical Data",
    "text": "7.5 Categorical Data\n\nvalues = pd.Series(['apple', 'orange', 'apple',\n                   'apple'] * 2)\n                   \npd.unique(values)\npd.value_counts(values)\n\napple     6\norange    2\ndtype: int64\n\n\nYou can improve performance by creating categorical representations that are numerical:\n\nvalues = pd.Series([0, 1, 0, 0] * 2)\ndim = pd.Series(['apple', 'orange'])\n\ndim\n\n0     apple\n1    orange\ndtype: object\n\n\nRetrieve the original set of strings with take\n\ndim.take(values)\n\n0     apple\n1    orange\n0     apple\n0     apple\n0     apple\n1    orange\n0     apple\n0     apple\ndtype: object\n\n\n\nComputations with Categoricals\n\nrng = np.random.default_rng(seed=12345)\ndraws = rng.standard_normal(1000)\nbins = pd.qcut(draws, 4)\nbins\n\n[(-3.121, -0.675], (0.687, 3.211], (-3.121, -0.675], (-0.675, 0.0134], (-0.675, 0.0134], ..., (0.0134, 0.687], (0.0134, 0.687], (-0.675, 0.0134], (0.0134, 0.687], (-0.675, 0.0134]]\nLength: 1000\nCategories (4, interval[float64, right]): [(-3.121, -0.675] &lt; (-0.675, 0.0134] &lt; (0.0134, 0.687] &lt; (0.687, 3.211]]\n\n\n\nbins = pd.qcut(draws, 4, labels=['Q1', 'Q2', 'Q3', 'Q4'])\nbins\n# then use groupby\nbins = pd.Series(bins, name='quartile')\nresults = (pd.Series(draws)\n               .groupby(bins)\n               .agg(['count', 'min', 'max'])\n               .reset_index())\n\nLeads to better performance",
    "crumbs": [
      "7. Data Cleaning and Preparation"
    ]
  },
  {
    "objectID": "04_main.html",
    "href": "04_main.html",
    "title": "4. NumPy Basics: Arrays and Vectorized Computation",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "4. NumPy Basics: Arrays and Vectorized Computation"
    ]
  },
  {
    "objectID": "04_main.html#learning-objectives",
    "href": "04_main.html#learning-objectives",
    "title": "4. NumPy Basics: Arrays and Vectorized Computation",
    "section": "",
    "text": "Learn about NumPy, a package for numerical computing in Python\nUse NumPy for array-based data: operations, algorithms",
    "crumbs": [
      "4. NumPy Basics: Arrays and Vectorized Computation"
    ]
  },
  {
    "objectID": "04_main.html#import-numpy",
    "href": "04_main.html#import-numpy",
    "title": "4. NumPy Basics: Arrays and Vectorized Computation",
    "section": "Import NumPy",
    "text": "Import NumPy\n\nimport numpy as np # Recommended standard NumPy convention",
    "crumbs": [
      "4. NumPy Basics: Arrays and Vectorized Computation"
    ]
  },
  {
    "objectID": "04_main.html#array-based-operations",
    "href": "04_main.html#array-based-operations",
    "title": "4. NumPy Basics: Arrays and Vectorized Computation",
    "section": "Array-based operations",
    "text": "Array-based operations\n\nA fast, flexible container for large datasets in Python\nStores multiple items of the same type together\nCan perform operations on whole blocks of data with similar syntax\n\n\n\n\nImage of an array with 10 length and the first index, 8th element, and indicies denoted by text\n\n\n\nCreate an arrayPerform operation\n\n\n\narr = np.array([[1.5, -0.1, 3], [0, -3, 6.5]])\narr\n\narray([[ 1.5, -0.1,  3. ],\n       [ 0. , -3. ,  6.5]])\n\n\n\n\nAll of the elements have been multiplied by 10.\n\narr * 10\n\narray([[ 15.,  -1.,  30.],\n       [  0., -30.,  65.]])\n\n\n\n\n\n\nEvery array has a shape indicating the size of each dimension\nand a dtype, an object describing the data type of the array\n\n\nShapedtype\n\n\n\narr.shape\n\n(2, 3)\n\n\n\n\n\narr.dtype\n\ndtype('float64')\n\n\n\n\n\n\nndarray\n\nGeneric one/multi-dimensional container where all elements are the same type\nCreated using numpy.array function\n\n\n1DMulti-dimensional\n\n\n\ndata1 = [6, 7.5, 8, 0, 1]\narr1 = np.array(data1)\narr1\n\narray([6. , 7.5, 8. , 0. , 1. ])\n\n\n\nprint(arr1.ndim)\nprint(arr1.shape)\n\n1\n(5,)\n\n\n\n\n\ndata2 = [[1, 2, 3, 4], [5, 6, 7, 8]]\narr2 = np.array(data2)\narr2\n\narray([[1, 2, 3, 4],\n       [5, 6, 7, 8]])\n\n\n\nprint(arr2.ndim)\nprint(arr2.shape)\n\n2\n(2, 4)\n\n\n\n\n\n\nSpecial array creation\n\nnumpy.zeros creates an array of zeros with a given length or shape\nnumpy.ones creates an array of ones with a given length or shape\nnumpy.empty creates an array without initialized values\nnumpy.arange creates a range\nPass a tuple for the shape to create a higher dimensional array\n\n\nZerosMulti-dimensional\n\n\n\nnp.zeros(10)\n\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n\n\n\n\n\nnp.zeros((3, 6))\n\narray([[0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.]])\n\n\n\n\n\n\n\nnumpy.empty does not return an array of zeros, though it may look like it.\n\nnp.empty(1)\n\narray([0.])\n\n\nWes provides a table of array creation functions in the book.\n\n\nData types for ndarrays\n\nUnless explicitly specified, numpy.array tries to infer a good data created arrays.\nData type is stored in a special dtype metadata object.\nCan be explict or converted (cast)\nIt is important to care about the general kind of data you’re dealing with.\n\n\nInferred dtypeExplicit dtypeCast dtypeCast dtype using another array\n\n\n\narr1.dtype\n\ndtype('float64')\n\n\n\n\n\narr2 = np.array([1, 2, 3], dtype=np.int32)\narr2.dtype\n\ndtype('int32')\n\n\n\n\n\nfloat_arr = arr1.astype(np.float64)\nfloat_arr.dtype\n\ndtype('float64')\n\n\n\n\n\nint_array = arr1.astype(arr2.dtype)\nint_array.dtype\n\ndtype('int32')\n\n\n\n\n\n\n\nCalling astype always creates a new array (a copy of the data), even if the new data type is the same as the old data type.\nWes provides a table of supported data types in the book.",
    "crumbs": [
      "4. NumPy Basics: Arrays and Vectorized Computation"
    ]
  },
  {
    "objectID": "04_main.html#arithmetic-with-numpy-arrays",
    "href": "04_main.html#arithmetic-with-numpy-arrays",
    "title": "4. NumPy Basics: Arrays and Vectorized Computation",
    "section": "Arithmetic with NumPy Arrays",
    "text": "Arithmetic with NumPy Arrays\n\nVectorizationArithmetic operations with scalarsComparisons between arrays\n\n\nBatch operations on data without for loops\n\narr = np.array([[1., 2., 3.], [4., 5., 6.]])\narr * arr\n\narray([[ 1.,  4.,  9.],\n       [16., 25., 36.]])\n\n\n\n\nPropagate the scalar argument to each element in the array\n\n1 / arr\n\narray([[1.        , 0.5       , 0.33333333],\n       [0.25      , 0.2       , 0.16666667]])\n\n\n\n\nof the same size yield boolean arrays\n\narr2 = np.array([[0., 4., 1.], [7., 2., 12.]])\n\narr2 &gt; arr\n\narray([[False,  True, False],\n       [ True, False,  True]])",
    "crumbs": [
      "4. NumPy Basics: Arrays and Vectorized Computation"
    ]
  },
  {
    "objectID": "04_main.html#basic-indexing-and-slicing",
    "href": "04_main.html#basic-indexing-and-slicing",
    "title": "4. NumPy Basics: Arrays and Vectorized Computation",
    "section": "Basic Indexing and Slicing",
    "text": "Basic Indexing and Slicing\n\nselect a subset of your data or individual elements\n\n\narr = np.arange(10)\narr\n\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n\n\n\nArray views are on the original data. Data is not copied, and any modifications to the view will be reflected in the source array. If you want a copy of a slice of an ndarray instead of a view, you will need to explicitly copy the array—for example, arr[5:8].copy().\n\nselect the sixth elementselect sixth through eighthbroadcast data\n\n\n\narr[5]\n\n5\n\n\n\n\n\narr[5:8]\n\narray([5, 6, 7])\n\n\n\n\n\narr[5:8] = 12\n\n\n\n\nExample of “not copied data”\nOriginal\n\narr_slice = arr[5:8]\narr\n\narray([ 0,  1,  2,  3,  4, 12, 12, 12,  8,  9])\n\n\nChange values in new array\nNotice that arr is now changed.\n\narr_slice[1] = 123\narr\n\narray([  0,   1,   2,   3,   4,  12, 123,  12,   8,   9])\n\n\nChange all values in an array\nThis is done with bare slice [:]:\n\narr_slice[:] = 64\narr_slice\n\narray([64, 64, 64])\n\n\nHigher dimensional arrays have 1D arrays at each index:\n\narr2d = np.array([[1,2,3], [4,5,6], [7,8,9]])\narr2d\n\narray([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\n\n\nTo slice, can pass a comma-separated list to select individual elements:\n\narr2d[0][2]\n\n3\n\n\n\nOmitting indicies will reduce number of dimensions:\n\narr2d[0]\n\narray([1, 2, 3])\n\n\nCan assign scalar values or arrays:\n\narr2d[0] = 9\narr2d\n\narray([[9, 9, 9],\n       [4, 5, 6],\n       [7, 8, 9]])\n\n\nOr create an array of the indices. This is like indexing in two steps:\n\narr2d = np.array([[1,2,3], [4,5,6], [7,8,9]])\narr2d[1,0]\n\n4\n\n\n\nIndexing with slices\nndarrays can be sliced with the same syntax as Python lists:\n\narr = np.arange(10)\n\narr[1:6]\n\narray([1, 2, 3, 4, 5])\n\n\nThis slices a range of elements (“select the first row of arr2d”):\n\n# arr2d[row, column]\narr2d[:1]\n\narray([[1, 2, 3]])\n\n\nCan pass multiple indicies:\n\narr2d[:3, :1] # colons keep the dimensions\n# arr2d[0:3, 0] # does not keep the dimensions\n\narray([[1],\n       [4],\n       [7]])",
    "crumbs": [
      "4. NumPy Basics: Arrays and Vectorized Computation"
    ]
  },
  {
    "objectID": "04_main.html#boolean-indexing",
    "href": "04_main.html#boolean-indexing",
    "title": "4. NumPy Basics: Arrays and Vectorized Computation",
    "section": "Boolean Indexing",
    "text": "Boolean Indexing\n\nnames = np.array([\"Bob\", \"Joe\", \"Will\", \"Bob\", \"Will\", \"Joe\", \"Joe\"])\nnames\n\narray(['Bob', 'Joe', 'Will', 'Bob', 'Will', 'Joe', 'Joe'], dtype='&lt;U4')\n\n\n\ndata = np.array([[4, 7], [0, 2], [-5, 6], [0, 0], [1, 2], [-12, -4], [3, 4]])\ndata\n\narray([[  4,   7],\n       [  0,   2],\n       [ -5,   6],\n       [  0,   0],\n       [  1,   2],\n       [-12,  -4],\n       [  3,   4]])\n\n\nLike arithmetic operations, comparisons (such as ==) with arrays are also vectorized.\n\nnames == \"Bob\"\n\narray([ True, False, False,  True, False, False, False])\n\n\nThis boolean array can be passed when indexing the array:\n\ndata[names == \"Bob\"]\n\narray([[4, 7],\n       [0, 0]])\n\n\nSelect from the rows where names == “Bob” and index the columns, too:\n\ndata[names == \"Bob\", 1:]\n\narray([[7],\n       [0]])\n\n\nSelect everything but “Bob”:\n\nnames != \"Bob\" # or ~(names == \"Bob\")\n\narray([False,  True,  True, False,  True,  True,  True])\n\n\nUse boolean arithmetic operators like & (and) and | (or):\n\nmask = (names == \"Bob\") | (names == \"Will\")\nmask\n\narray([ True, False,  True,  True,  True, False, False])\n\n\n\n\nSelecting data from an array by boolean indexing and assigning the result to a new variable always creates a copy of the data.\nSetting values with boolean arrays works by substituting the value or values on the righthand side into the locations where the boolean array’s values are True.\n\ndata[data &lt; 0] = 0\n\nYou can also set whole rows or columns using a one-dimensional boolean array:\n\ndata[names != \"Joe\"] = 7",
    "crumbs": [
      "4. NumPy Basics: Arrays and Vectorized Computation"
    ]
  },
  {
    "objectID": "04_main.html#fancy-indexing",
    "href": "04_main.html#fancy-indexing",
    "title": "4. NumPy Basics: Arrays and Vectorized Computation",
    "section": "Fancy Indexing",
    "text": "Fancy Indexing\nA term adopted by NumPy to describe indexing using integer arrays.\n\narr = np.zeros((8, 4)) # 8 × 4 array\n\nfor i in range(8):\n  arr[i] = i\n\narr\n\narray([[0., 0., 0., 0.],\n       [1., 1., 1., 1.],\n       [2., 2., 2., 2.],\n       [3., 3., 3., 3.],\n       [4., 4., 4., 4.],\n       [5., 5., 5., 5.],\n       [6., 6., 6., 6.],\n       [7., 7., 7., 7.]])\n\n\nPass a list or ndarray of integers specifying the desired order to subset rows in a particular order:\n\narr[[4, 3, 0, 6]]\n\narray([[4., 4., 4., 4.],\n       [3., 3., 3., 3.],\n       [0., 0., 0., 0.],\n       [6., 6., 6., 6.]])\n\n\nUse negative indices selects rows from the end:\n\narr[[-3, -5, -7]]\n\narray([[5., 5., 5., 5.],\n       [3., 3., 3., 3.],\n       [1., 1., 1., 1.]])\n\n\nPassing multiple index arrays selects a one-dimensional array of elements corresponding to each tuple of indices (go down then across):\n\narr = np.arange(32).reshape((8, 4))\narr\n\narray([[ 0,  1,  2,  3],\n       [ 4,  5,  6,  7],\n       [ 8,  9, 10, 11],\n       [12, 13, 14, 15],\n       [16, 17, 18, 19],\n       [20, 21, 22, 23],\n       [24, 25, 26, 27],\n       [28, 29, 30, 31]])\n\n\nHere, the elements (1, 0), (5, 3), (7, 1), and (2, 2) are selected.\n\narr[[1, 5, 7, 2], [0, 3, 1, 2]]\n\narray([ 4, 23, 29, 10])\n\n\n\n\nFancy indexing, unlike slicing, always copies the data into a new array when assigning the result to a new variable.",
    "crumbs": [
      "4. NumPy Basics: Arrays and Vectorized Computation"
    ]
  },
  {
    "objectID": "04_main.html#transposing-arrays-and-swapping-axes",
    "href": "04_main.html#transposing-arrays-and-swapping-axes",
    "title": "4. NumPy Basics: Arrays and Vectorized Computation",
    "section": "Transposing Arrays and Swapping Axes",
    "text": "Transposing Arrays and Swapping Axes\nTransposing is a special form of reshaping using the special T attribute:\n\narr = np.arange(15).reshape((3, 5))\narr\n\narray([[ 0,  1,  2,  3,  4],\n       [ 5,  6,  7,  8,  9],\n       [10, 11, 12, 13, 14]])\n\n\n\narr.T\n\narray([[ 0,  5, 10],\n       [ 1,  6, 11],\n       [ 2,  7, 12],\n       [ 3,  8, 13],\n       [ 4,  9, 14]])\n\n\n\nMatrix multiplication\n\nusing Tusing @ infix operator\n\n\n\nnp.dot(arr.T, arr)\n\narray([[125, 140, 155, 170, 185],\n       [140, 158, 176, 194, 212],\n       [155, 176, 197, 218, 239],\n       [170, 194, 218, 242, 266],\n       [185, 212, 239, 266, 293]])\n\n\n\n\n\narr.T @ arr\n\narray([[125, 140, 155, 170, 185],\n       [140, 158, 176, 194, 212],\n       [155, 176, 197, 218, 239],\n       [170, 194, 218, 242, 266],\n       [185, 212, 239, 266, 293]])\n\n\n\n\n\nndarray has the method swapaxes, which takes a pair of axis numbers and switches the indicated axes to rearrange the data:\n\narr = np.array([[0, 1, 0], [1, 2, -2], [6, 3, 2], [-1, 0, -1], [1, 0, 1], [3, 5, 6]])\narr\narr.swapaxes(0, 1)\n\narray([[ 0,  1,  6, -1,  1,  3],\n       [ 1,  2,  3,  0,  0,  5],\n       [ 0, -2,  2, -1,  1,  6]])",
    "crumbs": [
      "4. NumPy Basics: Arrays and Vectorized Computation"
    ]
  },
  {
    "objectID": "04_main.html#pseudorandom-number-generation",
    "href": "04_main.html#pseudorandom-number-generation",
    "title": "4. NumPy Basics: Arrays and Vectorized Computation",
    "section": "Pseudorandom Number Generation",
    "text": "Pseudorandom Number Generation\nThe numpy.random module supplements the built-in Python random module with functions for efficiently generating whole arrays of sample values from many kinds of probability distributions.\n\nMuch faster than Python’s built-in random module\n\n\nsamples = np.random.standard_normal(size=(4, 4))\nsamples\n\narray([[ 0.17488936,  1.40484911,  0.15183398, -1.02194459],\n       [ 0.69530047,  1.69838274, -0.5782449 , -0.32245913],\n       [ 1.30932161, -0.48999345, -0.13171682,  0.67943756],\n       [-0.12637043, -0.82355441, -0.86697578, -0.06906716]])\n\n\nCan use an explicit generator:\n\nseed determines initial state of generator\n\n\nrng = np.random.default_rng(seed=12345)\ndata = rng.standard_normal((2, 3))\ndata\n\narray([[-1.42382504,  1.26372846, -0.87066174],\n       [-0.25917323, -0.07534331, -0.74088465]])\n\n\nWes provides a table of NumPy random number generator methods",
    "crumbs": [
      "4. NumPy Basics: Arrays and Vectorized Computation"
    ]
  },
  {
    "objectID": "04_main.html#universal-functions-fast-element-wise-array-functions",
    "href": "04_main.html#universal-functions-fast-element-wise-array-functions",
    "title": "4. NumPy Basics: Arrays and Vectorized Computation",
    "section": "Universal Functions: Fast Element-Wise Array Functions",
    "text": "Universal Functions: Fast Element-Wise Array Functions\nA universal function, or ufunc, is a function that performs element-wise operations on data in ndarrays.\nMany ufuncs are simple element-wise transformations:\n\nunarybinarymultiple\n\n\nOne array\n\narr = np.arange(10)\nnp.sqrt(arr)\n\narray([0.        , 1.        , 1.41421356, 1.73205081, 2.        ,\n       2.23606798, 2.44948974, 2.64575131, 2.82842712, 3.        ])\n\n\n\n\n\narr1 = rng.standard_normal(10)\narr2 = rng.standard_normal(10)\nnp.maximum(arr1, arr2)\n\narray([ 0.78884434,  0.6488928 ,  0.57585751,  1.39897899,  2.34740965,\n        0.96849691,  0.90291934,  0.90219827, -0.15818926,  0.44948393])\n\n\n\n\n\nremainder, whole_part = np.modf(arr1)\nremainder\n\narray([-0.3677927 ,  0.6488928 ,  0.36105811, -0.95286306,  0.34740965,\n        0.96849691, -0.75938718,  0.90219827, -0.46695317, -0.06068952])\n\n\n\n\n\nUse the out argument to assign results into an existing array rather than create a new one:\n\nout = np.zeros_like(arr)\nnp.add(arr, 1, out=out)\n\narray([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])",
    "crumbs": [
      "4. NumPy Basics: Arrays and Vectorized Computation"
    ]
  },
  {
    "objectID": "04_main.html#array-oriented-programming-with-arrays",
    "href": "04_main.html#array-oriented-programming-with-arrays",
    "title": "4. NumPy Basics: Arrays and Vectorized Computation",
    "section": "Array-Oriented Programming with Arrays",
    "text": "Array-Oriented Programming with Arrays\nEvaluate the function sqrt(x^2 + y^2) across a regular grid of values: use the numpy.meshgrid function takes two one-dimensional arrays and produce two two-dimensional matrices corresponding to all pairs of (x, y) in the two arrays:\n\npoints = np.arange(-5, 5, 0.01) # 100 equally spaced points\nxs, ys = np.meshgrid(points, points)\nxs\n\narray([[-5.  , -4.99, -4.98, ...,  4.97,  4.98,  4.99],\n       [-5.  , -4.99, -4.98, ...,  4.97,  4.98,  4.99],\n       [-5.  , -4.99, -4.98, ...,  4.97,  4.98,  4.99],\n       ...,\n       [-5.  , -4.99, -4.98, ...,  4.97,  4.98,  4.99],\n       [-5.  , -4.99, -4.98, ...,  4.97,  4.98,  4.99],\n       [-5.  , -4.99, -4.98, ...,  4.97,  4.98,  4.99]])\n\n\n\nys\n\narray([[-5.  , -5.  , -5.  , ..., -5.  , -5.  , -5.  ],\n       [-4.99, -4.99, -4.99, ..., -4.99, -4.99, -4.99],\n       [-4.98, -4.98, -4.98, ..., -4.98, -4.98, -4.98],\n       ...,\n       [ 4.97,  4.97,  4.97, ...,  4.97,  4.97,  4.97],\n       [ 4.98,  4.98,  4.98, ...,  4.98,  4.98,  4.98],\n       [ 4.99,  4.99,  4.99, ...,  4.99,  4.99,  4.99]])\n\n\nEvaluate the function as if it were two points:\n\nz = np.sqrt(xs ** 2 + ys ** 2)\nz\n\narray([[7.07106781, 7.06400028, 7.05693985, ..., 7.04988652, 7.05693985,\n        7.06400028],\n       [7.06400028, 7.05692568, 7.04985815, ..., 7.04279774, 7.04985815,\n        7.05692568],\n       [7.05693985, 7.04985815, 7.04278354, ..., 7.03571603, 7.04278354,\n        7.04985815],\n       ...,\n       [7.04988652, 7.04279774, 7.03571603, ..., 7.0286414 , 7.03571603,\n        7.04279774],\n       [7.05693985, 7.04985815, 7.04278354, ..., 7.03571603, 7.04278354,\n        7.04985815],\n       [7.06400028, 7.05692568, 7.04985815, ..., 7.04279774, 7.04985815,\n        7.05692568]])\n\n\n\nBonus: matplotlib visualization\n\nimport matplotlib.pyplot as plt\n\nplt.imshow(z, cmap=plt.cm.gray)  #, extent=[-25, 10, -10, 10])\nplt.colorbar() \nplt.title(\"Image plot of $\\sqrt{x^2 + y^2}$ for a grid of values\")\n\nText(0.5, 1.0, 'Image plot of $\\\\sqrt{x^2 + y^2}$ for a grid of values')\n\n\n\n\n\n\n\n\n\n\nplt.close(\"all\")",
    "crumbs": [
      "4. NumPy Basics: Arrays and Vectorized Computation"
    ]
  },
  {
    "objectID": "04_main.html#expressing-conditional-logic-as-array-operations",
    "href": "04_main.html#expressing-conditional-logic-as-array-operations",
    "title": "4. NumPy Basics: Arrays and Vectorized Computation",
    "section": "Expressing Conditional Logic as Array Operations",
    "text": "Expressing Conditional Logic as Array Operations\nThe numpy.where function is a vectorized version of the ternary expression x if condition else.\n\nsecond and third arguments to numpy.where can also be scalars\ncan also combine scalars and arrays\n\n\nxarr = np.array([1.1, 1.2, 1.3, 1.4, 1.5])\nyarr = np.array([2.1, 2.2, 2.3, 2.4, 2.5])\ncond = np.array([True, False, True, True, False])\n\nTake a value from xarr whenever the corresponding value in cond is True, and otherwise take the value from yarr:\n\nx if condition elsenumpy.where\n\n\n\nresult = [(x if c else y)\n  for x, y, c in zip(xarr, yarr, cond)]\n  \nresult\n\n[1.1, 2.2, 1.3, 1.4, 2.5]\n\n\n\n\n\nresult = np.where(cond, xarr, yarr)\nresult\n\narray([1.1, 2.2, 1.3, 1.4, 2.5])\n\n\n\n\n\nCan also do this with scalars, or combine arrays and scalars:\n\narr = rng.standard_normal((4,4))\narr\n\narray([[-1.34360107, -0.08168759,  1.72473993,  2.61815943],\n       [ 0.77736134,  0.8286332 , -0.95898831, -1.20938829],\n       [-1.41229201,  0.54154683,  0.7519394 , -0.65876032],\n       [-1.22867499,  0.25755777,  0.31290292, -0.13081169]])\n\n\n\nnp.where(arr &gt; 0, 2, -2)\n\narray([[-2, -2,  2,  2],\n       [ 2,  2, -2, -2],\n       [-2,  2,  2, -2],\n       [-2,  2,  2, -2]])\n\n\n\n# set only positive to 2\nnp.where(arr &gt; 0,2,arr)\n\narray([[-1.34360107, -0.08168759,  2.        ,  2.        ],\n       [ 2.        ,  2.        , -0.95898831, -1.20938829],\n       [-1.41229201,  2.        ,  2.        , -0.65876032],\n       [-1.22867499,  2.        ,  2.        , -0.13081169]])",
    "crumbs": [
      "4. NumPy Basics: Arrays and Vectorized Computation"
    ]
  },
  {
    "objectID": "04_main.html#mathematical-and-statistical-methods",
    "href": "04_main.html#mathematical-and-statistical-methods",
    "title": "4. NumPy Basics: Arrays and Vectorized Computation",
    "section": "Mathematical and Statistical Methods",
    "text": "Mathematical and Statistical Methods\nUse “aggregations’ like sum, mean, and std\n\nIf using NumPy, must pass the array you want to aggregate as the first argument\n\n\narr = rng.standard_normal((5, 4))\n\narr.mean()\n\n0.06622379901441691\n\n\n\nnp.mean(arr)\n\n0.06622379901441691\n\n\nCan use axis to specify which axis to computer the statistic\n\n“compute across the columns”“compute across the rows”\n\n\n\narr.mean(axis=1)\n\narray([ 0.00066383,  0.40377331,  0.44452789, -0.36983452, -0.14801151])\n\n\n\n\n\narr.mean(axis=0)\n\narray([ 0.54494867, -0.10500845,  0.15080113, -0.32584615])\n\n\n\n\n\nOther methods like cumsum and cumprod do not aggregate, instead producing an array of the intermediate results:\n\narr.cumsum()\n\narray([1.26998312e+00, 1.17702066e+00, 1.11086977e+00, 2.65530664e-03,\n       1.38612157e-01, 1.48568992e+00, 1.54683394e+00, 1.61774854e+00,\n       2.05140308e+00, 2.32888674e+00, 2.85913913e+00, 3.39586010e+00,\n       4.01421011e+00, 3.21919265e+00, 3.51922360e+00, 1.91652201e+00,\n       2.18332084e+00, 9.21697056e-01, 8.50426250e-01, 1.32447598e+00])\n\n\nIn multidimensional arrays, accumulation functions like cumsum compute along the indicated axis:\n\n“compute across the columns”“compute across the rows”\n\n\n\narr.cumsum(axis=1)\n\narray([[ 1.26998312,  1.17702066,  1.11086977,  0.00265531],\n       [ 0.13595685,  1.48303461,  1.54417864,  1.61509324],\n       [ 0.43365454,  0.7111382 ,  1.24139058,  1.77811155],\n       [ 0.61835001, -0.17666744,  0.1233635 , -1.47933809],\n       [ 0.26679883, -0.99482495, -1.06609576, -0.59204603]])\n\n\n\n\n\narr.cumsum(axis=0)\n\narray([[ 1.26998312, -0.09296246, -0.06615089, -1.10821447],\n       [ 1.40593997,  1.25411531, -0.00500687, -1.03729987],\n       [ 1.83959451,  1.53159897,  0.52524552, -0.5005789 ],\n       [ 2.45794452,  0.73658151,  0.82527646, -2.10328049],\n       [ 2.72474335, -0.52504227,  0.75400566, -1.62923076]])",
    "crumbs": [
      "4. NumPy Basics: Arrays and Vectorized Computation"
    ]
  },
  {
    "objectID": "04_main.html#methods-for-boolean-arrays",
    "href": "04_main.html#methods-for-boolean-arrays",
    "title": "4. NumPy Basics: Arrays and Vectorized Computation",
    "section": "Methods for Boolean Arrays",
    "text": "Methods for Boolean Arrays\nBoolean values are coerced to 1 (True) and 0 (False) in the preceding methods. Thus, sum is often used as a means of counting True values in a boolean array:\n\n(arr &gt; 0).sum() # Number of positive values\n\n13\n\n\nany tests whether one or more values in an array is True, while all checks if every value is True:\n\nbools = np.array([False, False, True, False])\nbools.any()\n\nTrue",
    "crumbs": [
      "4. NumPy Basics: Arrays and Vectorized Computation"
    ]
  },
  {
    "objectID": "04_main.html#sorting",
    "href": "04_main.html#sorting",
    "title": "4. NumPy Basics: Arrays and Vectorized Computation",
    "section": "Sorting",
    "text": "Sorting\nNumPy arrays can be sorted in place with the sort method:\n\narr = rng.standard_normal(6)\narr.sort()\narr\n\narray([-1.64041784, -1.15452958, -0.85725882, -0.41485376,  0.0977165 ,\n        0.68828179])\n\n\nCan sort multidimensional section by providing an axis:\n\narr = rng.standard_normal((5, 3))\n\n\n“compute across the columns”“compute across the rows”\n\n\n\narr.cumsum(axis=1)\n\narray([[ 0.65045239, -0.73790756, -1.64529002],\n       [-1.09542531, -1.08827961, -0.55391971],\n       [-1.06580785, -1.24728059,  0.37467121],\n       [-0.31739195, -1.13320691, -0.7466279 ],\n       [-0.22363893, -0.92532973, -2.72104291]])\n\n\n\n\n\narr.cumsum(axis=0)\n\narray([[ 0.65045239, -1.38835995, -0.90738246],\n       [-0.44497292, -1.38121426, -0.37302255],\n       [-1.51078076, -1.562687  ,  1.24892924],\n       [-1.82817271, -2.37850196,  1.63550826],\n       [-2.05181164, -3.08019277, -0.16020491]])\n\n\n\n\n\nThe top-level method numpy.sort returns a sorted copy of an array (like the Python built-in function sorted) instead of modifying the array in place:\n\narr2 = np.array([5, -10, 7, 1, 0, -3])\nsorted_arr2 = np.sort(arr2)\nsorted_arr2\n\narray([-10,  -3,   0,   1,   5,   7])",
    "crumbs": [
      "4. NumPy Basics: Arrays and Vectorized Computation"
    ]
  },
  {
    "objectID": "04_main.html#unique-and-other-set-logic",
    "href": "04_main.html#unique-and-other-set-logic",
    "title": "4. NumPy Basics: Arrays and Vectorized Computation",
    "section": "Unique and Other Set Logic",
    "text": "Unique and Other Set Logic\nnumpy.unique returns the sorted unique values in an array:\n\nnp.unique(names)\n\narray(['Bob', 'Joe', 'Will'], dtype='&lt;U4')\n\n\nnumpy.in1d tests membership of the values in one array in another, returning a boolean array:\n\nnp.in1d(arr1, arr2)\n\narray([False, False, False, False, False, False, False, False, False,\n       False])",
    "crumbs": [
      "4. NumPy Basics: Arrays and Vectorized Computation"
    ]
  },
  {
    "objectID": "04_main.html#file-input-and-output-with-arrays",
    "href": "04_main.html#file-input-and-output-with-arrays",
    "title": "4. NumPy Basics: Arrays and Vectorized Computation",
    "section": "File Input and Output with Arrays",
    "text": "File Input and Output with Arrays\nNumPy is able to save np.save and load np.load data to and from disk in some text or binary formats.\nArrays are saved by default in an uncompressed raw binary format with file extension .npy:\n\narr = np.arange(10)\nnp.save(\"some_array\", arr)\n\n\nnp.load(\"some_array.npy\")\n\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n\n\nSave multiple arrays in an uncompressed archive using numpy.savez\nIf your data compresses well, use numpy.savez_compressed instead",
    "crumbs": [
      "4. NumPy Basics: Arrays and Vectorized Computation"
    ]
  },
  {
    "objectID": "04_main.html#linear-algebra",
    "href": "04_main.html#linear-algebra",
    "title": "4. NumPy Basics: Arrays and Vectorized Computation",
    "section": "4.6 Linear Algebra",
    "text": "4.6 Linear Algebra\nLinear algebra operations, like matrix multiplication, decompositions, determinants, and other square matrix math, can be done with Numpy (np.dot(y) vs x.dot(y)):\n\nnp.dot(arr1, arr)\n\n7.221776767282354",
    "crumbs": [
      "4. NumPy Basics: Arrays and Vectorized Computation"
    ]
  },
  {
    "objectID": "04_main.html#example-random-walks",
    "href": "04_main.html#example-random-walks",
    "title": "4. NumPy Basics: Arrays and Vectorized Computation",
    "section": "Example: Random Walks",
    "text": "Example: Random Walks\n\nimport matplotlib.pyplot as plt\n#! blockstart\nimport random\nposition = 0\nwalk = [position]\nnsteps = 1000\nfor _ in range(nsteps):\n    step = 1 if random.randint(0, 1) else -1\n    position += step\n    walk.append(position)\n#! blockend\n\nplt.plot(walk[:100])\nplt.show()",
    "crumbs": [
      "4. NumPy Basics: Arrays and Vectorized Computation"
    ]
  }
]