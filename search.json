[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Python for Data Analysis Book Club",
    "section": "",
    "text": "This is a companion for the book Python for Data Analysis, 3E by Wes McKinney.\nThis website is being developed by the R4DS Online Learning Community. Follow along and join the community to participate.\nThis companion follows the R4DS Online Learning Community Code of Conduct."
  },
  {
    "objectID": "index.html#book-club-meetings",
    "href": "index.html#book-club-meetings",
    "title": "Python for Data Analysis Book Club",
    "section": "Book club meetings",
    "text": "Book club meetings\n\nEach week, a volunteer will present a chapter from the book.\n\nThis is the best way to learn the material.\n\nPresentations will usually consist of a review of the material, a discussion, and/or a demonstration of the principles presented in that chapter.\nMore information about how to present is available in the GitHub repo.\nPresentations will be recorded and will be available on the R4DS Online Learning Community YouTube Channel."
  },
  {
    "objectID": "01_notes.html",
    "href": "01_notes.html",
    "title": "Notes",
    "section": "",
    "text": "One of the most important languages for data science, machine learning, and general software development in academia and industry."
  },
  {
    "objectID": "01_notes.html#essential-python-libraries",
    "href": "01_notes.html#essential-python-libraries",
    "title": "Notes",
    "section": "Essential Python Libraries",
    "text": "Essential Python Libraries\n\nNumPy ( Numerical Python)\nPandas\nMatplotlib\nIpython and Jupyter\nScipy\nSklearn\nStatsModel"
  },
  {
    "objectID": "01_notes.html#numpy",
    "href": "01_notes.html#numpy",
    "title": "Notes",
    "section": "Numpy",
    "text": "Numpy\n\nShort for Numerical Python, has long been a cornerstone of numerical computing in Python. It provides the data structures, algorithms, and library glue needed for most scientific applications involving numerical data in Python"
  },
  {
    "objectID": "01_notes.html#pandas",
    "href": "01_notes.html#pandas",
    "title": "Notes",
    "section": "Pandas",
    "text": "Pandas\n\npandas provides high-level data structures and functions designed to make working with structured or tabular data intuitive and flexible.\n\n\nIt provides convenient indexing functionality to enable you to reshape, slice and dice, perform aggregations, and select subsets of data. Since data manipulation, preparation, and cleaning is such an important skill in data analysis,\nSee R vs Pandas comparison"
  },
  {
    "objectID": "01_notes.html#matplotlib",
    "href": "01_notes.html#matplotlib",
    "title": "Notes",
    "section": "Matplotlib",
    "text": "Matplotlib\n\nis the most popular Python library for producing plots and other two-dimensional data visualizations"
  },
  {
    "objectID": "01_notes.html#ipython-and-jupyter",
    "href": "01_notes.html#ipython-and-jupyter",
    "title": "Notes",
    "section": "IPython and Jupyter",
    "text": "IPython and Jupyter\n\nThe IPython system can now be used as a kernel (a programming language mode) for using Python with Jupyter."
  },
  {
    "objectID": "01_notes.html#scipy",
    "href": "01_notes.html#scipy",
    "title": "Notes",
    "section": "SciPy",
    "text": "SciPy\n\nSciPy is a collection of packages addressing a number of foundational problems in scientific computing."
  },
  {
    "objectID": "01_notes.html#scikit-learn",
    "href": "01_notes.html#scikit-learn",
    "title": "Notes",
    "section": "Scikit-learn",
    "text": "Scikit-learn\n\ngeneral-purpose machine learning toolkit for Python programmers."
  },
  {
    "objectID": "01_notes.html#statsmodels",
    "href": "01_notes.html#statsmodels",
    "title": "Notes",
    "section": "Statsmodels",
    "text": "Statsmodels\n\nis a statistical analysis package\n\n\nCompared with scikit-learn, statsmodels contains algorithms for classical (primarily frequentist) statistics and econometrics."
  },
  {
    "objectID": "01_notes.html#other-packages",
    "href": "01_notes.html#other-packages",
    "title": "Notes",
    "section": "Other Packages",
    "text": "Other Packages\n\nTensorFlow or PyTorch or Keras"
  },
  {
    "objectID": "01_notes.html#installing-necessary-packages",
    "href": "01_notes.html#installing-necessary-packages",
    "title": "Notes",
    "section": "Installing Necessary Packages",
    "text": "Installing Necessary Packages\n\nWe can install Python packages using ‚ÄúPip‚Äù or ‚ÄúConda‚Äù. Read more about pip vs python\n\nThe author recommends:\n\nMiniconda, a minimal installation of the conda package manager, along with conda-forge, a community-maintained software distribution based on conda.\nThis book uses Python 3.10 throughout."
  },
  {
    "objectID": "01_notes.html#mini-conda",
    "href": "01_notes.html#mini-conda",
    "title": "Notes",
    "section": "Mini-conda",
    "text": "Mini-conda\n\nConda is a packaging tool and installer that aims to do more than what pip does; handle library dependencies outside of the Python packages as well as the Python packages themselves. Conda also creates a virtual environment, like virtualenv does"
  },
  {
    "objectID": "01_notes.html#mini-forge",
    "href": "01_notes.html#mini-forge",
    "title": "Notes",
    "section": "Mini-forge",
    "text": "Mini-forge\n\nminiforge is the community (conda-forge) driven minimalistic conda installer. Subsequent package installations come thus from conda-forge channel. Mini-forge\nminiconda is the Anaconda (company) driven minimalistic conda installer. Subsequent package installations come from the anaconda channels (default or otherwise).\nminiforge started because miniconda doens‚Äôt support aarch64, very quickly the ‚ÄòPyPy‚Äô people jumped on board, and in the mean time there are also miniforge versions for all Linux architectures, as well as MacOS.\nAARCH64, sometimes also referred to as ARM64, is a CPU architecture developed by ARM Ltd., and a 64-bit extension of the pre-existing ARM architecture. ARM architectures are primarily known for their energy efficiency and low power consumption. For that reason, virtually all mobile phones and tablets today use ARM architecture-based CPUs.\nAlthough AARCH64 and x64 (Intel, AMD, ‚Ä¶) are both 64-bit CPU architectures, their inner basics are vastly different. Programs compiled for one platform, won‚Äôt work on the other (except with some magic), and vice-versa. That means, software does not only need to be recompiled, but often requires extensive optimization for either platform.\n\n\nThe first step is to configure conda-forge as your default package channel by running the following commands in a shell:\n\n\n! conda config --add channels conda-forge\n! conda config --set channel_priority strict\n\nWarning: 'conda-forge' already in 'channels' list, moving to the top\n\n\nNow, we will install the essential packages used throughout the book (along with their dependencies) with conda install\n\nconda create -y -n pydata-book python=3.10 # create enviroment with python 3.10 installed\nconda activate pydata-book # activate enviroment \n(pydata-book) $ conda install -y pandas jupyter matplotlib # install a\n\nInstall complete packages used in the the book\nconda install lxml beautifulsoup4 html5lib openpyxl\nrequests sqlalchemy seaborn scipy statsmodels\npatsy scikit-learn pyarrow pytables numba"
  },
  {
    "objectID": "01_notes.html#should-i-use-pip-or-conda",
    "href": "01_notes.html#should-i-use-pip-or-conda",
    "title": "Notes",
    "section": "Should I use Pip or Conda ?",
    "text": "Should I use Pip or Conda ?\n\nWhile you can use both conda and pip to install packages, you should avoid updating packages originally installed with conda using pip (and vice versa), as doing so can lead to environment problems. I recommend sticking to conda if you can and falling back on pip only for packages which are unavailable with conda install.\n\n\nconda install should always be preferred, but some packages are not available through conda so if conda install $package_name fails, try pip install $package_name."
  },
  {
    "objectID": "01_notes.html#what-can-we-do-with-conda",
    "href": "01_notes.html#what-can-we-do-with-conda",
    "title": "Notes",
    "section": "What can we do with Conda?",
    "text": "What can we do with Conda?\n\nMany commands : create env, activate env, delete env, lists env\nInstall tldr (https://github.com/tldr-pages/tldr) : The tldr-pages project is a collection of community-maintained help pages for command-line tools, that aims to be a simpler, more approachable complement to traditional"
  },
  {
    "objectID": "01_notes.html#navigating-this-book",
    "href": "01_notes.html#navigating-this-book",
    "title": "Notes",
    "section": "Navigating This Book",
    "text": "Navigating This Book\n\nChapter two and three: provides prerequisite knowledge for the remainder of the book. If you have Python experienc you can skip\n\n\nChapter four : Numpy\n\n\nChapter five : Pandas\n\n\nChapter six : Data loading, Storage and File format\n\n\nChapter seven : Data cleaning and Preparation\n\n\nChpater eight : Data wrangling\n\n\nChapter Nine : Plotting and Visualization\n\n\nChapter 10 : Data agreegation and Group operation"
  },
  {
    "objectID": "01_notes.html#import-conventions",
    "href": "01_notes.html#import-conventions",
    "title": "Notes",
    "section": "Import Conventions",
    "text": "Import Conventions\n\nThe Python community has adopted a number of naming conventions for commonly used modules:\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport statsmodels as\n\nSyntaxError: invalid syntax (1580968464.py, line 5)\n\n\n\nThis means that when you see np.arange, this is a reference to the arange function in NumPy. This is done because it‚Äôs considered bad practice in Python software development to import everything (from numpy import *) from a large package like NumPy\n\n\nimport numpy as np \n\nx = np.random.random((64, 3, 32, 10)) \ny = np.random.random((32, 10)) \n\nz = np.maximum(x, y)"
  },
  {
    "objectID": "01_video.html",
    "href": "01_video.html",
    "title": "Video",
    "section": "",
    "text": "00:22:16    Isabella Vel√°squez: For more info on TidyTuesday: https://github.com/rfordatascience/tidytuesday#readme\n00:26:23    shamsuddeen:    https://docs.google.com/spreadsheets/d/1io_R_ZaGtzJcDF5KcY4hbgMBsJVi_4USE4JGsHcv9ug/edit#gid=0\n00:45:23    shamsuddeen:    https://github.com/conda-forge/miniforge\n00:59:07    shamsuddeen:    https://github.com/conda-forge/miniforge\n01:03:03    shamsuddeen:    https://blogs.shmuhammad.com/"
  },
  {
    "objectID": "02_video.html",
    "href": "02_video.html",
    "title": "Video",
    "section": "",
    "text": "LOG\n00:09:36    Isabella Vel√°squez: https://github.com/r4ds/bookclub-py4da\n00:15:43    Layla Bouzoubaa:    heart heart heart\n00:15:50    Layla Bouzoubaa:    I am lving quarto\n00:16:07    Layla Bouzoubaa:    üòªüòªüòªüòª\n00:16:27    shamsuddeen:    Yes\n00:17:49    Jadey Ryan: thank you so much Isabella! so helpful ü•≥\n00:21:12    Isabella Vel√°squez: Get started with Quarto: https://quarto.org/docs/get-started/\n00:21:50    Ron:    Busted ;)\n00:39:01    Isabella Vel√°squez: import pandas as pd\n\n%pdef pd.read_csv\n00:39:23    Isabella Vel√°squez: ^ this runs (no parentheses)\n00:41:15    Ron:    For simpler functions %pdef is pretty helpful, for example %pdef pd.DataFrame is only a few lines ;)\n00:41:46    Layla Bouzoubaa:    ahh, perfect! thank you much!\n00:43:13    Layla Bouzoubaa:    wooaahh\n00:52:53    Isabella Vel√°squez: Error in \"5\" + 6 : non-numeric argument to binary operator\n01:02:27    Ron:    I LOVE f-strings.\n01:06:45    Isabella Vel√°squez: I have to go, thanks so much, this was great!\n01:07:19    Layla Bouzoubaa:    me as well, have a great weekend all!\n01:08:17    Jim Gruman: thankyou!!!\n01:08:25    Jadey Ryan: thank you!!!"
  },
  {
    "objectID": "03_main.html",
    "href": "03_main.html",
    "title": "3. Built-in Data Structures, Functions, and Files",
    "section": "",
    "text": "The data structures tuples, lists, dictionaries, and sets\nFunctions\nErrors and Exception Handling\nFiles and the Operating System"
  },
  {
    "objectID": "03_notes.html",
    "href": "03_notes.html",
    "title": "1¬† Data Structures and Sequences",
    "section": "",
    "text": "A tuple is a fixed-length, immutable sequence of Python objects which, once assigned, cannot be changed. The easiest way to create one is with a comma-separated sequence of values wrapped in parentheses:\n\ntup = (4, 5, 6)\ntup\n\n(4, 5, 6)\n\n\nIn many contexts, the parentheses can be omitted\n\ntup = 4, 5, 6\ntup\n\n(4, 5, 6)\n\n\nYou can convert any sequence or iterator to a tuple by invoking\n\ntuple([4,0,2])\n\ntup = tuple('string')\n\ntup\n\n('s', 't', 'r', 'i', 'n', 'g')\n\n\nElements can be accessed with square brackets []\nNote the zero indexing\n\ntup[0]\n\n's'\n\n\nTuples of tuples\n\nnested_tup = (4,5,6),(7,8)\n\nnested_tup\n\n((4, 5, 6), (7, 8))\n\n\n\nnested_tup[0]\n\n(4, 5, 6)\n\n\n\nnested_tup[1]\n\n(7, 8)\n\n\nWhile the objects stored in a tuple may be mutable themselves, once the tuple is created it‚Äôs not possible to modify which object is stored in each slot:\n\ntup = tuple(['foo', [1, 2], True])\n\ntup[2]\n\nTrue\n\n\n```{python}\n\ntup[2] = False\n\n```\nTypeError                                 Traceback (most recent call last)\nInput In [9], in <cell line: 1>()\n----> 1 tup[2] = False\n\nTypeError: 'tuple' object does not support item assignment\nTypeError: 'tuple' object does not support item assignment\nIf an object inside a tuple is mutable, such as a list, you can modify it in place\n\ntup[1].append(3)\n\ntup\n\n('foo', [1, 2, 3], True)\n\n\nYou can concatenate tuples using the + operator to produce longer tuples:\n\n(4, None, 'foo') + (6, 0) + ('bar',)\n\n(4, None, 'foo', 6, 0, 'bar')\n\n\n\n\nIf you try to assign to a tuple-like expression of variables, Python will attempt to unpack the value on the righthand side of the equals sign:\n\ntup = (4, 5, 6)\ntup\n\n(4, 5, 6)\n\n\n\na, b, c = tup\n\nc\n\n6\n\n\nEven sequences with nested tuples can be unpacked:\n\ntup = 4, 5, (6,7)\n\na, b, (c, d) = tup\n\nd\n\n7\n\n\nTo easily swap variable names\n\na, b = 1, 4\n\na\n\n1\n\n\n\nb\n\n4\n\n\n\nb, a = a, b\n\na\n\n4\n\n\n\nb\n\n1\n\n\nA common use of variable unpacking is iterating over sequences of tuples or lists\n\nseq = [(1, 2, 3), (4, 5, 6), (7, 8, 9)]\n\nseq\n\n[(1, 2, 3), (4, 5, 6), (7, 8, 9)]\n\n\n\nfor a, b, c in seq:\n     print(f'a={a}, b={b}, c={c}')\n\na=1, b=2, c=3\na=4, b=5, c=6\na=7, b=8, c=9\n\n\n*rest syntax for plucking elements\n\nvalues = 1,2,3,4,5\n\na, b, *rest = values\n\nrest\n\n[3, 4, 5]\n\n\nAs a matter of convention, many Python programmers will use the underscore (_) for unwanted variables:\n\na, b, *_ = values\n\n\n\n\nSince the size and contents of a tuple cannot be modified, it is very light on instance methods. A particularly useful one (also available on lists) is count\n\na = (1,2,2,2,2,3,4,5,7,8,9)\n\na.count(2)\n\n4"
  },
  {
    "objectID": "03_notes.html#list",
    "href": "03_notes.html#list",
    "title": "1¬† Data Structures and Sequences",
    "section": "1.2 List",
    "text": "1.2 List\n\nIn contrast with tuples, lists are variable length and their contents can be modified in place.\nLists are mutable.\nLists use [] square brackts or the list function\n\na_list = [2, 3, 7, None]\n\ntup = (\"foo\", \"bar\", \"baz\")\n\nb_list = list(tup)\n\nb_list\n\n['foo', 'bar', 'baz']\n\n\n\nb_list[1] = \"peekaboo\"\n\nb_list\n\n['foo', 'peekaboo', 'baz']\n\n\nLists and tuples are semantically similar (though tuples cannot be modified) and can be used interchangeably in many functions.\n\ngen = range(10)\n\ngen\n\nrange(0, 10)\n\n\n\nlist(gen)\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\n\n1.2.1 Adding and removing list elements\nthe append method\n\nb_list.append(\"dwarf\")\n\nb_list\n\n['foo', 'peekaboo', 'baz', 'dwarf']\n\n\nthe insert method\n\nb_list.insert(1, \"red\")\n\nb_list\n\n['foo', 'red', 'peekaboo', 'baz', 'dwarf']\n\n\ninsert is computationally more expensive than append\nthe pop method, the inverse of insert\n\nb_list.pop(2)\n\n'peekaboo'\n\n\n\nb_list\n\n['foo', 'red', 'baz', 'dwarf']\n\n\nthe remove method\n\nb_list.append(\"foo\")\n\nb_list\n\n['foo', 'red', 'baz', 'dwarf', 'foo']\n\n\n\nb_list.remove(\"foo\")\n\nb_list\n\n['red', 'baz', 'dwarf', 'foo']\n\n\nCheck if a list contains a value using the in keyword:\n\n\"dwarf\" in b_list\n\nTrue\n\n\nThe keyword not can be used to negate an in\n\n\"dwarf\" not in b_list\n\nFalse\n\n\n\n\n1.2.2 Concatenating and combining lists\nsimilar with tuples, use + to concatenate\n\n[4, None, \"foo\"] + [7, 8, (2, 3)]\n\n[4, None, 'foo', 7, 8, (2, 3)]\n\n\nthe extend method\n\nx = [4, None, \"foo\"]\n\nx.extend([7,8,(2,3)])\n\nx\n\n[4, None, 'foo', 7, 8, (2, 3)]\n\n\nlist concatenation by addition is an expensive operation\nusing extend is preferable\n```{python}\neverything = []\nfor chunk in list_of_lists:\n    everything.extend(chunk)\n\n```\nis generally faster than\n```{python}\n\neverything = []\nfor chunk in list_of_lists:\n    everything = everything + chunk\n\n```\n\n\n1.2.3 Sorting\nthe sort method\n\na = [7, 2, 5, 1, 3]\n\na.sort()\n\na\n\n[1, 2, 3, 5, 7]\n\n\nsort options\n\nb = [\"saw\", \"small\", \"He\", \"foxes\", \"six\"]\n\nb.sort(key = len)\n\nb\n\n['He', 'saw', 'six', 'small', 'foxes']\n\n\n\n\n1.2.4 Slicing\nSlicing semantics takes a bit of getting used to, especially if you‚Äôre coming from R or MATLAB.\nusing the indexing operator []\n\nseq = [7, 2, 3, 7, 5, 6, 0, 1]\n\nseq[3:5]\n\n[7, 5]\n\n\nalso assigned with a sequence\n\nseq[3:5] = [6,3]\n\nseq\n\n[7, 2, 3, 6, 3, 6, 0, 1]\n\n\nEither the start or stop can be omitted\n\nseq[:5]\n\n[7, 2, 3, 6, 3]\n\n\n\nseq[3:]\n\n[6, 3, 6, 0, 1]\n\n\nNegative indices slice the sequence relative to the end:\n\nseq[-4:]\n\n[3, 6, 0, 1]\n\n\nA step can also be used after a second colon to, say, take every other element:\n\nseq[::2]\n\n[7, 3, 3, 0]\n\n\nA clever use of this is to pass -1, which has the useful effect of reversing a list or tuple:\n\nseq[::-1]\n\n[1, 0, 6, 3, 6, 3, 2, 7]"
  },
  {
    "objectID": "03_notes.html#dictionary",
    "href": "03_notes.html#dictionary",
    "title": "1¬† Data Structures and Sequences",
    "section": "1.3 Dictionary",
    "text": "1.3 Dictionary\n\nThe dictionary or dict may be the most important built-in Python data structure.\nOne approach for creating a dictionary is to use curly braces {} and colons to separate keys and values:\n\nempty_dict = {}\n\nd1 = {\"a\": \"some value\", \"b\": [1, 2, 3, 4]}\n\nd1\n\n{'a': 'some value', 'b': [1, 2, 3, 4]}\n\n\naccess, insert, or set elements\n\nd1[7] = \"an integer\"\n\nd1\n\n{'a': 'some value', 'b': [1, 2, 3, 4], 7: 'an integer'}\n\n\nand as before\n\n\"b\" in d1\n\nTrue\n\n\nthe del and pop methods\n\ndel d1[7]\n\nd1\n\n{'a': 'some value', 'b': [1, 2, 3, 4]}\n\n\n\nret = d1.pop(\"a\")\n\nret\n\n'some value'\n\n\nThe keys and values methods\n\nlist(d1.keys())\n\n['b']\n\n\n\nlist(d1.values())\n\n[[1, 2, 3, 4]]\n\n\nthe items method\n\nlist(d1.items())\n\n[('b', [1, 2, 3, 4])]\n\n\nthe update method to merge one dictionary into another\n\nd1.update({\"b\": \"foo\", \"c\": 12})\n\nd1\n\n{'b': 'foo', 'c': 12}\n\n\n### Creating dictionaries from sequences\n\nlist(range(5))\n\n[0, 1, 2, 3, 4]\n\n\n\ntuples = zip(range(5), reversed(range(5)))\n\ntuples\n\nmapping = dict(tuples)\n\nmapping\n\n{0: 4, 1: 3, 2: 2, 3: 1, 4: 0}\n\n\n\n1.3.1 Default values\nimagine categorizing a list of words by their first letters as a dictionary of lists\n\nwords = [\"apple\", \"bat\", \"bar\", \"atom\", \"book\"]\n\nby_letter = {}\n\nfor word in words:\n        letter = word[0]\n        if letter not in by_letter:\n            by_letter[letter] = [word]\n        else:\n            by_letter[letter].append(word)\n\nby_letter\n\n{'a': ['apple', 'atom'], 'b': ['bat', 'bar', 'book']}\n\n\nThe setdefault dictionary method can be used to simplify this workflow. The preceding for loop can be rewritten as:\n\nby_letter = {}\n\nfor word in words:\n        letter = word[0]\n        by_letter.setdefault(letter, []).append(word)\n\nby_letter\n\n{'a': ['apple', 'atom'], 'b': ['bat', 'bar', 'book']}\n\n\nThe built-in collectionsmodule has a useful class, defaultdict, which makes this even easier.\n\nfrom collections import defaultdict\n\nby_letter = defaultdict(list)\n\nfor word in words:\n        by_letter[word[0]].append(word)\n\nby_letter\n\ndefaultdict(list, {'a': ['apple', 'atom'], 'b': ['bat', 'bar', 'book']})\n\n\n\n\n1.3.2 Valid dictionary key types\nkeys generally have to be immutable objects like scalars or tuples for hashability\nTo use a list as a key, one option is to convert it to a tuple, which can be hashed as long as its elements also can be:\n\nd = {}\n\nd[tuple([1,2,3])] = 5\n\nd\n\n{(1, 2, 3): 5}"
  },
  {
    "objectID": "03_notes.html#set",
    "href": "03_notes.html#set",
    "title": "1¬† Data Structures and Sequences",
    "section": "1.4 Set",
    "text": "1.4 Set\n\ncan be created in two ways: via the set function or via a set literal with curly braces:\n\nset([2, 2, 2, 1, 3, 3])\n\n{2,2,1,3,3}\n\n{1, 2, 3}\n\n\nSets support mathematical set operations like union, intersection, difference, and symmetric difference.\nThe union of these two sets:\n\na = {1, 2, 3, 4, 5}\n\nb = {3, 4, 5, 6, 7, 8}\n\na.union(b)\n\na | b\n\n{1, 2, 3, 4, 5, 6, 7, 8}\n\n\nThe &operator or the intersection method\n\na.intersection(b)\n\na & b\n\n{3, 4, 5}\n\n\nA table of commonly used set methods\nAll of the logical set operations have in-place counterparts, which enable you to replace the contents of the set on the left side of the operation with the result. For very large sets, this may be more efficient\n\nc = a.copy()\n\nc |= b\n\nc\n\n{1, 2, 3, 4, 5, 6, 7, 8}\n\n\n\nd = a.copy()\n\nd &= b\n\nd\n\n{3, 4, 5}\n\n\nset elements generally must be immutable, and they must be hashable\nyou can convert them to tuples\nYou can also check if a set is a subset of (is contained in) or a superset of (contains all elements of) another set\n\na_set = {1, 2, 3, 4, 5}\n\n{1, 2, 3}.issubset(a_set)\n\nTrue\n\n\n\na_set.issuperset({1, 2, 3})\n\nTrue"
  },
  {
    "objectID": "03_notes.html#built-in-sequence-functions",
    "href": "03_notes.html#built-in-sequence-functions",
    "title": "1¬† Data Structures and Sequences",
    "section": "1.5 Built-In Sequence Functions",
    "text": "1.5 Built-In Sequence Functions\n\n1.5.1 enumerate\nenumerate returns a sequence of (i, value) tuples\n\n\n1.5.2 sorted\nsorted returns a new sorted list\n\nsorted([7,1,2,9,3,6,5,0,22])\n\n[0, 1, 2, 3, 5, 6, 7, 9, 22]\n\n\n\n\n1.5.3 zip\nzip ‚Äúpairs‚Äù up the elements of a number of lists, tuples, or other sequences to create a list of tuples\n\nseq1 = [\"foo\", \"bar\", \"baz\"]\n\nseq2 = [\"one\", \"two\", \"three\"]\n\nzipped = zip(seq1, seq2)\n\nlist(zipped)\n\n[('foo', 'one'), ('bar', 'two'), ('baz', 'three')]\n\n\nzip can take an arbitrary number of sequences, and the number of elements it produces is determined by the shortest sequence\n\nseq3 = [False, True]\n\nlist(zip(seq1, seq2, seq3))\n\n[('foo', 'one', False), ('bar', 'two', True)]\n\n\nA common use of zip is simultaneously iterating over multiple sequences, possibly also combined with enumerate\n\nfor index, (a, b) in enumerate(zip(seq1, seq2)):\n    print(f\"{index}: {a}, {b}\")\n\n0: foo, one\n1: bar, two\n2: baz, three\n\n\nreversed iterates over the elements of a sequence in reverse order\n\nlist(reversed(range(10)))\n\n[9, 8, 7, 6, 5, 4, 3, 2, 1, 0]"
  },
  {
    "objectID": "03_notes.html#list-set-and-dictionary-comprehensions",
    "href": "03_notes.html#list-set-and-dictionary-comprehensions",
    "title": "1¬† Data Structures and Sequences",
    "section": "1.6 List, Set, and Dictionary Comprehensions",
    "text": "1.6 List, Set, and Dictionary Comprehensions\n[expr for value in collection if condition]\nFor example, given a list of strings, we could filter out strings with length 2 or less and convert them to uppercase like this\n\nstrings = [\"a\", \"as\", \"bat\", \"car\", \"dove\", \"python\"]\n\n[x.upper() for x in strings if len(x) > 2]\n\n['BAT', 'CAR', 'DOVE', 'PYTHON']\n\n\nA dictionary comprehension looks like this\ndict_comp = {key-expr: value-expr for value in collection\n             if condition}\nSuppose we wanted a set containing just the lengths of the strings contained in the collection\n\nunique_lengths = {len(x) for x in strings}\n\nunique_lengths\n\n{1, 2, 3, 4, 6}\n\n\nwe could create a lookup map of these strings for their locations in the list\n\nloc_mapping = {value: index for index, value in enumerate(strings)}\n\nloc_mapping\n\n{'a': 0, 'as': 1, 'bat': 2, 'car': 3, 'dove': 4, 'python': 5}"
  },
  {
    "objectID": "03_notes.html#nested-list-comprehensions",
    "href": "03_notes.html#nested-list-comprehensions",
    "title": "1¬† Data Structures and Sequences",
    "section": "1.7 Nested list comprehensions",
    "text": "1.7 Nested list comprehensions\nSuppose we have a list of lists containing some English and Spanish names. We want to get a single list containing all names with two or more a‚Äôs in them\n\nall_data = [[\"John\", \"Emily\", \"Michael\", \"Mary\", \"Steven\"],\n            [\"Maria\", \"Juan\", \"Javier\", \"Natalia\", \"Pilar\"]]\n\nresult = [name for names in all_data for name in names\n          if name.count(\"a\") >= 2]\n\nresult\n\n['Maria', 'Natalia']\n\n\nHere is another example where we ‚Äúflatten‚Äù a list of tuples of integers into a simple list of integers\n\nsome_tuples = [(1, 2, 3), (4, 5, 6), (7, 8, 9)]\n\nflattened = [x for tup in some_tuples for x in tup]\n\nflattened\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9]"
  },
  {
    "objectID": "03_notes.html#namespaces-scope-and-local-functions",
    "href": "03_notes.html#namespaces-scope-and-local-functions",
    "title": "1¬† Data Structures and Sequences",
    "section": "2.1 Namespaces, Scope, and Local Functions",
    "text": "2.1 Namespaces, Scope, and Local Functions\nA more descriptive name describing a variable scope in Python is a namespace.\nConsider the following function\n\na = []\n\ndef func():\n    for i in range(5):\n        a.append(i)\n\nWhen func() is called, the empty list a is created, five elements are appended, and then a is destroyed when the function exits.\n\nfunc()\n\nfunc()\n\na\n\n[0, 1, 2, 3, 4, 0, 1, 2, 3, 4]"
  },
  {
    "objectID": "03_notes.html#returing-multiple-values",
    "href": "03_notes.html#returing-multiple-values",
    "title": "1¬† Data Structures and Sequences",
    "section": "2.2 Returing Multiple Values",
    "text": "2.2 Returing Multiple Values\nWhat‚Äôs happening here is that the function is actually just returning one object, a tuple, which is then being unpacked into the result variables.\n\ndef f():\n    a = 5\n    b = 6\n    c = 7\n    return a, b, c\n\na, b, c = f()\n\na\n\n5"
  },
  {
    "objectID": "03_notes.html#functions-are-objects",
    "href": "03_notes.html#functions-are-objects",
    "title": "1¬† Data Structures and Sequences",
    "section": "2.3 Functions are Objects",
    "text": "2.3 Functions are Objects\nSuppose we were doing some data cleaning and needed to apply a bunch of transformations to the following list of strings:\n\nstates = [\"   Alabama \", \"Georgia!\", \"Georgia\", \"georgia\", \"FlOrIda\",\n          \"south   carolina##\", \"West virginia?\"]\n\nimport re\n\ndef clean_strings(strings):\n    result = []\n    for value in strings:\n        value = value.strip()\n        value = re.sub(\"[!#?]\", \"\", value)\n        value = value.title()\n        result.append(value)\n    return result\n\nclean_strings(states)\n\n['Alabama',\n 'Georgia',\n 'Georgia',\n 'Georgia',\n 'Florida',\n 'South   Carolina',\n 'West Virginia']\n\n\nAnother approach\n\ndef remove_punctuation(value):\n    return re.sub(\"[!#?]\", \"\", value)\n\nclean_ops = [str.strip, remove_punctuation, str.title]\n\ndef clean_strings(strings, ops):\n    result = []\n    for value in strings:\n        for func in ops:\n            value = func(value)\n        result.append(value)\n    return result\n\nclean_strings(states, clean_ops)\n\n['Alabama',\n 'Georgia',\n 'Georgia',\n 'Georgia',\n 'Florida',\n 'South   Carolina',\n 'West Virginia']\n\n\nYou can use functions as arguments to other functions like the built-in map function\n\nfor x in map(remove_punctuation, states):\n    print(x)\n\n   Alabama \nGeorgia\nGeorgia\ngeorgia\nFlOrIda\nsouth   carolina\nWest virginia"
  },
  {
    "objectID": "03_notes.html#anonymous-lambda-functions",
    "href": "03_notes.html#anonymous-lambda-functions",
    "title": "1¬† Data Structures and Sequences",
    "section": "2.4 Anonymous Lambda Functions",
    "text": "2.4 Anonymous Lambda Functions\na way of writing functions consisting of a single statement\nsuppose you wanted to sort a collection of strings by the number of distinct letters in each string\n\nstrings = [\"foo\", \"card\", \"bar\", \"aaaaaaa\", \"ababdo\"]\n\nstrings.sort(key=lambda x: len(set(x)))\n\nstrings\n\n['aaaaaaa', 'foo', 'bar', 'card', 'ababdo']"
  },
  {
    "objectID": "03_notes.html#generator-expressions",
    "href": "03_notes.html#generator-expressions",
    "title": "1¬† Data Structures and Sequences",
    "section": "3.1 Generator expressions",
    "text": "3.1 Generator expressions\nThis is a generator analogue to list, dictionary, and set comprehensions. To create one, enclose what would otherwise be a list comprehension within parentheses instead of brackets:\n\ngen = (x ** 2 for x in range(100))\n\ngen\n\n<generator object <genexpr> at 0x000001A3294B9C10>\n\n\nGenerator expressions can be used instead of list comprehensions as function arguments in some cases:\n\nsum(x ** 2 for x in range(100))\n\n328350\n\n\n\ndict((i, i ** 2) for i in range(5))\n\n{0: 0, 1: 1, 2: 4, 3: 9, 4: 16}"
  },
  {
    "objectID": "03_notes.html#itertools-module",
    "href": "03_notes.html#itertools-module",
    "title": "1¬† Data Structures and Sequences",
    "section": "3.2 itertools module",
    "text": "3.2 itertools module\nitertools module has a collection of generators for many common data algorithms.\ngroupby takes any sequence and a function, grouping consecutive elements in the sequence by return value of the function\n\nimport itertools\n\ndef first_letter(x):\n    return x[0]\n\nnames = [\"Alan\", \"Adam\", \"Jackie\", \"Lily\", \"Katie\", \"Molly\"]\n\nfor letter, names in itertools.groupby(names, first_letter):\n    print(letter, list(names))\n\nA ['Alan', 'Adam']\nJ ['Jackie']\nL ['Lily']\nK ['Katie']\nM ['Molly']\n\n\nTable of other itertools functions"
  },
  {
    "objectID": "03_notes.html#exceptions-in-ipython",
    "href": "03_notes.html#exceptions-in-ipython",
    "title": "1¬† Data Structures and Sequences",
    "section": "4.1 Exceptions in IPython",
    "text": "4.1 Exceptions in IPython\nIf an exception is raised while you are %run-ing a script or executing any statement, IPython will by default print a full call stack trace. Having additional context by itself is a big advantage over the standard Python interpreter"
  },
  {
    "objectID": "03_notes.html#byte-and-unicode-with-files",
    "href": "03_notes.html#byte-and-unicode-with-files",
    "title": "1¬† Data Structures and Sequences",
    "section": "5.1 Byte and Unicode with Files",
    "text": "5.1 Byte and Unicode with Files\nThe default behavior for Python files (whether readable or writable) is text mode, which means that you intend to work with Python strings (i.e., Unicode)."
  },
  {
    "objectID": "03_video.html",
    "href": "03_video.html",
    "title": "Video",
    "section": "",
    "text": "LOG\n00:30:39    Karim Badr: Is there a difference between append and extend?\n00:30:57    Ron:    Append is for adding a single element\n00:31:11    Karim Badr: Thanks!\n00:36:34    Ron:    Some discussion of this here: https://stackoverflow.com/questions/11364533/why-are-slice-and-range-upper-bound-exclusive\n00:36:49    Ron:    if you are wondering (like I am) why it is like that :)"
  },
  {
    "objectID": "04_video.html",
    "href": "04_video.html",
    "title": "Video",
    "section": "",
    "text": "LOG\n00:37:17    Serena DeStefani:   Sorry, I have to go today because I have some work left to do. But this session is very nice and hopefully I will be back! Thank you\n00:53:43    Karim Badr: Quick google search about transpose vs swapaxes and other forms for this: https://blog.fearcat.in/a?ID=01750-160525d9-9c23-4590-9020-6a3f90d3bdbf"
  },
  {
    "objectID": "05_main.html",
    "href": "05_main.html",
    "title": "5. Getting Started with pandas",
    "section": "",
    "text": "Learn about Pandas two major data structures: Series and DataFrame\nLearn some essential functionality!"
  },
  {
    "objectID": "05_notes.html",
    "href": "05_notes.html",
    "title": "Notes",
    "section": "",
    "text": "Note\n\n\n\nThis is a long chapter, these notes are intended as a tour of main ideas!\n\n\n\n\n\nPanda bus tour!\n\n\n\nPandas is a major tool in Python data analysis\nWorks with Numpy, adding support for tabular / heterogenous data"
  },
  {
    "objectID": "05_notes.html#import-conventions",
    "href": "05_notes.html#import-conventions",
    "title": "Notes",
    "section": "Import conventions:",
    "text": "Import conventions:\n\nimport numpy as np\nimport pandas as pd"
  },
  {
    "objectID": "05_notes.html#pandas-primary-data-structures",
    "href": "05_notes.html#pandas-primary-data-structures",
    "title": "Notes",
    "section": "Panda‚Äôs primary data structures",
    "text": "Panda‚Äôs primary data structures\n\nSeries: One dimensional object containing a sequence of values of the same type.\nDataFrame: Tabular data, similar (and inspired by) R dataframe.\nOther structures will be introduced as they arise, e.g.¬†Index and Groupby objects.\n\n\nSeries\n\nobj = pd.Series([4,7,-4,3], index = [\"A\",\"B\",\"C\",\"D\"])\nobj\n\nA    4\nB    7\nC   -4\nD    3\ndtype: int64\n\n\nThe index is optional, if not specified it will default to 0 through N-1\n\nSelection\nSelect elements or sub-Series by labels, sets of labels, boolean arrays ‚Ä¶\n\nobj['A']\n\n4\n\n\n\nobj[['A','C']]\n\nA    4\nC   -4\ndtype: int64\n\n\n\nobj[obj > 3]\n\nA    4\nB    7\ndtype: int64\n\n\n\n\nOther things you can do\n\nNumpy functions and Numpy-like operations work as expected:\n\n\nobj*3\n\nA    12\nB    21\nC   -12\nD     9\ndtype: int64\n\n\n\nnp.exp(obj)\n\nA      54.598150\nB    1096.633158\nC       0.018316\nD      20.085537\ndtype: float64\n\n\n\nSeries can be created from and converted to a dictionary\n\n\nobj.to_dict()\n\n{'A': 4, 'B': 7, 'C': -4, 'D': 3}\n\n\n\nSeries can be converted to numpy array:\n\n\nobj.to_numpy()\n\narray([ 4,  7, -4,  3], dtype=int64)\n\n\n\n\n\nDataFrame\n\nRepresents table of data\nHas row index index and column index column\nCommon way to create is from a dictionary, but see Table 5.1 for more!\n\n\ntest = pd.DataFrame({\"cars\":['Chevy','Ford','Dodge','BMW'],'MPG':[14,15,16,12], 'Year':[1979, 1980, 2001, 2020]})\ntest\n\n\n\n\n\n  \n    \n      \n      cars\n      MPG\n      Year\n    \n  \n  \n    \n      0\n      Chevy\n      14\n      1979\n    \n    \n      1\n      Ford\n      15\n      1980\n    \n    \n      2\n      Dodge\n      16\n      2001\n    \n    \n      3\n      BMW\n      12\n      2020\n    \n  \n\n\n\n\n\nIf you want a non-default index, it can be specified just like with Series.\nhead(n) / tail(n) - return the first / last n rows, 5 by default\n\n\nSelecting\n\nCan retrieve columns or sets of columns by using obj[...]:\n\n\ntest['cars']\n\n0    Chevy\n1     Ford\n2    Dodge\n3      BMW\nName: cars, dtype: object\n\n\nNote that we got a Series here.\n\ntest[['cars','MPG']]\n\n\n\n\n\n  \n    \n      \n      cars\n      MPG\n    \n  \n  \n    \n      0\n      Chevy\n      14\n    \n    \n      1\n      Ford\n      15\n    \n    \n      2\n      Dodge\n      16\n    \n    \n      3\n      BMW\n      12\n    \n  \n\n\n\n\n\nDot notation can also be used (test.cars) as long as the column names are valid identifiers\nRows can be retrieved with iloc[...] and loc[...]:\n\nloc retrieves by index\niloc retrieves by position.\n\n\n\n\nModifying / Creating Columns\n\nColumns can be modified (and created) by assignment:\n\n\ntest['MPG^2'] = test['MPG']**2\ntest\n\n\n\n\n\n  \n    \n      \n      cars\n      MPG\n      Year\n      MPG^2\n    \n  \n  \n    \n      0\n      Chevy\n      14\n      1979\n      196\n    \n    \n      1\n      Ford\n      15\n      1980\n      225\n    \n    \n      2\n      Dodge\n      16\n      2001\n      256\n    \n    \n      3\n      BMW\n      12\n      2020\n      144\n    \n  \n\n\n\n\n\ndel keyword can be used to drop columns, or drop method can be used to do so non-destructively\n\n\n\n\nIndex object\n\nIndex objects are used for holding axis labels and other metadata\n\n\ntest.index\n\nRangeIndex(start=0, stop=4, step=1)\n\n\n\nCan change the index, in this case replacing the default:\n\n\n# Create index from one of the columns\ntest.index = test['cars']  \n\n # remove 'cars' column since i am using as an index now.  s\ntest=test.drop('cars', axis = \"columns\")  # or axis  = 1\ntest\n\n\n\n\n\n  \n    \n      \n      MPG\n      Year\n      MPG^2\n    \n    \n      cars\n      \n      \n      \n    \n  \n  \n    \n      Chevy\n      14\n      1979\n      196\n    \n    \n      Ford\n      15\n      1980\n      225\n    \n    \n      Dodge\n      16\n      2001\n      256\n    \n    \n      BMW\n      12\n      2020\n      144\n    \n  \n\n\n\n\n\nNote the axis keyword argument above, many DataFrame methods use this.\nAbove I changed a column into an index. Often you want to go the other way, this can be done with reset_index:\n\n\ntest.reset_index()  # Note this doesn't actually change test\n\n\n\n\n\n  \n    \n      \n      cars\n      MPG\n      Year\n      MPG^2\n    \n  \n  \n    \n      0\n      Chevy\n      14\n      1979\n      196\n    \n    \n      1\n      Ford\n      15\n      1980\n      225\n    \n    \n      2\n      Dodge\n      16\n      2001\n      256\n    \n    \n      3\n      BMW\n      12\n      2020\n      144\n    \n  \n\n\n\n\n\nColumns are an index as well:\n\n\ntest.columns\n\nIndex(['MPG', 'Year', 'MPG^2'], dtype='object')\n\n\n\nIndexes act like immutable sets, see Table 5.2 in book for Index methods and properties"
  },
  {
    "objectID": "05_notes.html#essential-functionality",
    "href": "05_notes.html#essential-functionality",
    "title": "Notes",
    "section": "Essential Functionality",
    "text": "Essential Functionality\n\nReindexing and dropping\n\nreindex creats a new object with the values arranged according to the new index. Missing values are used if necessary, or you can use optional fill methods. You can use iloc and loc to reindex as well.\n\n\ns = pd.Series([1,2,3,4,5], index = list(\"abcde\"))\ns2 = s.reindex(list(\"abcfu\"))  #  not a song by GAYLE \ns2\n\na    1.0\nb    2.0\nc    3.0\nf    NaN\nu    NaN\ndtype: float64\n\n\n\nMissing values and can be tested for with isna or notna methods\n\n\npd.isna(s2)\n\na    False\nb    False\nc    False\nf     True\nu     True\ndtype: bool\n\n\n\ndrop , illustrated above can drop rows or columns. In addition to using axis you can use columns or index. Again these make copies.\n\n\ntest.drop(columns = 'MPG')\n\n\n\n\n\n  \n    \n      \n      Year\n      MPG^2\n    \n    \n      cars\n      \n      \n    \n  \n  \n    \n      Chevy\n      1979\n      196\n    \n    \n      Ford\n      1980\n      225\n    \n    \n      Dodge\n      2001\n      256\n    \n    \n      BMW\n      2020\n      144\n    \n  \n\n\n\n\n\ntest.drop(index = ['Ford', 'BMW'])\n\n\n\n\n\n  \n    \n      \n      MPG\n      Year\n      MPG^2\n    \n    \n      cars\n      \n      \n      \n    \n  \n  \n    \n      Chevy\n      14\n      1979\n      196\n    \n    \n      Dodge\n      16\n      2001\n      256\n    \n  \n\n\n\n\n\n\nIndexing, Selection and Filtering\n\nSeries\n\nFor Series, indexing is similar to Numpy, except you can use the index as well as integers.\n\n\nobj = pd.Series(np.arange(4.), index=[\"a\", \"b\", \"c\", \"d\"])\nobj[0:3]\n\na    0.0\nb    1.0\nc    2.0\ndtype: float64\n\n\n\nobj['a':'c']\n\na    0.0\nb    1.0\nc    2.0\ndtype: float64\n\n\n\nobj[obj<2]\n\na    0.0\nb    1.0\ndtype: float64\n\n\n\nobj[['a','d']]\n\na    0.0\nd    3.0\ndtype: float64\n\n\n\nHowever, preferred way is to use loc for selection by index and iloc for selection by position. This is to avoid the issue where the index is itself integers.\n\n\nobj.loc[['a','d']]\n\na    0.0\nd    3.0\ndtype: float64\n\n\n\nobj.iloc[1]\n\n1.0\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote if a range or a set of indexes is used, a Series is returned. If a single item is requested, you get just that item.\n\n\n\n\nDataFrame\n\nSelecting with df[...] for a DataFrame retrieves one or more columns as we have seen, if you select a single column you get a Series\nThere are some special cases, indexing with a boolean selects rows, as does selecting with a slice:\n\n\ntest[0:1]\n\n\n\n\n\n  \n    \n      \n      MPG\n      Year\n      MPG^2\n    \n    \n      cars\n      \n      \n      \n    \n  \n  \n    \n      Chevy\n      14\n      1979\n      196\n    \n  \n\n\n\n\n\ntest[test['MPG'] < 15]\n\n\n\n\n\n  \n    \n      \n      MPG\n      Year\n      MPG^2\n    \n    \n      cars\n      \n      \n      \n    \n  \n  \n    \n      Chevy\n      14\n      1979\n      196\n    \n    \n      BMW\n      12\n      2020\n      144\n    \n  \n\n\n\n\n\niloc and loc can be used to select rows as illustrated before, but can also be used to select columns or subsets of rows/columns\n\n\ntest.loc[:,['Year','MPG']]\n\n\n\n\n\n  \n    \n      \n      Year\n      MPG\n    \n    \n      cars\n      \n      \n    \n  \n  \n    \n      Chevy\n      1979\n      14\n    \n    \n      Ford\n      1980\n      15\n    \n    \n      Dodge\n      2001\n      16\n    \n    \n      BMW\n      2020\n      12\n    \n  \n\n\n\n\n\ntest.loc['Ford','MPG']\n\n15\n\n\n\nThese work with slices and booleans as well! The following says ‚Äúgive me all the rows with MPG more then 15, and the columns starting from Year‚Äù\n\n\ntest.loc[test['MPG'] > 15, 'Year':]\n\n\n\n\n\n  \n    \n      \n      Year\n      MPG^2\n    \n    \n      cars\n      \n      \n    \n  \n  \n    \n      Dodge\n      2001\n      256\n    \n  \n\n\n\n\n\nIndexing options are fully illustrated in the book and Table 5.4\nBe careful with chained indexing:\n\n\ntest[test['MPG']> 15].loc[:,'MPG'] = 18\n\nC:\\Users\\jryan\\AppData\\Local\\Temp\\ipykernel_13388\\2484144822.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  test[test['MPG']> 15].loc[:,'MPG'] = 18\n\n\nHere we are assigning to a ‚Äòslice‚Äô, which is probably not what is intended. You will get a warning and a recommendation to fix it by using one loc:\n\ntest.loc[test['MPG']> 15 ,'MPG'] = 18\ntest\n\n\n\n\n\n  \n    \n      \n      MPG\n      Year\n      MPG^2\n    \n    \n      cars\n      \n      \n      \n    \n  \n  \n    \n      Chevy\n      14\n      1979\n      196\n    \n    \n      Ford\n      15\n      1980\n      225\n    \n    \n      Dodge\n      18\n      2001\n      256\n    \n    \n      BMW\n      12\n      2020\n      144\n    \n  \n\n\n\n\n\n\n\n\n\n\nRule of Thumb\n\n\n\nAvoid chained indexing when doing assignments\n\n\n\n\n\nArithmetic and Data Alignment\n\nPandas can make it simpler to work with objects that have different indexes, usually ‚Äòdoing the right thing‚Äô\n\n\ns1 = pd.Series([7.3, -2.5, 3.4, 1.5], index=[\"a\", \"c\", \"d\", \"e\"])\ns2 = pd.Series([-2.1, 3.6, -1.5, 4, 3.1], index=[\"a\", \"c\", \"e\", \"f\", \"g\"])\ns1+s2\n\na    5.2\nc    1.1\nd    NaN\ne    0.0\nf    NaN\ng    NaN\ndtype: float64\n\n\n\nFills can be specified by using methods:\n\n\ns1.add(s2, fill_value = 0)\n\na    5.2\nc    1.1\nd    3.4\ne    0.0\nf    4.0\ng    3.1\ndtype: float64\n\n\n\nSee Table 5.5 for list of these methods.\nYou can also do arithmetic between DataFrames and Series in a way that is similar to Numpy.\n\n\n\nFunction Application and Mapping\n\nNumpy ufuncs also work with Pandas objects.\n\n\nframe = pd.DataFrame(np.random.standard_normal((4, 3)),\n                         columns=list(\"bde\"),\n                         index=[\"Utah\", \"Ohio\", \"Texas\", \"Oregon\"])\nframe\n\n\n\n\n\n  \n    \n      \n      b\n      d\n      e\n    \n  \n  \n    \n      Utah\n      0.798548\n      -1.455476\n      0.507618\n    \n    \n      Ohio\n      -0.405775\n      -0.132380\n      -0.563721\n    \n    \n      Texas\n      -0.404526\n      0.703566\n      1.661291\n    \n    \n      Oregon\n      -0.359775\n      -0.450894\n      -0.712254\n    \n  \n\n\n\n\n\nnp.abs(frame)\n\n\n\n\n\n  \n    \n      \n      b\n      d\n      e\n    \n  \n  \n    \n      Utah\n      0.798548\n      1.455476\n      0.507618\n    \n    \n      Ohio\n      0.405775\n      0.132380\n      0.563721\n    \n    \n      Texas\n      0.404526\n      0.703566\n      1.661291\n    \n    \n      Oregon\n      0.359775\n      0.450894\n      0.712254\n    \n  \n\n\n\n\n\napply can be used to apply a function on 1D arrays to each column or row:\n\n\nframe.apply(np.max, axis = 'rows') #'axis' is optional here, default is rows\n\nb    0.798548\nd    0.703566\ne    1.661291\ndtype: float64\n\n\nApplying accross columns is common, especially to combine different columns in some way:\n\nframe['max'] = frame.apply(np.max, axis = 'columns')\nframe\n\n\n\n\n\n  \n    \n      \n      b\n      d\n      e\n      max\n    \n  \n  \n    \n      Utah\n      0.798548\n      -1.455476\n      0.507618\n      0.798548\n    \n    \n      Ohio\n      -0.405775\n      -0.132380\n      -0.563721\n      -0.132380\n    \n    \n      Texas\n      -0.404526\n      0.703566\n      1.661291\n      1.661291\n    \n    \n      Oregon\n      -0.359775\n      -0.450894\n      -0.712254\n      -0.359775\n    \n  \n\n\n\n\n\nMany more examples of this in the book.\n\n\n\nSorting and Ranking\n\nsort_index will sort with the index (on either axis for DataFrame)\nsort_values is used to sort by values or a particular column\n\n\ntest.sort_values('MPG')\n\n\n\n\n\n  \n    \n      \n      MPG\n      Year\n      MPG^2\n    \n    \n      cars\n      \n      \n      \n    \n  \n  \n    \n      BMW\n      12\n      2020\n      144\n    \n    \n      Chevy\n      14\n      1979\n      196\n    \n    \n      Ford\n      15\n      1980\n      225\n    \n    \n      Dodge\n      18\n      2001\n      256\n    \n  \n\n\n\n\n\nrank will assign ranks from on through the number of data points."
  },
  {
    "objectID": "05_notes.html#summarizing-and-computing-descriptive-statistics",
    "href": "05_notes.html#summarizing-and-computing-descriptive-statistics",
    "title": "Notes",
    "section": "Summarizing and Computing Descriptive Statistics",
    "text": "Summarizing and Computing Descriptive Statistics\n\ndf = pd.DataFrame([[1.4, np.nan], [7.1, -4.5],\n                      [np.nan, np.nan], [0.75, -1.3]],\n                      index=[\"a\", \"b\", \"c\", \"d\"],\n                      columns=[\"one\", \"two\"])\ndf\n\n\n\n\n\n  \n    \n      \n      one\n      two\n    \n  \n  \n    \n      a\n      1.40\n      NaN\n    \n    \n      b\n      7.10\n      -4.5\n    \n    \n      c\n      NaN\n      NaN\n    \n    \n      d\n      0.75\n      -1.3\n    \n  \n\n\n\n\nSome Examples:\nSum over rows:\n\ndf.sum()\n\none    9.25\ntwo   -5.80\ndtype: float64\n\n\nSum over columns:\n\n# Sum Rows\ndf.sum(axis=\"columns\")\n\na    1.40\nb    2.60\nc    0.00\nd   -0.55\ndtype: float64\n\n\nExtremely useful is describe:\n\ndf.describe()\n\n\n\n\n\n  \n    \n      \n      one\n      two\n    \n  \n  \n    \n      count\n      3.000000\n      2.000000\n    \n    \n      mean\n      3.083333\n      -2.900000\n    \n    \n      std\n      3.493685\n      2.262742\n    \n    \n      min\n      0.750000\n      -4.500000\n    \n    \n      25%\n      1.075000\n      -3.700000\n    \n    \n      50%\n      1.400000\n      -2.900000\n    \n    \n      75%\n      4.250000\n      -2.100000\n    \n    \n      max\n      7.100000\n      -1.300000\n    \n  \n\n\n\n\nBook chapter contains many more examples and a full list of summary statistics and related methods."
  },
  {
    "objectID": "05_notes.html#summary",
    "href": "05_notes.html#summary",
    "title": "Notes",
    "section": "Summary",
    "text": "Summary\n\nPrimary Panda‚Äôs data structures:\n\nSeries\nDataFrame\n\nMany ways to access and transform these objects. Key ones are:\n\n[] : access an element(s) of a Series or columns(s) of a DataFrame\nloc[r ,c] : access a row / column / cell by the index.\niloc[i, j] : access ar row / column / cell by the integer position.\n\nOnline reference.\n\n\n\n\n\n\n\nSuggestion\n\n\n\nWork though the chapter‚Äôs code and try stuff!"
  },
  {
    "objectID": "05_notes.html#references",
    "href": "05_notes.html#references",
    "title": "Notes",
    "section": "References",
    "text": "References\n\nChapter‚Äôs code.\nPanda reference."
  },
  {
    "objectID": "05_notes.html#next-chapter",
    "href": "05_notes.html#next-chapter",
    "title": "Notes",
    "section": "Next Chapter",
    "text": "Next Chapter\n\nLoading and writing data sets!"
  },
  {
    "objectID": "05_video.html",
    "href": "05_video.html",
    "title": "Video",
    "section": "",
    "text": "LOG\n00:43:47    Karim Badr: Great summary!\n00:44:18    Oluwafemi Oyedele:  Thank you !!!\n00:44:23    Jim Gruman: Thank You!!!!\n00:44:24    shamsuddeen:    thank you\n00:44:30    Jadey Ryan: thank you Ron!\n00:44:56    Layla Bouzoubaa:    Jim i love you hex stickers in the frame! i'm going to steal that idea üòÑ\n00:46:06    Serena DeStefani:   üòÇ"
  },
  {
    "objectID": "06_notes.html",
    "href": "06_notes.html",
    "title": "Notes",
    "section": "",
    "text": "Before we can even get to the fun of data analysis, we first need to learn how to load in our data!\nToday, we‚Äôll learn to work with the following categories of data inputs and outputs:"
  },
  {
    "objectID": "06_notes.html#reading-and-writing-data-in-text-format",
    "href": "06_notes.html#reading-and-writing-data-in-text-format",
    "title": "Notes",
    "section": "Reading and Writing Data in Text Format",
    "text": "Reading and Writing Data in Text Format\n\nread_csv Arguments\nTable 6.1 lists the various data types pandas can read.\nEach function can be called with pd.read_* (for example, pd.read_csv).\n\n\n\n\n\n\nNote\n\n\n\nWes points out that the number of arguments can be overwhelming. pd.read_csv has about 50. The pandas documentation is a good resource for finding the right arguments.\n\n\nTable 6.2 lists frequently used options in pd.read_csv.\nLet‚Äôs import the Palmer Penguins dataset to explore this function and some of the csv arguments. Note: I added random numbers for month and day to demonstrate date parsing.\n\nimport pandas as pd\n\npenguins = pd.read_csv(\"data/penguins.csv\")\n\npenguins.head(5)\n\n\n\n\n\n  \n    \n      \n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n      body_mass_g\n      sex\n      month\n      day\n      year\n    \n  \n  \n    \n      0\n      Adelie\n      Torgersen\n      39.1\n      18.7\n      181.0\n      3750.0\n      male\n      4\n      10\n      2007\n    \n    \n      1\n      Adelie\n      Torgersen\n      39.5\n      17.4\n      186.0\n      3800.0\n      female\n      3\n      6\n      2007\n    \n    \n      2\n      Adelie\n      Torgersen\n      40.3\n      18.0\n      195.0\n      3250.0\n      female\n      7\n      22\n      2007\n    \n    \n      3\n      Adelie\n      Torgersen\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      2\n      13\n      2007\n    \n    \n      4\n      Adelie\n      Torgersen\n      36.7\n      19.3\n      193.0\n      3450.0\n      female\n      8\n      21\n      2007\n    \n  \n\n\n\n\n\nIndex Columns\nIndexing gets column names from the file or from this argument\n\npenguins_indexed = pd.read_csv(\"data/penguins.csv\", index_col = \"species\")\npenguins_indexed.head(5)\n\n\n\n\n\n  \n    \n      \n      island\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n      body_mass_g\n      sex\n      month\n      day\n      year\n    \n    \n      species\n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Adelie\n      Torgersen\n      39.1\n      18.7\n      181.0\n      3750.0\n      male\n      4\n      10\n      2007\n    \n    \n      Adelie\n      Torgersen\n      39.5\n      17.4\n      186.0\n      3800.0\n      female\n      3\n      6\n      2007\n    \n    \n      Adelie\n      Torgersen\n      40.3\n      18.0\n      195.0\n      3250.0\n      female\n      7\n      22\n      2007\n    \n    \n      Adelie\n      Torgersen\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      2\n      13\n      2007\n    \n    \n      Adelie\n      Torgersen\n      36.7\n      19.3\n      193.0\n      3450.0\n      female\n      8\n      21\n      2007\n    \n  \n\n\n\n\n\n\nInfer or Convert Data Type\nType inference and data conversion converts values (including missing) to a user-defined value.\nIf you data uses another string value as the missing placeholder, you can add it to na_values.\n\npenguins_NA = pd.read_csv(\n  \"data/penguins.csv\", \n  na_values = [\"male\"]\n  )\n  \npenguins_NA.head(5)\n\n\n\n\n\n  \n    \n      \n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n      body_mass_g\n      sex\n      month\n      day\n      year\n    \n  \n  \n    \n      0\n      Adelie\n      Torgersen\n      39.1\n      18.7\n      181.0\n      3750.0\n      NaN\n      4\n      10\n      2007\n    \n    \n      1\n      Adelie\n      Torgersen\n      39.5\n      17.4\n      186.0\n      3800.0\n      female\n      3\n      6\n      2007\n    \n    \n      2\n      Adelie\n      Torgersen\n      40.3\n      18.0\n      195.0\n      3250.0\n      female\n      7\n      22\n      2007\n    \n    \n      3\n      Adelie\n      Torgersen\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      2\n      13\n      2007\n    \n    \n      4\n      Adelie\n      Torgersen\n      36.7\n      19.3\n      193.0\n      3450.0\n      female\n      8\n      21\n      2007\n    \n  \n\n\n\n\n\n\nParse Date and Time\nDate and time parsing combines date and time from multiple columns into a single column\n\npenguins_dates = pd.read_csv(\n  \"data/penguins.csv\", \n  parse_dates = {\"date\": [\"month\", \"day\", \"year\"]}\n  )\n  \npenguins_dates[\"date\"] = pd.to_datetime(\n  penguins_dates.date, \n  format = \"%m%d%Y\"\n  )\n  \nprint(penguins_dates.date.head(5))\n\nprint(penguins_dates.date.dtypes)\n\n0   2007-04-10\n1   2007-03-06\n2   2007-07-22\n3   2007-02-13\n4   2007-08-21\nName: date, dtype: datetime64[ns]\ndatetime64[ns]\n\n\n\n\nIterate Through Large Files\nIterating allows iteration over chunks of very large files\nUsing nrows to read in only 5 rows:\n\npd.read_csv(\"data/penguins.csv\", nrows = 5\n  )\n\n\n\n\n\n  \n    \n      \n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n      body_mass_g\n      sex\n      month\n      day\n      year\n    \n  \n  \n    \n      0\n      Adelie\n      Torgersen\n      39.1\n      18.7\n      181.0\n      3750.0\n      male\n      4\n      10\n      2007\n    \n    \n      1\n      Adelie\n      Torgersen\n      39.5\n      17.4\n      186.0\n      3800.0\n      female\n      3\n      6\n      2007\n    \n    \n      2\n      Adelie\n      Torgersen\n      40.3\n      18.0\n      195.0\n      3250.0\n      female\n      7\n      22\n      2007\n    \n    \n      3\n      Adelie\n      Torgersen\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      2\n      13\n      2007\n    \n    \n      4\n      Adelie\n      Torgersen\n      36.7\n      19.3\n      193.0\n      3450.0\n      female\n      8\n      21\n      2007\n    \n  \n\n\n\n\nUsing chunksize and the TextFileReader to aggregate and summarize the data by species:\n\nchunker = pd.read_csv(\"data/penguins.csv\", chunksize = 10)\n\nprint(type(chunker))\n\ntot = pd.Series([], dtype = 'int64')\nfor piece in chunker:\n    tot = tot.add(piece[\"species\"].value_counts(), fill_value = 0)\n\ntot\n\n<class 'pandas.io.parsers.readers.TextFileReader'>\n\n\nAdelie       152.0\nChinstrap     68.0\nGentoo       124.0\ndtype: float64\n\n\n\n\nImport Semi-Clean Data\nUnclean data issues skips rows, comments, punctuation, etc.\nWe can import a subset of the columns using usecols and change their names (header = 0; names = [list]).\n\npenguins_custom = pd.read_csv(\n  \"data/penguins.csv\", \n  usecols = [0,1,6],\n  header = 0, \n  names = [\"Species\", \"Island\", \"Sex\"]\n  )\n\npenguins_custom.head(5)\n\n\n\n\n\n  \n    \n      \n      Species\n      Island\n      Sex\n    \n  \n  \n    \n      0\n      Adelie\n      Torgersen\n      male\n    \n    \n      1\n      Adelie\n      Torgersen\n      female\n    \n    \n      2\n      Adelie\n      Torgersen\n      female\n    \n    \n      3\n      Adelie\n      Torgersen\n      NaN\n    \n    \n      4\n      Adelie\n      Torgersen\n      female\n    \n  \n\n\n\n\n\n\n\nWriting Data to Text Format\nTo write to a csv file, we can use pandas DataFrame‚Äôs to_csv method with index = False so the row numbers are not stored in the first column. Missing values are written as empty strings, we can specify a placeholder with na_rep = \"NA\":\n\npenguins_custom.to_csv(\n  \"data/penguins_custom.csv\", \n  index = False,\n  na_rep = \"NA\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorking with Other Delimited Formats\n\nReading\nIn case your tabular data makes pandas trip up and you need a little extra manual processing, you can use Python‚Äôs built in csv module.\nLet‚Äôs read in the penguins dataset the hard, manual way.\n\nimport csv\n\npenguin_reader = csv.reader(penguins)\n\nprint(penguin_reader)\n\n<_csv.reader object at 0x0000026704996340>\n\n\nNow we have the _csv_reader object.\nNext, Wes iterated through the reader to print the lines, which seems to only give me the row with my headings.\n\nfor line in penguin_reader:\n  print(line)\n\n['species']\n['island']\n['bill_length_mm']\n['bill_depth_mm']\n['flipper_length_mm']\n['body_mass_g']\n['sex']\n['month']\n['day']\n['year']\n\n\nWe‚Äôll keep following along to wrangle it into a form we can use:\n\nwith open(\"data/penguins.csv\") as penguin_reader:\n  lines = list(csv.reader(penguin_reader))\n  \nheader, values = lines[0], lines[1:]\n\nprint(header)\nprint(values[5])\n\n['species', 'island', 'bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g', 'sex', 'month', 'day', 'year']\n['Adelie', 'Torgersen', '39.3', '20.6', '190', '3650', 'male', '3', '3', '2007']\n\n\nNow we have two lists: header and values. We use a dictionary of data columns and the expression zip(*values). This combination of dictionary comprehension and expression is generally faster than iterating through a loop. However, Wes warns that this can use a lot of memory on large files.\n\npenguin_dict = {h: v for h, v in zip(header, zip(*values))}\n\n# too big to print and I'm not sure how to print a select few key-value pairs\n\n\n\n\n\n\n\nRecall\n\n\n\nFor a reminder on dictionary comprehensions, see Chapter 3.\n\n\nNow to finally get this into a usable dataframe we‚Äôll use pandas DataFrame from_dict method!\n\npenguin_df = pd.DataFrame.from_dict(penguin_dict)\npenguin_df.head(5)\n\n\n\n\n\n  \n    \n      \n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n      body_mass_g\n      sex\n      month\n      day\n      year\n    \n  \n  \n    \n      0\n      Adelie\n      Torgersen\n      39.1\n      18.7\n      181\n      3750\n      male\n      4\n      10\n      2007\n    \n    \n      1\n      Adelie\n      Torgersen\n      39.5\n      17.4\n      186\n      3800\n      female\n      3\n      6\n      2007\n    \n    \n      2\n      Adelie\n      Torgersen\n      40.3\n      18\n      195\n      3250\n      female\n      7\n      22\n      2007\n    \n    \n      3\n      Adelie\n      Torgersen\n      NA\n      NA\n      NA\n      NA\n      NA\n      2\n      13\n      2007\n    \n    \n      4\n      Adelie\n      Torgersen\n      36.7\n      19.3\n      193\n      3450\n      female\n      8\n      21\n      2007\n    \n  \n\n\n\n\n\n\ncsv.Dialect\nSince there are many kinds of delimited files, string quoting conventions, and line terminators, you may find yourself wanting to define a ‚ÄúDialect‚Äù to read in your delimited file. The options available are found in Table 6.3.\nYou can either define a csv.Dialect subclass or pass dialect parameters to csv.reader.\n\n# option 1\n\n## define a dialect subclass\n\nclass my_dialect(csv.Dialect):\n    lineterminator = \"\\n\"\n    delimiter = \";\"\n    quotechar = '\"'\n    quoting = csv.QUOTE_MINIMAL\n    \n## use the subclass\n\nreader = csv.reader(penguins, dialect = my_dialect)\n\n# option 2\n\n## pass just dialect parameters\n\nreader = csv.reader(penguins, delimiter = \",\")\n\n\n\n\n\n\n\nRecap for when to use what?\n\n\n\nFor most data, pandas read_* functions, plus the overwhelming number of options, will likely get you close to what you need.\nIf there are additional, minor wrangling needs, you can try using Python‚Äôs csv.reader with either a csv.Dialect subclass or just by passing in dialect parameters.\nIf you have complicated or multicharacter delimiters, you‚Äôll likely need to import the string module and use the split method or regular expression method re.split.\n\n\n\n\nWriting\ncsv.writer is the companion to csv.reader with the same dialect and format options. The first argument in open is the path and filename you want to write to and the second argument \"w\" makes the file writeable.\n\n\n\n\n\n\nNote\n\n\n\nPython documentation notes that newline=\"\" should be specified in case there are newlines embedded inside quoted fields to ensure they are interpreted correctly.\n\n\n\nwith open(\"data/write_data.csv\", \"w\", newline = \"\") as f:\n    writer = csv.writer(f, dialect = my_dialect)\n    writer.writerow((\"one\", \"two\", \"three\"))\n    writer.writerow((\"1\", \"2\", \"3\"))\n    writer.writerow((\"4\", \"5\", \"6\"))\n    writer.writerow((\"7\", \"8\", \"9\"))\n\n\n\nJavaScript Object Notation (JSON) Data\nStandard format for HTTP requests between web browsers, applications, and APIs. Its almost valid Python code:\n\nInstead of NaN, it uses null\nDoesn‚Äôt allow trailing commas at end of lists\nData types: objects (dictionaries), arrays (lists), strings, numbers, booleans, and nulls.\n\nWe‚Äôll make up a simple file of my pets‚Äô names, types, and sex to demonstrate JSON data loading and writing.\n\nImport the json module and use json.loads to convert a JSON string to Python. There are multiple ways to convert JSON objects to a DataFrame.\n\nimport json\n\nobj = \"\"\"\n{\"name\": \"Jadey\",\n \"pets\": [{\"name\": \"Mai\", \"type\": \"cat\", \"sex\": \"Female\"},\n          {\"name\": \"Tai\", \"type\": \"cat\", \"sex\": \"Male\"},\n          {\"name\": \"Skye\", \"type\": \"cat\", \"sex\": \"Female\"}]\n}\n\"\"\"\n\njson_to_py = json.loads(obj)\n\nprint(json_to_py)\ntype(json_to_py)\n\n{'name': 'Jadey', 'pets': [{'name': 'Mai', 'type': 'cat', 'sex': 'Female'}, {'name': 'Tai', 'type': 'cat', 'sex': 'Male'}, {'name': 'Skye', 'type': 'cat', 'sex': 'Female'}]}\n\n\ndict\n\n\nSince this imported the object as a dictionary, we can use pd.DataFrame to create a DataFrame of the pets‚Äô names, type, and sex.\n\npets_df = pd.DataFrame(json_to_py[\"pets\"], columns = [\"name\", \"type\", \"sex\"])\n\nprint(type(pets_df))\npets_df\n\n<class 'pandas.core.frame.DataFrame'>\n\n\n\n\n\n\n  \n    \n      \n      name\n      type\n      sex\n    \n  \n  \n    \n      0\n      Mai\n      cat\n      Female\n    \n    \n      1\n      Tai\n      cat\n      Male\n    \n    \n      2\n      Skye\n      cat\n      Female\n    \n  \n\n\n\n\nUse json.dumps to convert from Python (class: dictionary) back to JSON (class: string).\n\npy_to_json = json.dumps(json_to_py)\n\nprint(\"json_to_py type:\", type(json_to_py))\nprint(\"py_to_json type:\", type(py_to_json))\npy_to_json\n\njson_to_py type: <class 'dict'>\npy_to_json type: <class 'str'>\n\n\n'{\"name\": \"Jadey\", \"pets\": [{\"name\": \"Mai\", \"type\": \"cat\", \"sex\": \"Female\"}, {\"name\": \"Tai\", \"type\": \"cat\", \"sex\": \"Male\"}, {\"name\": \"Skye\", \"type\": \"cat\", \"sex\": \"Female\"}]}'\n\n\nWe can use pandas pd.read_json function and to_json DataFrame method to read and write JSON files.\n\npets_df.to_json(\"data/pets.json\")\n\nWe can easily import a JSON file using pandas.read_json.\n\npet_data = pd.read_json(\"data/pets.json\")\npet_data\n\n\n\n\n\n  \n    \n      \n      name\n      type\n      sex\n    \n  \n  \n    \n      0\n      Mai\n      cat\n      Female\n    \n    \n      1\n      Tai\n      cat\n      Male\n    \n    \n      2\n      Skye\n      cat\n      Female\n    \n  \n\n\n\n\n\n\n\nWeb Scraping\n\nHTML\npd.read_html uses libraries to read and write HTML and XML:\n\nTry: xlml [faster]\nCatch: beautifulsoup4 and html5lib [better equipped for malformed files]\n\nIf you want to specify which parsing engine is used, you can use the flavor argument.\n\ntables = pd.read_html(\n  \"https://www.fdic.gov/resources/resolutions/bank-failures/failed-bank-list/\", \n  flavor = \"html5lib\"\n  )\n\nprint(\"Table Length:\", len(tables))\n\n# since this outputs a list of tables, we can grab just the first table\n\ntables[0].head(5)\n\nTable Length: 1\n\n\n\n\n\n\n  \n    \n      \n      Bank NameBank\n      CityCity\n      StateSt\n      CertCert\n      Acquiring InstitutionAI\n      Closing DateClosing\n      FundFund\n    \n  \n  \n    \n      0\n      Almena State Bank\n      Almena\n      KS\n      15426\n      Equity Bank\n      October 23, 2020\n      10538\n    \n    \n      1\n      First City Bank of Florida\n      Fort Walton Beach\n      FL\n      16748\n      United Fidelity Bank, fsb\n      October 16, 2020\n      10537\n    \n    \n      2\n      The First State Bank\n      Barboursville\n      WV\n      14361\n      MVB Bank, Inc.\n      April 3, 2020\n      10536\n    \n    \n      3\n      Ericson State Bank\n      Ericson\n      NE\n      18265\n      Farmers and Merchants Bank\n      February 14, 2020\n      10535\n    \n    \n      4\n      City National Bank of New Jersey\n      Newark\n      NJ\n      21111\n      Industrial Bank\n      November 1, 2019\n      10534\n    \n  \n\n\n\n\n\n\nXML\nXML format is more general than HTML, but they are structurally similar. See pandas documentation for pd.read_xml.\nThis snippet of an xml file is from Microsoft.\n<catalog>\n   <book id=\"bk101\">\n      <author>Gambardella, Matthew</author>\n      <title>XML Developer's Guide</title>\n      <genre>Computer</genre>\n      <price>44.95</price>\n      <publish_date>2000-10-01</publish_date>\n      <description>An in-depth look at creating applications \n      with XML.</description>\n   </book>\n\nbooks = pd.read_xml(\"data/books.xml\")\n\nbooks.head(5)\n\n\n\n\n\n  \n    \n      \n      id\n      author\n      title\n      genre\n      price\n      publish_date\n      description\n    \n  \n  \n    \n      0\n      bk101\n      Gambardella, Matthew\n      XML Developer's Guide\n      Computer\n      44.95\n      2000-10-01\n      An in-depth look at creating applications \\n  ...\n    \n    \n      1\n      bk102\n      Ralls, Kim\n      Midnight Rain\n      Fantasy\n      5.95\n      2000-12-16\n      A former architect battles corporate zombies, ...\n    \n    \n      2\n      bk103\n      Corets, Eva\n      Maeve Ascendant\n      Fantasy\n      5.95\n      2000-11-17\n      After the collapse of a nanotechnology \\n     ...\n    \n    \n      3\n      bk104\n      Corets, Eva\n      Oberon's Legacy\n      Fantasy\n      5.95\n      2001-03-10\n      In post-apocalypse England, the mysterious \\n ...\n    \n    \n      4\n      bk105\n      Corets, Eva\n      The Sundered Grail\n      Fantasy\n      5.95\n      2001-09-10\n      The two daughters of Maeve, half-sisters, \\n  ...\n    \n  \n\n\n\n\nIf you‚Äôd like to manually parse a file, Wes demonstrates this process in the textbook, before demonstrating how the following steps are turned into one line of code using pd.read_xml.\n\nfrom lxml import objectify\nUse lxml.objectify,\nCreate a dictionary of tag names to data values\nCnvert that list of dictionaries into a DataFrame.\n\n\n\n\nBinary Data Formats\n\nPickle\nPython has a built-in pickle module that converts pandas objects into the pickle format (serializes the data into a byte stream), which is generally readable only in Python.\nMore information can be found in Python documentation.\nHere‚Äôs a demo to show pickling and unpickling the penguins dataset.\n\nprint(\"Unpickled penguins type:\", type(penguins))\n\npenguins.to_pickle(\"data/penguins_pickle\")\n\n# do some machine learning\n\npickled_penguins = pd.read_pickle(\"data/penguins_pickle\")\npickled_penguins\n\nUnpickled penguins type: <class 'pandas.core.frame.DataFrame'>\n\n\n\n\n\n\n  \n    \n      \n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n      body_mass_g\n      sex\n      month\n      day\n      year\n    \n  \n  \n    \n      0\n      Adelie\n      Torgersen\n      39.1\n      18.7\n      181.0\n      3750.0\n      male\n      4\n      10\n      2007\n    \n    \n      1\n      Adelie\n      Torgersen\n      39.5\n      17.4\n      186.0\n      3800.0\n      female\n      3\n      6\n      2007\n    \n    \n      2\n      Adelie\n      Torgersen\n      40.3\n      18.0\n      195.0\n      3250.0\n      female\n      7\n      22\n      2007\n    \n    \n      3\n      Adelie\n      Torgersen\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      2\n      13\n      2007\n    \n    \n      4\n      Adelie\n      Torgersen\n      36.7\n      19.3\n      193.0\n      3450.0\n      female\n      8\n      21\n      2007\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      339\n      Chinstrap\n      Dream\n      55.8\n      19.8\n      207.0\n      4000.0\n      male\n      6\n      4\n      2009\n    \n    \n      340\n      Chinstrap\n      Dream\n      43.5\n      18.1\n      202.0\n      3400.0\n      female\n      6\n      8\n      2009\n    \n    \n      341\n      Chinstrap\n      Dream\n      49.6\n      18.2\n      193.0\n      3775.0\n      male\n      4\n      8\n      2009\n    \n    \n      342\n      Chinstrap\n      Dream\n      50.8\n      19.0\n      210.0\n      4100.0\n      male\n      8\n      24\n      2009\n    \n    \n      343\n      Chinstrap\n      Dream\n      50.2\n      18.7\n      198.0\n      3775.0\n      female\n      2\n      11\n      2009\n    \n  \n\n344 rows √ó 10 columns\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\npickle is recommended only as a short-term storage format (i.e.¬†loading and unloading your machine learning models) because the format may not be stable over time. Also, the module is not secure ‚Äì pickle data can be maliciously tampered with. Python docs recommend signing data with hmac to ensure it hasn‚Äôt been tampered with.\n\n\n\n\nMicrosoft Excel Files\npd.ExcelFile class or pd.read_excel functions use packages xlrd (for older .xlx files) and openpyxl (for newer .xlsx files), which must be installed separately from pandas.\nconda install xlrd openpyxl\npd.read_excel takes most of the same arguments as pd.read_csv.\n\npenguins_excel = pd.read_excel(\n  \"data/penguins.xlsx\", \n  index_col = \"species\",\n  parse_dates = {\"date\": [\"month\", \"day\", \"year\"]}\n)\n\npenguins_excel.head(5)\n\n\n\n\n\n  \n    \n      \n      date\n      island\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n      body_mass_g\n      sex\n    \n    \n      species\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Adelie\n      2007-04-10\n      Torgersen\n      39.1\n      18.7\n      181.0\n      3750.0\n      male\n    \n    \n      Adelie\n      2007-03-06\n      Torgersen\n      39.5\n      17.4\n      186.0\n      3800.0\n      female\n    \n    \n      Adelie\n      2007-07-22\n      Torgersen\n      40.3\n      18.0\n      195.0\n      3250.0\n      female\n    \n    \n      Adelie\n      2007-02-13\n      Torgersen\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      Adelie\n      2007-08-21\n      Torgersen\n      36.7\n      19.3\n      193.0\n      3450.0\n      female\n    \n  \n\n\n\n\nTo read multiple sheets, use pd.ExcelFile.\n\npenguins_sheets = pd.ExcelFile(\"data/penguins_sheets.xlsx\")\n\nprint(\"Available sheet names:\", penguins_sheets.sheet_names)\n\npenguins_sheets\n\nAvailable sheet names: ['chinstrap', 'gentoo', 'adelie']\n\n\n<pandas.io.excel._base.ExcelFile at 0x26705acdd90>\n\n\nThen we can parse all sheets into a dictionary by specifying the sheet_name argument as None. Or, we can read in a subset of sheets.\n\nsheets = penguins_sheets.parse(sheet_name = None)\n\nsheets\n\n{'chinstrap':       species island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n 0   Chinstrap  Dream            46.5           17.9                192   \n 1   Chinstrap  Dream            50.0           19.5                196   \n 2   Chinstrap  Dream            51.3           19.2                193   \n 3   Chinstrap  Dream            45.4           18.7                188   \n 4   Chinstrap  Dream            52.7           19.8                197   \n ..        ...    ...             ...            ...                ...   \n 63  Chinstrap  Dream            55.8           19.8                207   \n 64  Chinstrap  Dream            43.5           18.1                202   \n 65  Chinstrap  Dream            49.6           18.2                193   \n 66  Chinstrap  Dream            50.8           19.0                210   \n 67  Chinstrap  Dream            50.2           18.7                198   \n \n     body_mass_g     sex  month  day  year  \n 0          3500  female      7    4  2007  \n 1          3900    male      9    6  2007  \n 2          3650    male      4   15  2007  \n 3          3525  female      6   10  2007  \n 4          3725    male      8   19  2007  \n ..          ...     ...    ...  ...   ...  \n 63         4000    male      6    4  2009  \n 64         3400  female      6    8  2009  \n 65         3775    male      4    8  2009  \n 66         4100    male      8   24  2009  \n 67         3775  female      2   11  2009  \n \n [68 rows x 10 columns],\n 'gentoo':     species  island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n 0    Gentoo  Biscoe            46.1           13.2              211.0   \n 1    Gentoo  Biscoe            50.0           16.3              230.0   \n 2    Gentoo  Biscoe            48.7           14.1              210.0   \n 3    Gentoo  Biscoe            50.0           15.2              218.0   \n 4    Gentoo  Biscoe            47.6           14.5              215.0   \n ..      ...     ...             ...            ...                ...   \n 119  Gentoo  Biscoe             NaN            NaN                NaN   \n 120  Gentoo  Biscoe            46.8           14.3              215.0   \n 121  Gentoo  Biscoe            50.4           15.7              222.0   \n 122  Gentoo  Biscoe            45.2           14.8              212.0   \n 123  Gentoo  Biscoe            49.9           16.1              213.0   \n \n      body_mass_g     sex  month  day  year  \n 0         4500.0  female      3    8  2007  \n 1         5700.0    male      2    4  2007  \n 2         4450.0  female      7    1  2007  \n 3         5700.0    male      9   15  2007  \n 4         5400.0    male     11   19  2007  \n ..           ...     ...    ...  ...   ...  \n 119          NaN     NaN     12   11  2009  \n 120       4850.0  female      7   20  2009  \n 121       5750.0    male      9   18  2009  \n 122       5200.0  female     12   11  2009  \n 123       5400.0    male      6   15  2009  \n \n [124 rows x 10 columns],\n 'adelie':     species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n 0    Adelie  Torgersen            39.1           18.7              181.0   \n 1    Adelie  Torgersen            39.5           17.4              186.0   \n 2    Adelie  Torgersen            40.3           18.0              195.0   \n 3    Adelie  Torgersen             NaN            NaN                NaN   \n 4    Adelie  Torgersen            36.7           19.3              193.0   \n ..      ...        ...             ...            ...                ...   \n 147  Adelie      Dream            36.6           18.4              184.0   \n 148  Adelie      Dream            36.0           17.8              195.0   \n 149  Adelie      Dream            37.8           18.1              193.0   \n 150  Adelie      Dream            36.0           17.1              187.0   \n 151  Adelie      Dream            41.5           18.5              201.0   \n \n      body_mass_g     sex  month  day  year  \n 0         3750.0    male      4   10  2007  \n 1         3800.0  female      3    6  2007  \n 2         3250.0  female      7   22  2007  \n 3            NaN     NaN      2   13  2007  \n 4         3450.0  female      8   21  2007  \n ..           ...     ...    ...  ...   ...  \n 147       3475.0  female     11    4  2009  \n 148       3450.0  female      5   21  2009  \n 149       3750.0    male      8   15  2009  \n 150       3700.0  female      1   16  2009  \n 151       4000.0    male      5    8  2009  \n \n [152 rows x 10 columns]}\n\n\nThen we can subset one of the sheets as a pandas DataFrame object.\n\nchinstrap = sheets[\"chinstrap\"].head(5)\nchinstrap\n\n\n\n\n\n  \n    \n      \n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n      body_mass_g\n      sex\n      month\n      day\n      year\n    \n  \n  \n    \n      0\n      Chinstrap\n      Dream\n      46.5\n      17.9\n      192\n      3500\n      female\n      7\n      4\n      2007\n    \n    \n      1\n      Chinstrap\n      Dream\n      50.0\n      19.5\n      196\n      3900\n      male\n      9\n      6\n      2007\n    \n    \n      2\n      Chinstrap\n      Dream\n      51.3\n      19.2\n      193\n      3650\n      male\n      4\n      15\n      2007\n    \n    \n      3\n      Chinstrap\n      Dream\n      45.4\n      18.7\n      188\n      3525\n      female\n      6\n      10\n      2007\n    \n    \n      4\n      Chinstrap\n      Dream\n      52.7\n      19.8\n      197\n      3725\n      male\n      8\n      19\n      2007\n    \n  \n\n\n\n\nWrite one sheet to using to_excel:\n\nchinstrap.to_excel(\"data/chinstrap.xlsx\")\n\nIf you want to write to multiple sheets, create an ExcelWriter class and then write the data to it:\n\ngentoo = sheets[\"gentoo\"].head(5)\n\nwriter = pd.ExcelWriter(\"data/chinstrap_gentoo.xlsx\")\n\nchinstrap.to_excel(writer, sheet_name = \"chinstrap\")\n\ngentoo.to_excel(writer, sheet_name = \"gentoo\")\n\nwriter.save()\n\n\n\nHDF5 Format\nHierarchical data format (HDF) is used in Python, C, Java, Julia, MATLAB, and others for storing big scientific array data (multiple datasets and metadata within one file). HDF5 can be used to efficiently read/write chunks of large arrays.\nThe PyTables package must first be installed.\nconda install pytables\n\npip install tables # the package is called \"tables\" in PyPI\npandas provides an dictionary-like-class for HDF5 files called HDFStore:\n\nstore = pd.HDFStore(\"data/pets.h5\")\n\nstore[\"pets\"] = pets_df\nstore[\"pets\"]\n\n\n\n\n\n  \n    \n      \n      name\n      type\n      sex\n    \n  \n  \n    \n      0\n      Mai\n      cat\n      Female\n    \n    \n      1\n      Tai\n      cat\n      Male\n    \n    \n      2\n      Skye\n      cat\n      Female\n    \n  \n\n\n\n\nHDFStore can store data as a fixed or as a table schema. Table allows querying but is generally slower.\n\npets_df.to_hdf(\"data/petnames.h5\", \"pets\", format = \"table\")\npd.read_hdf(\"data/petnames.h5\", \"pets\", where=[\"columns = name\"])\n\n\n\n\n\n  \n    \n      \n      name\n    \n  \n  \n    \n      0\n      Mai\n    \n    \n      1\n      Tai\n    \n    \n      2\n      Skye\n    \n  \n\n\n\n\n\n\n\n\n\n\nWhen should I use HDF5?\n\n\n\nWes recommends using HDF5 for write-once, read-many datasets that are worked with locally. If your data is stored on remote servers, then you may try other binary formats designed for distributed storage (for example, Apache Parquet).\n\n\n\n\n\nInteracting with Web APIs\nTo access data from APIs, Wes suggests using the requests package.\nconda install requests\nLet‚Äôs pull from this free zoo animal API.\n\nimport requests\n\nurl = \"https://zoo-animal-api.herokuapp.com/animals/rand\"\n\nresp = requests.get(url)\n\nresp.raise_for_status()\n\nprint(\"HTTP status\", resp)\n\nanimal = resp.json()\nanimal\n\nanimal_df = pd.DataFrame([animal]) # important to wrap the dictionary object into a list\nanimal_df\n\nHTTP status <Response [200]>\n\n\n\n\n\n\n  \n    \n      \n      name\n      latin_name\n      animal_type\n      active_time\n      length_min\n      length_max\n      weight_min\n      weight_max\n      lifespan\n      habitat\n      diet\n      geo_range\n      image_link\n      id\n    \n  \n  \n    \n      0\n      Brazilian Porcupine\n      Coendou prehensilis\n      Mammal\n      Nocturnal\n      1.5\n      1.7\n      9\n      11\n      17\n      Tropical rainforest\n      Leaves, bark, fruits, shoots and small animals\n      Northern and eastern South America\n      https://upload.wikimedia.org/wikipedia/commons...\n      45\n    \n  \n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIt is important to note that the dictionary is wrapped into a list. If it isn‚Äôt, then you will get the following error: ValueError: If using all scalar values, you must pass an index.\n\n\n\n\nInteracting with Databases\nSome popular SQL-based relational databases are: SQL Server, PostgreSQL, MySQL, SQLite3. We can use pandas to load the results of a SQL query into a DataFrame.\nImport sqlite3 and create a database.\n\nimport sqlite3\n\ncon = sqlite3.connect(\"data/data.sqlite\")\n\nThis creates a table.\n\nquery = \"\"\"\n  CREATE TABLE states\n  (Capital VARCHAR(20), State VARCHAR(20),\n  x1 REAL, x2 INTEGER\n);\"\"\"\n\ncon.execute(query)\n\ncon.commit()\n\nThis inserts the rows of data:\n\ndata = [(\"Atlanta\", \"Georgia\", 1.25, 6), (\"Seattle\", \"Washington\", 2.6, 3), (\"Sacramento\", \"California\", 1.7, 5)]\n        \nstmt = \"INSERT INTO states VALUES(?, ?, ?, ?)\"\n\ncon.executemany(stmt, data)\n\ncon.commit()\n\nNow we can look at the data:\n\ncursor = con.execute(\"SELECT * FROM states\")\n\nrows = cursor.fetchall()\n\nrows\n\n[('Atlanta', 'Georgia', 1.25, 6),\n ('Seattle', 'Washington', 2.6, 3),\n ('Sacramento', 'California', 1.7, 5),\n ('Atlanta', 'Georgia', 1.25, 6),\n ('Seattle', 'Washington', 2.6, 3),\n ('Sacramento', 'California', 1.7, 5),\n ('Atlanta', 'Georgia', 1.25, 6),\n ('Seattle', 'Washington', 2.6, 3),\n ('Sacramento', 'California', 1.7, 5),\n ('Atlanta', 'Georgia', 1.25, 6),\n ('Seattle', 'Washington', 2.6, 3),\n ('Sacramento', 'California', 1.7, 5),\n ('Atlanta', 'Georgia', 1.25, 6),\n ('Seattle', 'Washington', 2.6, 3),\n ('Sacramento', 'California', 1.7, 5),\n ('Atlanta', 'Georgia', 1.25, 6),\n ('Seattle', 'Washington', 2.6, 3),\n ('Sacramento', 'California', 1.7, 5),\n ('Atlanta', 'Georgia', 1.25, 6),\n ('Seattle', 'Washington', 2.6, 3),\n ('Sacramento', 'California', 1.7, 5)]\n\n\nTo get the data into a pandas DataFrame, we‚Äôll need to provide column names in the cursor.description.\n\nprint(cursor.description)\n\npd.DataFrame(rows, columns = [x[0] for x in cursor.description])\n\n(('Capital', None, None, None, None, None, None), ('State', None, None, None, None, None, None), ('x1', None, None, None, None, None, None), ('x2', None, None, None, None, None, None))\n\n\n\n\n\n\n  \n    \n      \n      Capital\n      State\n      x1\n      x2\n    \n  \n  \n    \n      0\n      Atlanta\n      Georgia\n      1.25\n      6\n    \n    \n      1\n      Seattle\n      Washington\n      2.60\n      3\n    \n    \n      2\n      Sacramento\n      California\n      1.70\n      5\n    \n    \n      3\n      Atlanta\n      Georgia\n      1.25\n      6\n    \n    \n      4\n      Seattle\n      Washington\n      2.60\n      3\n    \n    \n      5\n      Sacramento\n      California\n      1.70\n      5\n    \n    \n      6\n      Atlanta\n      Georgia\n      1.25\n      6\n    \n    \n      7\n      Seattle\n      Washington\n      2.60\n      3\n    \n    \n      8\n      Sacramento\n      California\n      1.70\n      5\n    \n    \n      9\n      Atlanta\n      Georgia\n      1.25\n      6\n    \n    \n      10\n      Seattle\n      Washington\n      2.60\n      3\n    \n    \n      11\n      Sacramento\n      California\n      1.70\n      5\n    \n    \n      12\n      Atlanta\n      Georgia\n      1.25\n      6\n    \n    \n      13\n      Seattle\n      Washington\n      2.60\n      3\n    \n    \n      14\n      Sacramento\n      California\n      1.70\n      5\n    \n    \n      15\n      Atlanta\n      Georgia\n      1.25\n      6\n    \n    \n      16\n      Seattle\n      Washington\n      2.60\n      3\n    \n    \n      17\n      Sacramento\n      California\n      1.70\n      5\n    \n    \n      18\n      Atlanta\n      Georgia\n      1.25\n      6\n    \n    \n      19\n      Seattle\n      Washington\n      2.60\n      3\n    \n    \n      20\n      Sacramento\n      California\n      1.70\n      5\n    \n  \n\n\n\n\nAs per usual, Wes likes to show us the manual way first and then the easier version. Using SQLAlchemy, we can must less verbosely create our DataFrame.\n\nimport sqlalchemy as sqla\n\ndb = sqla.create_engine(\"sqlite:///data/data.sqlite\")\n\npd.read_sql(\"SELECT * FROM states\", db)\n\n\n\n\n\n  \n    \n      \n      Capital\n      State\n      x1\n      x2\n    \n  \n  \n    \n      0\n      Atlanta\n      Georgia\n      1.25\n      6\n    \n    \n      1\n      Seattle\n      Washington\n      2.60\n      3\n    \n    \n      2\n      Sacramento\n      California\n      1.70\n      5\n    \n    \n      3\n      Atlanta\n      Georgia\n      1.25\n      6\n    \n    \n      4\n      Seattle\n      Washington\n      2.60\n      3\n    \n    \n      5\n      Sacramento\n      California\n      1.70\n      5\n    \n    \n      6\n      Atlanta\n      Georgia\n      1.25\n      6\n    \n    \n      7\n      Seattle\n      Washington\n      2.60\n      3\n    \n    \n      8\n      Sacramento\n      California\n      1.70\n      5\n    \n    \n      9\n      Atlanta\n      Georgia\n      1.25\n      6\n    \n    \n      10\n      Seattle\n      Washington\n      2.60\n      3\n    \n    \n      11\n      Sacramento\n      California\n      1.70\n      5\n    \n    \n      12\n      Atlanta\n      Georgia\n      1.25\n      6\n    \n    \n      13\n      Seattle\n      Washington\n      2.60\n      3\n    \n    \n      14\n      Sacramento\n      California\n      1.70\n      5\n    \n    \n      15\n      Atlanta\n      Georgia\n      1.25\n      6\n    \n    \n      16\n      Seattle\n      Washington\n      2.60\n      3\n    \n    \n      17\n      Sacramento\n      California\n      1.70\n      5\n    \n    \n      18\n      Atlanta\n      Georgia\n      1.25\n      6\n    \n    \n      19\n      Seattle\n      Washington\n      2.60\n      3\n    \n    \n      20\n      Sacramento\n      California\n      1.70\n      5"
  },
  {
    "objectID": "06_video.html",
    "href": "06_video.html",
    "title": "Video",
    "section": "",
    "text": "LOG\n00:04:53    phanikumar tata:    hello everyone , got some issues with my camera\n00:57:00    Jim Gruman: thank you Jadey!!\n00:57:30    phanikumar tata:    Great presentation\n00:57:47    phanikumar tata:    can you\n00:58:14    phanikumar tata:    you show you how u r doing .QMD plese\n00:58:33    phanikumar tata:    Quearto to run Python\n00:58:37    Isabella Vel√°squez: quarto --help\n01:00:22    Isabella Vel√°squez: https://www.youtube.com/watch?v=MaEjLS2ouGg&list=PL3x6DOfs2NGh7IQIQ_pXNkjLVKa-7lgCw&index=2\n01:00:50    phanikumar tata:    do u have a link\n01:00:56    phanikumar tata:    Thanks a lot\n01:02:08    phanikumar tata:    Oh u need to install reticulate on top of Quearto\n01:03:06    Isabella Vel√°squez: pip install nbformat\n01:04:33    Isabella Vel√°squez: you do if you're using RStudio. If you're using Jupyter Lab or VS Code, you do not\n01:04:54    Isabella Vel√°squez: actually, you may not need reticulate on RStudio - have to double check\n01:07:51    phanikumar tata:    some times\n01:07:59    phanikumar tata:    .lock file would fail it\n01:08:02    phanikumar tata:    to work"
  },
  {
    "objectID": "07_video.html",
    "href": "07_video.html",
    "title": "Video",
    "section": "",
    "text": "LOG"
  },
  {
    "objectID": "08_video.html",
    "href": "08_video.html",
    "title": "Video",
    "section": "",
    "text": "LOG"
  },
  {
    "objectID": "09_video.html",
    "href": "09_video.html",
    "title": "Video",
    "section": "",
    "text": "LOG"
  },
  {
    "objectID": "10_video.html",
    "href": "10_video.html",
    "title": "Video",
    "section": "",
    "text": "LOG"
  },
  {
    "objectID": "11_video.html",
    "href": "11_video.html",
    "title": "Video",
    "section": "",
    "text": "LOG"
  },
  {
    "objectID": "12_video.html",
    "href": "12_video.html",
    "title": "Video",
    "section": "",
    "text": "LOG"
  },
  {
    "objectID": "example_quarto.html",
    "href": "example_quarto.html",
    "title": "Example Quarto Document",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "example_quarto.html#running-code",
    "href": "example_quarto.html#running-code",
    "title": "Example Quarto Document",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n2\n\n\nYou can add options to executable code like this\n\n\n4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "example_python.html",
    "href": "example_python.html",
    "title": "Example Jupyter Notebook",
    "section": "",
    "text": "import numpy as np\na = np.arange(15).reshape(3, 5)\na\n\narray([[ 0,  1,  2,  3,  4],\n       [ 5,  6,  7,  8,  9],\n       [10, 11, 12, 13, 14]])"
  },
  {
    "objectID": "example_python.html#matplotlib",
    "href": "example_python.html#matplotlib",
    "title": "Example Jupyter Notebook",
    "section": "Matplotlib",
    "text": "Matplotlib\n\nimport matplotlib.pyplot as plt\n\nfig = plt.figure()\nx = np.arange(10)\ny = 2.5 * np.sin(x / 20 * np.pi)\nyerr = np.linspace(0.05, 0.2, 10)\n\nplt.errorbar(x, y + 3, yerr=yerr, label='both limits (default)')\nplt.errorbar(x, y + 2, yerr=yerr, uplims=True, label='uplims=True')\nplt.errorbar(x, y + 1, yerr=yerr, uplims=True, lolims=True,\n             label='uplims=True, lolims=True')\n\nupperlimits = [True, False] * 5\nlowerlimits = [False, True] * 5\nplt.errorbar(x, y, yerr=yerr, uplims=upperlimits, lolims=lowerlimits,\n             label='subsets of uplims and lolims')\n\nplt.legend(loc='lower right')\nplt.show(fig)"
  },
  {
    "objectID": "how-to.html",
    "href": "how-to.html",
    "title": "How to add to the book",
    "section": "",
    "text": "This book is made with Quarto. Please see the Get Started chapter of the Quarto documentation to learn how to install and run Quarto in your IDE."
  },
  {
    "objectID": "how-to.html#add-to-book",
    "href": "how-to.html#add-to-book",
    "title": "How to add to the book",
    "section": "Add to book",
    "text": "Add to book\nOnce you have everything set up, forked the repo, and cloned to your computer, you can add a new chapter to the book.\nCreate a new file in the repository folder. For example, to create a new file called 01_exercises.qmd, navigate to the folder then create one using touch 01_exercises.qmd. If you are using VSCode, you can use the Quarto plug-in. You can use plain .md files, Quarto .qmd, or Jupyter .ipynb files in this book. Check out the files under Examples to see the various options.\nWrite in what you would like in the file.\nThen, in the _quarto.yml file, under chapters, add a part with your chapter. The file listed after part is the first page of chapter; the ones under chapters will be subpages.\n  - part: 01_main.qmd\n      chapters: \n      - 01_notes.qmd\n      - 01_video.qmd\n      - 01_exercises.qmd"
  },
  {
    "objectID": "how-to.html#render-the-book",
    "href": "how-to.html#render-the-book",
    "title": "How to add to the book",
    "section": "Render the book",
    "text": "Render the book\nOnce you have added and edited your files, don‚Äôt forget to render the book. Run this in the terminal:\nquarto render --to html"
  },
  {
    "objectID": "how-to.html#push-up-to-github",
    "href": "how-to.html#push-up-to-github",
    "title": "How to add to the book",
    "section": "Push up to GitHub",
    "text": "Push up to GitHub\nPush your changes to your forked repo and then create a pull request for the R4DS admins to merge your changes.\ngit add .\ngit commit -m \"Message here\"\ngit push"
  }
]